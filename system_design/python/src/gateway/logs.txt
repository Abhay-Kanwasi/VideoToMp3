
==> Audit <==
|------------|---------------------|----------|-------|---------|---------------------|---------------------|
|  Command   |        Args         | Profile  | User  | Version |     Start Time      |      End Time       |
|------------|---------------------|----------|-------|---------|---------------------|---------------------|
| start      |                     | minikube | abhay | v1.35.0 | 18 Mar 25 22:14 IST | 18 Mar 25 22:22 IST |
| docker-env | minikube docker-env | minikube | abhay | v1.35.0 | 18 Mar 25 23:05 IST | 18 Mar 25 23:05 IST |
| docker-env | minikube docker-env | minikube | abhay | v1.35.0 | 18 Mar 25 23:06 IST | 18 Mar 25 23:06 IST |
| docker-env | minikube docker-env | minikube | abhay | v1.35.0 | 18 Mar 25 23:07 IST | 18 Mar 25 23:07 IST |
| start      |                     | minikube | abhay | v1.35.0 | 19 Mar 25 20:47 IST | 19 Mar 25 20:48 IST |
| addons     | lis                 | minikube | abhay | v1.35.0 | 23 Mar 25 20:56 IST | 23 Mar 25 20:56 IST |
| addons     | list                | minikube | abhay | v1.35.0 | 23 Mar 25 20:56 IST | 23 Mar 25 20:56 IST |
| addons     | ingress enable      | minikube | abhay | v1.35.0 | 23 Mar 25 20:56 IST | 23 Mar 25 20:56 IST |
| addons     | enable ingress      | minikube | abhay | v1.35.0 | 23 Mar 25 20:57 IST |                     |
|------------|---------------------|----------|-------|---------|---------------------|---------------------|


==> Last Start <==
Log file created at: 2025/03/19 20:47:21
Running on machine: abhay-Inspiron-3585
Binary: Built with gc go1.23.4 for linux/amd64
Log line format: [IWEF]mmdd hh:mm:ss.uuuuuu threadid file:line] msg
I0319 20:47:21.129168   14863 out.go:345] Setting OutFile to fd 1 ...
I0319 20:47:21.129484   14863 out.go:397] isatty.IsTerminal(1) = true
I0319 20:47:21.129487   14863 out.go:358] Setting ErrFile to fd 2...
I0319 20:47:21.129492   14863 out.go:397] isatty.IsTerminal(2) = true
I0319 20:47:21.129714   14863 root.go:338] Updating PATH: /home/abhay/.minikube/bin
W0319 20:47:21.129845   14863 root.go:314] Error reading config file at /home/abhay/.minikube/config/config.json: open /home/abhay/.minikube/config/config.json: no such file or directory
I0319 20:47:21.141673   14863 out.go:352] Setting JSON to false
I0319 20:47:21.143218   14863 start.go:129] hostinfo: {"hostname":"abhay-Inspiron-3585","uptime":1382,"bootTime":1742396059,"procs":282,"os":"linux","platform":"ubuntu","platformFamily":"debian","platformVersion":"22.04","kernelVersion":"6.8.0-52-generic","kernelArch":"x86_64","virtualizationSystem":"kvm","virtualizationRole":"host","hostId":"86c3254d-fbe7-4c94-8190-a218338cc0b0"}
I0319 20:47:21.143340   14863 start.go:139] virtualization: kvm host
I0319 20:47:21.200775   14863 out.go:177] 😄  minikube v1.35.0 on Ubuntu 22.04
I0319 20:47:21.258473   14863 notify.go:220] Checking for updates...
I0319 20:47:21.259567   14863 config.go:182] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.32.0
I0319 20:47:21.260835   14863 driver.go:394] Setting default libvirt URI to qemu:///system
I0319 20:47:21.304427   14863 docker.go:123] docker version: linux-26.1.3:
I0319 20:47:21.304630   14863 cli_runner.go:164] Run: docker system info --format "{{json .}}"
I0319 20:47:22.708828   14863 cli_runner.go:217] Completed: docker system info --format "{{json .}}": (1.404153461s)
I0319 20:47:22.709460   14863 info.go:266] docker info: {ID:f260cda2-7fa7-4dd7-b336-0234968f0ab0 Containers:1 ContainersRunning:0 ContainersPaused:0 ContainersStopped:1 Images:9 Driver:overlay2 DriverStatus:[[Backing Filesystem extfs] [Supports d_type true] [Using metacopy false] [Native Overlay Diff true] [userxattr false]] SystemStatus:<nil> Plugins:{Volume:[local] Network:[bridge host ipvlan macvlan null overlay] Authorization:<nil> Log:[awslogs fluentd gcplogs gelf journald json-file local splunk syslog]} MemoryLimit:true SwapLimit:true KernelMemory:false KernelMemoryTCP:false CPUCfsPeriod:true CPUCfsQuota:true CPUShares:true CPUSet:true PidsLimit:true IPv4Forwarding:true BridgeNfIptables:true BridgeNfIP6Tables:true Debug:false NFd:23 OomKillDisable:false NGoroutines:41 SystemTime:2025-03-19 20:47:22.694989052 +0530 IST LoggingDriver:json-file CgroupDriver:systemd NEventsListener:0 KernelVersion:6.8.0-52-generic OperatingSystem:Ubuntu 22.04.5 LTS OSType:linux Architecture:x86_64 IndexServerAddress:https://index.docker.io/v1/ RegistryConfig:{AllowNondistributableArtifactsCIDRs:[] AllowNondistributableArtifactsHostnames:[] InsecureRegistryCIDRs:[127.0.0.0/8] IndexConfigs:{DockerIo:{Name:docker.io Mirrors:[] Secure:true Official:true}} Mirrors:[]} NCPU:4 MemTotal:15597641728 GenericResources:<nil> DockerRootDir:/var/lib/docker HTTPProxy: HTTPSProxy: NoProxy: Name:abhay-Inspiron-3585 Labels:[] ExperimentalBuild:false ServerVersion:26.1.3 ClusterStore: ClusterAdvertise: Runtimes:{Runc:{Path:runc}} DefaultRuntime:runc Swarm:{NodeID: NodeAddr: LocalNodeState:inactive ControlAvailable:false Error: RemoteManagers:<nil>} LiveRestoreEnabled:false Isolation: InitBinary:docker-init ContainerdCommit:{ID: Expected:} RuncCommit:{ID: Expected:} InitCommit:{ID: Expected:} SecurityOptions:[name=apparmor name=seccomp,profile=builtin name=cgroupns] ProductLicense: Warnings:<nil> ServerErrors:[] ClientInfo:{Debug:false Plugins:[map[Name:compose Path:/usr/libexec/docker/cli-plugins/docker-compose SchemaVersion:0.1.0 ShortDescription:Docker Compose Vendor:Docker Inc. Version:v2.32.4]] Warnings:<nil>}}
I0319 20:47:22.709569   14863 docker.go:318] overlay module found
I0319 20:47:22.748442   14863 out.go:177] ✨  Using the docker driver based on existing profile
I0319 20:47:22.771031   14863 start.go:297] selected driver: docker
I0319 20:47:22.771051   14863 start.go:901] validating driver "docker" against &{Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.46@sha256:fd2d445ddcc33ebc5c6b68a17e6219ea207ce63c005095ea1525296da2d1a279 Memory:3700 CPUs:2 DiskSize:20000 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.32.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.32.0 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[default-storageclass:true storage-provisioner:true] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:/home/abhay:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s}
I0319 20:47:22.771204   14863 start.go:912] status for docker: {Installed:true Healthy:true Running:false NeedsImprovement:false Error:<nil> Reason: Fix: Doc: Version:}
I0319 20:47:22.771484   14863 cli_runner.go:164] Run: docker system info --format "{{json .}}"
I0319 20:47:22.859198   14863 info.go:266] docker info: {ID:f260cda2-7fa7-4dd7-b336-0234968f0ab0 Containers:1 ContainersRunning:0 ContainersPaused:0 ContainersStopped:1 Images:9 Driver:overlay2 DriverStatus:[[Backing Filesystem extfs] [Supports d_type true] [Using metacopy false] [Native Overlay Diff true] [userxattr false]] SystemStatus:<nil> Plugins:{Volume:[local] Network:[bridge host ipvlan macvlan null overlay] Authorization:<nil> Log:[awslogs fluentd gcplogs gelf journald json-file local splunk syslog]} MemoryLimit:true SwapLimit:true KernelMemory:false KernelMemoryTCP:false CPUCfsPeriod:true CPUCfsQuota:true CPUShares:true CPUSet:true PidsLimit:true IPv4Forwarding:true BridgeNfIptables:true BridgeNfIP6Tables:true Debug:false NFd:23 OomKillDisable:false NGoroutines:41 SystemTime:2025-03-19 20:47:22.847467487 +0530 IST LoggingDriver:json-file CgroupDriver:systemd NEventsListener:0 KernelVersion:6.8.0-52-generic OperatingSystem:Ubuntu 22.04.5 LTS OSType:linux Architecture:x86_64 IndexServerAddress:https://index.docker.io/v1/ RegistryConfig:{AllowNondistributableArtifactsCIDRs:[] AllowNondistributableArtifactsHostnames:[] InsecureRegistryCIDRs:[127.0.0.0/8] IndexConfigs:{DockerIo:{Name:docker.io Mirrors:[] Secure:true Official:true}} Mirrors:[]} NCPU:4 MemTotal:15597641728 GenericResources:<nil> DockerRootDir:/var/lib/docker HTTPProxy: HTTPSProxy: NoProxy: Name:abhay-Inspiron-3585 Labels:[] ExperimentalBuild:false ServerVersion:26.1.3 ClusterStore: ClusterAdvertise: Runtimes:{Runc:{Path:runc}} DefaultRuntime:runc Swarm:{NodeID: NodeAddr: LocalNodeState:inactive ControlAvailable:false Error: RemoteManagers:<nil>} LiveRestoreEnabled:false Isolation: InitBinary:docker-init ContainerdCommit:{ID: Expected:} RuncCommit:{ID: Expected:} InitCommit:{ID: Expected:} SecurityOptions:[name=apparmor name=seccomp,profile=builtin name=cgroupns] ProductLicense: Warnings:<nil> ServerErrors:[] ClientInfo:{Debug:false Plugins:[map[Name:compose Path:/usr/libexec/docker/cli-plugins/docker-compose SchemaVersion:0.1.0 ShortDescription:Docker Compose Vendor:Docker Inc. Version:v2.32.4]] Warnings:<nil>}}
I0319 20:47:22.860047   14863 cni.go:84] Creating CNI manager for ""
I0319 20:47:22.880642   14863 cni.go:158] "docker" driver + "docker" container runtime found on kubernetes v1.24+, recommending bridge
I0319 20:47:22.880736   14863 start.go:340] cluster config:
{Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.46@sha256:fd2d445ddcc33ebc5c6b68a17e6219ea207ce63c005095ea1525296da2d1a279 Memory:3700 CPUs:2 DiskSize:20000 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.32.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.32.0 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[default-storageclass:true storage-provisioner:true] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:/home/abhay:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s}
I0319 20:47:22.915960   14863 out.go:177] 👍  Starting "minikube" primary control-plane node in "minikube" cluster
I0319 20:47:22.938576   14863 cache.go:121] Beginning downloading kic base image for docker with docker
I0319 20:47:22.961332   14863 out.go:177] 🚜  Pulling base image v0.0.46 ...
I0319 20:47:22.983732   14863 preload.go:131] Checking if preload exists for k8s version v1.32.0 and runtime docker
I0319 20:47:22.983817   14863 preload.go:146] Found local preload: /home/abhay/.minikube/cache/preloaded-tarball/preloaded-images-k8s-v18-v1.32.0-docker-overlay2-amd64.tar.lz4
I0319 20:47:22.983826   14863 cache.go:56] Caching tarball of preloaded images
I0319 20:47:22.983881   14863 image.go:81] Checking for gcr.io/k8s-minikube/kicbase:v0.0.46@sha256:fd2d445ddcc33ebc5c6b68a17e6219ea207ce63c005095ea1525296da2d1a279 in local docker daemon
I0319 20:47:22.984305   14863 preload.go:172] Found /home/abhay/.minikube/cache/preloaded-tarball/preloaded-images-k8s-v18-v1.32.0-docker-overlay2-amd64.tar.lz4 in cache, skipping download
I0319 20:47:22.984320   14863 cache.go:59] Finished verifying existence of preloaded tar for v1.32.0 on docker
I0319 20:47:22.984459   14863 profile.go:143] Saving config to /home/abhay/.minikube/profiles/minikube/config.json ...
I0319 20:47:23.050270   14863 image.go:100] Found gcr.io/k8s-minikube/kicbase:v0.0.46@sha256:fd2d445ddcc33ebc5c6b68a17e6219ea207ce63c005095ea1525296da2d1a279 in local docker daemon, skipping pull
I0319 20:47:23.050293   14863 cache.go:145] gcr.io/k8s-minikube/kicbase:v0.0.46@sha256:fd2d445ddcc33ebc5c6b68a17e6219ea207ce63c005095ea1525296da2d1a279 exists in daemon, skipping load
I0319 20:47:23.050306   14863 cache.go:227] Successfully downloaded all kic artifacts
I0319 20:47:23.050332   14863 start.go:360] acquireMachinesLock for minikube: {Name:mke55d43dfe5d63c3ff37bc55ed5be0675e3556c Clock:{} Delay:500ms Timeout:10m0s Cancel:<nil>}
I0319 20:47:23.050655   14863 start.go:364] duration metric: took 300.869µs to acquireMachinesLock for "minikube"
I0319 20:47:23.050680   14863 start.go:96] Skipping create...Using existing machine configuration
I0319 20:47:23.050692   14863 fix.go:54] fixHost starting: 
I0319 20:47:23.051240   14863 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0319 20:47:23.075341   14863 fix.go:112] recreateIfNeeded on minikube: state=Stopped err=<nil>
W0319 20:47:23.075360   14863 fix.go:138] unexpected machine state, will restart: <nil>
I0319 20:47:23.107180   14863 out.go:177] 🔄  Restarting existing docker container for "minikube" ...
I0319 20:47:23.152104   14863 cli_runner.go:164] Run: docker start minikube
I0319 20:47:24.752375   14863 cli_runner.go:217] Completed: docker start minikube: (1.600228458s)
I0319 20:47:24.752537   14863 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0319 20:47:24.775112   14863 kic.go:430] container "minikube" state is running.
I0319 20:47:24.775695   14863 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I0319 20:47:24.798580   14863 profile.go:143] Saving config to /home/abhay/.minikube/profiles/minikube/config.json ...
I0319 20:47:24.798970   14863 machine.go:93] provisionDockerMachine start ...
I0319 20:47:24.799076   14863 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0319 20:47:24.822265   14863 main.go:141] libmachine: Using SSH client type: native
I0319 20:47:24.847854   14863 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x8641c0] 0x866ea0 <nil>  [] 0s} 127.0.0.1 32772 <nil> <nil>}
I0319 20:47:24.847871   14863 main.go:141] libmachine: About to run SSH command:
hostname
I0319 20:47:24.849156   14863 main.go:141] libmachine: Error dialing TCP: ssh: handshake failed: read tcp 127.0.0.1:40878->127.0.0.1:32772: read: connection reset by peer
I0319 20:47:27.851590   14863 main.go:141] libmachine: Error dialing TCP: ssh: handshake failed: read tcp 127.0.0.1:40892->127.0.0.1:32772: read: connection reset by peer
I0319 20:47:30.853494   14863 main.go:141] libmachine: Error dialing TCP: ssh: handshake failed: read tcp 127.0.0.1:40906->127.0.0.1:32772: read: connection reset by peer
I0319 20:47:34.492686   14863 main.go:141] libmachine: SSH cmd err, output: <nil>: minikube

I0319 20:47:34.492730   14863 ubuntu.go:169] provisioning hostname "minikube"
I0319 20:47:34.492900   14863 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0319 20:47:34.518189   14863 main.go:141] libmachine: Using SSH client type: native
I0319 20:47:34.518413   14863 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x8641c0] 0x866ea0 <nil>  [] 0s} 127.0.0.1 32772 <nil> <nil>}
I0319 20:47:34.518422   14863 main.go:141] libmachine: About to run SSH command:
sudo hostname minikube && echo "minikube" | sudo tee /etc/hostname
I0319 20:47:35.378038   14863 main.go:141] libmachine: SSH cmd err, output: <nil>: minikube

I0319 20:47:35.378223   14863 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0319 20:47:35.406344   14863 main.go:141] libmachine: Using SSH client type: native
I0319 20:47:35.406570   14863 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x8641c0] 0x866ea0 <nil>  [] 0s} 127.0.0.1 32772 <nil> <nil>}
I0319 20:47:35.406585   14863 main.go:141] libmachine: About to run SSH command:

		if ! grep -xq '.*\sminikube' /etc/hosts; then
			if grep -xq '127.0.1.1\s.*' /etc/hosts; then
				sudo sed -i 's/^127.0.1.1\s.*/127.0.1.1 minikube/g' /etc/hosts;
			else 
				echo '127.0.1.1 minikube' | sudo tee -a /etc/hosts; 
			fi
		fi
I0319 20:47:35.561282   14863 main.go:141] libmachine: SSH cmd err, output: <nil>: 
I0319 20:47:35.561323   14863 ubuntu.go:175] set auth options {CertDir:/home/abhay/.minikube CaCertPath:/home/abhay/.minikube/certs/ca.pem CaPrivateKeyPath:/home/abhay/.minikube/certs/ca-key.pem CaCertRemotePath:/etc/docker/ca.pem ServerCertPath:/home/abhay/.minikube/machines/server.pem ServerKeyPath:/home/abhay/.minikube/machines/server-key.pem ClientKeyPath:/home/abhay/.minikube/certs/key.pem ServerCertRemotePath:/etc/docker/server.pem ServerKeyRemotePath:/etc/docker/server-key.pem ClientCertPath:/home/abhay/.minikube/certs/cert.pem ServerCertSANs:[] StorePath:/home/abhay/.minikube}
I0319 20:47:35.561356   14863 ubuntu.go:177] setting up certificates
I0319 20:47:35.561368   14863 provision.go:84] configureAuth start
I0319 20:47:35.561477   14863 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I0319 20:47:35.585330   14863 provision.go:143] copyHostCerts
I0319 20:47:35.607294   14863 exec_runner.go:144] found /home/abhay/.minikube/cert.pem, removing ...
I0319 20:47:35.621597   14863 exec_runner.go:203] rm: /home/abhay/.minikube/cert.pem
I0319 20:47:35.621773   14863 exec_runner.go:151] cp: /home/abhay/.minikube/certs/cert.pem --> /home/abhay/.minikube/cert.pem (1119 bytes)
I0319 20:47:35.662623   14863 exec_runner.go:144] found /home/abhay/.minikube/key.pem, removing ...
I0319 20:47:35.662667   14863 exec_runner.go:203] rm: /home/abhay/.minikube/key.pem
I0319 20:47:35.662849   14863 exec_runner.go:151] cp: /home/abhay/.minikube/certs/key.pem --> /home/abhay/.minikube/key.pem (1675 bytes)
I0319 20:47:35.663393   14863 exec_runner.go:144] found /home/abhay/.minikube/ca.pem, removing ...
I0319 20:47:35.663412   14863 exec_runner.go:203] rm: /home/abhay/.minikube/ca.pem
I0319 20:47:35.663508   14863 exec_runner.go:151] cp: /home/abhay/.minikube/certs/ca.pem --> /home/abhay/.minikube/ca.pem (1074 bytes)
I0319 20:47:35.664129   14863 provision.go:117] generating server cert: /home/abhay/.minikube/machines/server.pem ca-key=/home/abhay/.minikube/certs/ca.pem private-key=/home/abhay/.minikube/certs/ca-key.pem org=abhay.minikube san=[127.0.0.1 192.168.49.2 localhost minikube]
I0319 20:47:35.925328   14863 provision.go:177] copyRemoteCerts
I0319 20:47:35.925452   14863 ssh_runner.go:195] Run: sudo mkdir -p /etc/docker /etc/docker /etc/docker
I0319 20:47:35.925503   14863 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0319 20:47:35.948110   14863 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:32772 SSHKeyPath:/home/abhay/.minikube/machines/minikube/id_rsa Username:docker}
I0319 20:47:36.053895   14863 ssh_runner.go:362] scp /home/abhay/.minikube/certs/ca.pem --> /etc/docker/ca.pem (1074 bytes)
I0319 20:47:36.466761   14863 ssh_runner.go:362] scp /home/abhay/.minikube/machines/server.pem --> /etc/docker/server.pem (1176 bytes)
I0319 20:47:36.517557   14863 ssh_runner.go:362] scp /home/abhay/.minikube/machines/server-key.pem --> /etc/docker/server-key.pem (1679 bytes)
I0319 20:47:36.560863   14863 provision.go:87] duration metric: took 999.481361ms to configureAuth
I0319 20:47:36.560884   14863 ubuntu.go:193] setting minikube options for container-runtime
I0319 20:47:36.561063   14863 config.go:182] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.32.0
I0319 20:47:36.561136   14863 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0319 20:47:36.583333   14863 main.go:141] libmachine: Using SSH client type: native
I0319 20:47:36.583589   14863 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x8641c0] 0x866ea0 <nil>  [] 0s} 127.0.0.1 32772 <nil> <nil>}
I0319 20:47:36.583599   14863 main.go:141] libmachine: About to run SSH command:
df --output=fstype / | tail -n 1
I0319 20:47:36.776602   14863 main.go:141] libmachine: SSH cmd err, output: <nil>: overlay

I0319 20:47:36.776646   14863 ubuntu.go:71] root file system type: overlay
I0319 20:47:36.776878   14863 provision.go:314] Updating docker unit: /lib/systemd/system/docker.service ...
I0319 20:47:36.777081   14863 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0319 20:47:36.808465   14863 main.go:141] libmachine: Using SSH client type: native
I0319 20:47:36.808697   14863 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x8641c0] 0x866ea0 <nil>  [] 0s} 127.0.0.1 32772 <nil> <nil>}
I0319 20:47:36.808787   14863 main.go:141] libmachine: About to run SSH command:
sudo mkdir -p /lib/systemd/system && printf %s "[Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.com
BindsTo=containerd.service
After=network-online.target firewalld.service containerd.service
Wants=network-online.target
Requires=docker.socket
StartLimitBurst=3
StartLimitIntervalSec=60

[Service]
Type=notify
Restart=on-failure



# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
# The base configuration already specifies an 'ExecStart=...' command. The first directive
# here is to clear out that command inherited from the base configuration. Without this,
# the command from the base configuration and the command specified here are treated as
# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
# will catch this invalid input and refuse to start the service with an error like:
#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.

# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
ExecStart=
ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --default-ulimit=nofile=1048576:1048576 --tlsverify --tlscacert /etc/docker/ca.pem --tlscert /etc/docker/server.pem --tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 
ExecReload=/bin/kill -s HUP \$MAINPID

# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity

# Uncomment TasksMax if your systemd version supports it.
# Only systemd 226 and above support this version.
TasksMax=infinity
TimeoutStartSec=0

# set delegate yes so that systemd does not reset the cgroups of docker containers
Delegate=yes

# kill only the docker process, not all processes in the cgroup
KillMode=process

[Install]
WantedBy=multi-user.target
" | sudo tee /lib/systemd/system/docker.service.new
I0319 20:47:36.988115   14863 main.go:141] libmachine: SSH cmd err, output: <nil>: [Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.com
BindsTo=containerd.service
After=network-online.target firewalld.service containerd.service
Wants=network-online.target
Requires=docker.socket
StartLimitBurst=3
StartLimitIntervalSec=60

[Service]
Type=notify
Restart=on-failure



# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
# The base configuration already specifies an 'ExecStart=...' command. The first directive
# here is to clear out that command inherited from the base configuration. Without this,
# the command from the base configuration and the command specified here are treated as
# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
# will catch this invalid input and refuse to start the service with an error like:
#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.

# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
ExecStart=
ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --default-ulimit=nofile=1048576:1048576 --tlsverify --tlscacert /etc/docker/ca.pem --tlscert /etc/docker/server.pem --tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 
ExecReload=/bin/kill -s HUP $MAINPID

# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity

# Uncomment TasksMax if your systemd version supports it.
# Only systemd 226 and above support this version.
TasksMax=infinity
TimeoutStartSec=0

# set delegate yes so that systemd does not reset the cgroups of docker containers
Delegate=yes

# kill only the docker process, not all processes in the cgroup
KillMode=process

[Install]
WantedBy=multi-user.target

I0319 20:47:36.988227   14863 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0319 20:47:37.012526   14863 main.go:141] libmachine: Using SSH client type: native
I0319 20:47:37.012778   14863 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x8641c0] 0x866ea0 <nil>  [] 0s} 127.0.0.1 32772 <nil> <nil>}
I0319 20:47:37.012797   14863 main.go:141] libmachine: About to run SSH command:
sudo diff -u /lib/systemd/system/docker.service /lib/systemd/system/docker.service.new || { sudo mv /lib/systemd/system/docker.service.new /lib/systemd/system/docker.service; sudo systemctl -f daemon-reload && sudo systemctl -f enable docker && sudo systemctl -f restart docker; }
I0319 20:47:37.172407   14863 main.go:141] libmachine: SSH cmd err, output: <nil>: 
I0319 20:47:37.172420   14863 machine.go:96] duration metric: took 12.373441943s to provisionDockerMachine
I0319 20:47:37.172432   14863 start.go:293] postStartSetup for "minikube" (driver="docker")
I0319 20:47:37.172443   14863 start.go:322] creating required directories: [/etc/kubernetes/addons /etc/kubernetes/manifests /var/tmp/minikube /var/lib/minikube /var/lib/minikube/certs /var/lib/minikube/images /var/lib/minikube/binaries /tmp/gvisor /usr/share/ca-certificates /etc/ssl/certs]
I0319 20:47:37.172542   14863 ssh_runner.go:195] Run: sudo mkdir -p /etc/kubernetes/addons /etc/kubernetes/manifests /var/tmp/minikube /var/lib/minikube /var/lib/minikube/certs /var/lib/minikube/images /var/lib/minikube/binaries /tmp/gvisor /usr/share/ca-certificates /etc/ssl/certs
I0319 20:47:37.172609   14863 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0319 20:47:37.196442   14863 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:32772 SSHKeyPath:/home/abhay/.minikube/machines/minikube/id_rsa Username:docker}
I0319 20:47:37.364073   14863 ssh_runner.go:195] Run: cat /etc/os-release
I0319 20:47:37.372037   14863 main.go:141] libmachine: Couldn't set key VERSION_CODENAME, no corresponding struct field found
I0319 20:47:37.372081   14863 main.go:141] libmachine: Couldn't set key PRIVACY_POLICY_URL, no corresponding struct field found
I0319 20:47:37.372097   14863 main.go:141] libmachine: Couldn't set key UBUNTU_CODENAME, no corresponding struct field found
I0319 20:47:37.372106   14863 info.go:137] Remote host: Ubuntu 22.04.5 LTS
I0319 20:47:37.372121   14863 filesync.go:126] Scanning /home/abhay/.minikube/addons for local assets ...
I0319 20:47:37.418506   14863 filesync.go:126] Scanning /home/abhay/.minikube/files for local assets ...
I0319 20:47:37.451786   14863 start.go:296] duration metric: took 279.335679ms for postStartSetup
I0319 20:47:37.451974   14863 ssh_runner.go:195] Run: sh -c "df -h /var | awk 'NR==2{print $5}'"
I0319 20:47:37.452073   14863 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0319 20:47:37.478186   14863 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:32772 SSHKeyPath:/home/abhay/.minikube/machines/minikube/id_rsa Username:docker}
I0319 20:47:37.640323   14863 ssh_runner.go:195] Run: sh -c "df -BG /var | awk 'NR==2{print $4}'"
I0319 20:47:37.650846   14863 fix.go:56] duration metric: took 14.600143458s for fixHost
I0319 20:47:37.650867   14863 start.go:83] releasing machines lock for "minikube", held for 14.600200459s
I0319 20:47:37.650990   14863 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I0319 20:47:37.679177   14863 ssh_runner.go:195] Run: cat /version.json
I0319 20:47:37.679255   14863 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0319 20:47:37.679265   14863 ssh_runner.go:195] Run: curl -sS -m 2 https://registry.k8s.io/
I0319 20:47:37.679371   14863 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0319 20:47:37.703012   14863 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:32772 SSHKeyPath:/home/abhay/.minikube/machines/minikube/id_rsa Username:docker}
I0319 20:47:37.704261   14863 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:32772 SSHKeyPath:/home/abhay/.minikube/machines/minikube/id_rsa Username:docker}
I0319 20:47:42.007381   14863 ssh_runner.go:235] Completed: cat /version.json: (4.328171546s)
I0319 20:47:42.007439   14863 ssh_runner.go:235] Completed: curl -sS -m 2 https://registry.k8s.io/: (4.328151523s)
W0319 20:47:42.007487   14863 start.go:867] [curl -sS -m 2 https://registry.k8s.io/] failed: curl -sS -m 2 https://registry.k8s.io/: Process exited with status 28
stdout:

stderr:
curl: (28) Resolving timed out after 2000 milliseconds
I0319 20:47:42.007656   14863 ssh_runner.go:195] Run: systemctl --version
I0319 20:47:42.194930   14863 ssh_runner.go:195] Run: sh -c "stat /etc/cni/net.d/*loopback.conf*"
I0319 20:47:42.202862   14863 ssh_runner.go:195] Run: sudo find /etc/cni/net.d -maxdepth 1 -type f -name *loopback.conf* -not -name *.mk_disabled -exec sh -c "grep -q loopback {} && ( grep -q name {} || sudo sed -i '/"type": "loopback"/i \ \ \ \ "name": "loopback",' {} ) && sudo sed -i 's|"cniVersion": ".*"|"cniVersion": "1.0.0"|g' {}" ;
I0319 20:47:42.396200   14863 cni.go:230] loopback cni configuration patched: "/etc/cni/net.d/*loopback.conf*" found
I0319 20:47:42.396373   14863 ssh_runner.go:195] Run: sudo find /etc/cni/net.d -maxdepth 1 -type f ( ( -name *bridge* -or -name *podman* ) -and -not -name *.mk_disabled ) -printf "%p, " -exec sh -c "sudo mv {} {}.mk_disabled" ;
I0319 20:47:42.414194   14863 cni.go:259] no active bridge cni configs found in "/etc/cni/net.d" - nothing to disable
I0319 20:47:42.414214   14863 start.go:495] detecting cgroup driver to use...
I0319 20:47:42.414246   14863 detect.go:190] detected "systemd" cgroup driver on host os
I0319 20:47:42.414427   14863 ssh_runner.go:195] Run: /bin/bash -c "sudo mkdir -p /etc && printf %s "runtime-endpoint: unix:///run/containerd/containerd.sock
" | sudo tee /etc/crictl.yaml"
I0319 20:47:42.446705   14863 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)sandbox_image = .*$|\1sandbox_image = "registry.k8s.io/pause:3.10"|' /etc/containerd/config.toml"
I0319 20:47:42.518568   14863 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)restrict_oom_score_adj = .*$|\1restrict_oom_score_adj = false|' /etc/containerd/config.toml"
I0319 20:47:42.541118   14863 containerd.go:146] configuring containerd to use "systemd" as cgroup driver...
I0319 20:47:42.541240   14863 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)SystemdCgroup = .*$|\1SystemdCgroup = true|g' /etc/containerd/config.toml"
I0319 20:47:42.567454   14863 ssh_runner.go:195] Run: sh -c "sudo sed -i 's|"io.containerd.runtime.v1.linux"|"io.containerd.runc.v2"|g' /etc/containerd/config.toml"
I0319 20:47:42.584640   14863 ssh_runner.go:195] Run: sh -c "sudo sed -i '/systemd_cgroup/d' /etc/containerd/config.toml"
I0319 20:47:42.612417   14863 ssh_runner.go:195] Run: sh -c "sudo sed -i 's|"io.containerd.runc.v1"|"io.containerd.runc.v2"|g' /etc/containerd/config.toml"
I0319 20:47:42.627237   14863 ssh_runner.go:195] Run: sh -c "sudo rm -rf /etc/cni/net.mk"
I0319 20:47:42.640619   14863 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)conf_dir = .*$|\1conf_dir = "/etc/cni/net.d"|g' /etc/containerd/config.toml"
I0319 20:47:42.691528   14863 ssh_runner.go:195] Run: sh -c "sudo sed -i '/^ *enable_unprivileged_ports = .*/d' /etc/containerd/config.toml"
I0319 20:47:42.710890   14863 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)\[plugins."io.containerd.grpc.v1.cri"\]|&\n\1  enable_unprivileged_ports = true|' /etc/containerd/config.toml"
I0319 20:47:42.727284   14863 ssh_runner.go:195] Run: sudo sysctl net.bridge.bridge-nf-call-iptables
I0319 20:47:42.777909   14863 ssh_runner.go:195] Run: sudo sh -c "echo 1 > /proc/sys/net/ipv4/ip_forward"
I0319 20:47:42.862406   14863 ssh_runner.go:195] Run: sudo systemctl daemon-reload
W0319 20:47:42.953343   14863 out.go:270] ❗  Failing to connect to https://registry.k8s.io/ from inside the minikube container
W0319 20:47:42.953398   14863 out.go:270] 💡  To pull new external images, you may need to configure a proxy: https://minikube.sigs.k8s.io/docs/reference/networking/proxy/
I0319 20:47:43.095663   14863 ssh_runner.go:195] Run: sudo systemctl restart containerd
I0319 20:47:43.589584   14863 start.go:495] detecting cgroup driver to use...
I0319 20:47:43.589630   14863 detect.go:190] detected "systemd" cgroup driver on host os
I0319 20:47:43.589786   14863 ssh_runner.go:195] Run: sudo systemctl cat docker.service
I0319 20:47:43.607710   14863 cruntime.go:279] skipping containerd shutdown because we are bound to it
I0319 20:47:43.607793   14863 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service crio
I0319 20:47:43.867388   14863 ssh_runner.go:195] Run: /bin/bash -c "sudo mkdir -p /etc && printf %s "runtime-endpoint: unix:///var/run/cri-dockerd.sock
" | sudo tee /etc/crictl.yaml"
I0319 20:47:43.909432   14863 ssh_runner.go:195] Run: which cri-dockerd
I0319 20:47:43.915515   14863 ssh_runner.go:195] Run: sudo mkdir -p /etc/systemd/system/cri-docker.service.d
I0319 20:47:43.933991   14863 ssh_runner.go:362] scp memory --> /etc/systemd/system/cri-docker.service.d/10-cni.conf (190 bytes)
I0319 20:47:43.963968   14863 ssh_runner.go:195] Run: sudo systemctl unmask docker.service
I0319 20:47:44.210761   14863 ssh_runner.go:195] Run: sudo systemctl enable docker.socket
I0319 20:47:44.324575   14863 docker.go:574] configuring docker to use "systemd" as cgroup driver...
I0319 20:47:44.324709   14863 ssh_runner.go:362] scp memory --> /etc/docker/daemon.json (129 bytes)
I0319 20:47:44.350392   14863 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0319 20:47:44.458758   14863 ssh_runner.go:195] Run: sudo systemctl restart docker
I0319 20:47:47.513056   14863 ssh_runner.go:235] Completed: sudo systemctl restart docker: (3.054262907s)
I0319 20:47:47.513201   14863 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service cri-docker.socket
I0319 20:47:47.537478   14863 ssh_runner.go:195] Run: sudo systemctl stop cri-docker.socket
I0319 20:47:47.558681   14863 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service cri-docker.service
I0319 20:47:47.582587   14863 ssh_runner.go:195] Run: sudo systemctl unmask cri-docker.socket
I0319 20:47:47.694944   14863 ssh_runner.go:195] Run: sudo systemctl enable cri-docker.socket
I0319 20:47:47.809506   14863 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0319 20:47:47.913562   14863 ssh_runner.go:195] Run: sudo systemctl restart cri-docker.socket
I0319 20:47:47.949350   14863 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service cri-docker.service
I0319 20:47:47.969302   14863 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0319 20:47:48.071632   14863 ssh_runner.go:195] Run: sudo systemctl restart cri-docker.service
I0319 20:47:50.143156   14863 ssh_runner.go:235] Completed: sudo systemctl restart cri-docker.service: (2.07149438s)
I0319 20:47:50.143178   14863 start.go:542] Will wait 60s for socket path /var/run/cri-dockerd.sock
I0319 20:47:50.143303   14863 ssh_runner.go:195] Run: stat /var/run/cri-dockerd.sock
I0319 20:47:50.149787   14863 start.go:563] Will wait 60s for crictl version
I0319 20:47:50.149887   14863 ssh_runner.go:195] Run: which crictl
I0319 20:47:50.155453   14863 ssh_runner.go:195] Run: sudo /usr/bin/crictl version
I0319 20:47:50.944135   14863 start.go:579] Version:  0.1.0
RuntimeName:  docker
RuntimeVersion:  27.4.1
RuntimeApiVersion:  v1
I0319 20:47:50.944289   14863 ssh_runner.go:195] Run: docker version --format {{.Server.Version}}
I0319 20:47:51.813385   14863 ssh_runner.go:195] Run: docker version --format {{.Server.Version}}
I0319 20:47:51.871596   14863 out.go:235] 🐳  Preparing Kubernetes v1.32.0 on Docker 27.4.1 ...
I0319 20:47:51.871774   14863 cli_runner.go:164] Run: docker network inspect minikube --format "{"Name": "{{.Name}}","Driver": "{{.Driver}}","Subnet": "{{range .IPAM.Config}}{{.Subnet}}{{end}}","Gateway": "{{range .IPAM.Config}}{{.Gateway}}{{end}}","MTU": {{if (index .Options "com.docker.network.driver.mtu")}}{{(index .Options "com.docker.network.driver.mtu")}}{{else}}0{{end}}, "ContainerIPs": [{{range $k,$v := .Containers }}"{{$v.IPv4Address}}",{{end}}]}"
I0319 20:47:51.904410   14863 ssh_runner.go:195] Run: grep 192.168.49.1	host.minikube.internal$ /etc/hosts
I0319 20:47:51.912195   14863 ssh_runner.go:195] Run: /bin/bash -c "{ grep -v $'\thost.minikube.internal$' "/etc/hosts"; echo "192.168.49.1	host.minikube.internal"; } > /tmp/h.$$; sudo cp /tmp/h.$$ "/etc/hosts""
I0319 20:47:51.938343   14863 kubeadm.go:883] updating cluster {Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.46@sha256:fd2d445ddcc33ebc5c6b68a17e6219ea207ce63c005095ea1525296da2d1a279 Memory:3700 CPUs:2 DiskSize:20000 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.32.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.32.0 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[default-storageclass:true storage-provisioner:true] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:/home/abhay:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s} ...
I0319 20:47:51.938469   14863 preload.go:131] Checking if preload exists for k8s version v1.32.0 and runtime docker
I0319 20:47:51.938557   14863 ssh_runner.go:195] Run: docker images --format {{.Repository}}:{{.Tag}}
I0319 20:47:51.966930   14863 docker.go:689] Got preloaded images: -- stdout --
<none>:<none>
abhay4617/authentication:latest
registry.k8s.io/kube-apiserver:v1.32.0
registry.k8s.io/kube-controller-manager:v1.32.0
registry.k8s.io/kube-scheduler:v1.32.0
registry.k8s.io/kube-proxy:v1.32.0
python:3.10-slim-bullseye
registry.k8s.io/etcd:3.5.16-0
registry.k8s.io/coredns/coredns:v1.11.3
registry.k8s.io/pause:3.10
gcr.io/k8s-minikube/storage-provisioner:v5

-- /stdout --
I0319 20:47:51.966945   14863 docker.go:619] Images already preloaded, skipping extraction
I0319 20:47:51.967033   14863 ssh_runner.go:195] Run: docker images --format {{.Repository}}:{{.Tag}}
I0319 20:47:52.001750   14863 docker.go:689] Got preloaded images: -- stdout --
<none>:<none>
abhay4617/authentication:latest
registry.k8s.io/kube-apiserver:v1.32.0
registry.k8s.io/kube-scheduler:v1.32.0
registry.k8s.io/kube-controller-manager:v1.32.0
registry.k8s.io/kube-proxy:v1.32.0
python:3.10-slim-bullseye
registry.k8s.io/etcd:3.5.16-0
registry.k8s.io/coredns/coredns:v1.11.3
registry.k8s.io/pause:3.10
gcr.io/k8s-minikube/storage-provisioner:v5

-- /stdout --
I0319 20:47:52.001765   14863 cache_images.go:84] Images are preloaded, skipping loading
I0319 20:47:52.001801   14863 kubeadm.go:934] updating node { 192.168.49.2 8443 v1.32.0 docker true true} ...
I0319 20:47:52.001976   14863 kubeadm.go:946] kubelet [Unit]
Wants=docker.socket

[Service]
ExecStart=
ExecStart=/var/lib/minikube/binaries/v1.32.0/kubelet --bootstrap-kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf --config=/var/lib/kubelet/config.yaml --hostname-override=minikube --kubeconfig=/etc/kubernetes/kubelet.conf --node-ip=192.168.49.2

[Install]
 config:
{KubernetesVersion:v1.32.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:}
I0319 20:47:52.002076   14863 ssh_runner.go:195] Run: docker info --format {{.CgroupDriver}}
I0319 20:47:53.593893   14863 ssh_runner.go:235] Completed: docker info --format {{.CgroupDriver}}: (1.591792226s)
I0319 20:47:53.594011   14863 cni.go:84] Creating CNI manager for ""
I0319 20:47:53.594030   14863 cni.go:158] "docker" driver + "docker" container runtime found on kubernetes v1.24+, recommending bridge
I0319 20:47:53.594118   14863 kubeadm.go:84] Using pod CIDR: 10.244.0.0/16
I0319 20:47:53.594148   14863 kubeadm.go:189] kubeadm options: {CertDir:/var/lib/minikube/certs ServiceCIDR:10.96.0.0/12 PodSubnet:10.244.0.0/16 AdvertiseAddress:192.168.49.2 APIServerPort:8443 KubernetesVersion:v1.32.0 EtcdDataDir:/var/lib/minikube/etcd EtcdExtraArgs:map[] ClusterName:minikube NodeName:minikube DNSDomain:cluster.local CRISocket:/var/run/cri-dockerd.sock ImageRepository: ComponentOptions:[{Component:apiServer ExtraArgs:map[enable-admission-plugins:NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota] Pairs:map[certSANs:["127.0.0.1", "localhost", "192.168.49.2"]]} {Component:controllerManager ExtraArgs:map[allocate-node-cidrs:true leader-elect:false] Pairs:map[]} {Component:scheduler ExtraArgs:map[leader-elect:false] Pairs:map[]}] FeatureArgs:map[] NodeIP:192.168.49.2 CgroupDriver:systemd ClientCAFile:/var/lib/minikube/certs/ca.crt StaticPodPath:/etc/kubernetes/manifests ControlPlaneAddress:control-plane.minikube.internal KubeProxyOptions:map[] ResolvConfSearchRegression:false KubeletConfigOpts:map[containerRuntimeEndpoint:unix:///var/run/cri-dockerd.sock hairpinMode:hairpin-veth runtimeRequestTimeout:15m] PrependCriSocketUnix:true}
I0319 20:47:53.594304   14863 kubeadm.go:195] kubeadm config:
apiVersion: kubeadm.k8s.io/v1beta4
kind: InitConfiguration
localAPIEndpoint:
  advertiseAddress: 192.168.49.2
  bindPort: 8443
bootstrapTokens:
  - groups:
      - system:bootstrappers:kubeadm:default-node-token
    ttl: 24h0m0s
    usages:
      - signing
      - authentication
nodeRegistration:
  criSocket: unix:///var/run/cri-dockerd.sock
  name: "minikube"
  kubeletExtraArgs:
    - name: "node-ip"
      value: "192.168.49.2"
  taints: []
---
apiVersion: kubeadm.k8s.io/v1beta4
kind: ClusterConfiguration
apiServer:
  certSANs: ["127.0.0.1", "localhost", "192.168.49.2"]
  extraArgs:
    - name: "enable-admission-plugins"
      value: "NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota"
controllerManager:
  extraArgs:
    - name: "allocate-node-cidrs"
      value: "true"
    - name: "leader-elect"
      value: "false"
scheduler:
  extraArgs:
    - name: "leader-elect"
      value: "false"
certificatesDir: /var/lib/minikube/certs
clusterName: mk
controlPlaneEndpoint: control-plane.minikube.internal:8443
etcd:
  local:
    dataDir: /var/lib/minikube/etcd
    extraArgs:
      - name: "proxy-refresh-interval"
        value: "70000"
kubernetesVersion: v1.32.0
networking:
  dnsDomain: cluster.local
  podSubnet: "10.244.0.0/16"
  serviceSubnet: 10.96.0.0/12
---
apiVersion: kubelet.config.k8s.io/v1beta1
kind: KubeletConfiguration
authentication:
  x509:
    clientCAFile: /var/lib/minikube/certs/ca.crt
cgroupDriver: systemd
containerRuntimeEndpoint: unix:///var/run/cri-dockerd.sock
hairpinMode: hairpin-veth
runtimeRequestTimeout: 15m
clusterDomain: "cluster.local"
# disable disk resource management by default
imageGCHighThresholdPercent: 100
evictionHard:
  nodefs.available: "0%"
  nodefs.inodesFree: "0%"
  imagefs.available: "0%"
failSwapOn: false
staticPodPath: /etc/kubernetes/manifests
---
apiVersion: kubeproxy.config.k8s.io/v1alpha1
kind: KubeProxyConfiguration
clusterCIDR: "10.244.0.0/16"
metricsBindAddress: 0.0.0.0:10249
conntrack:
  maxPerCore: 0
# Skip setting "net.netfilter.nf_conntrack_tcp_timeout_established"
  tcpEstablishedTimeout: 0s
# Skip setting "net.netfilter.nf_conntrack_tcp_timeout_close"
  tcpCloseWaitTimeout: 0s

I0319 20:47:53.594414   14863 ssh_runner.go:195] Run: sudo ls /var/lib/minikube/binaries/v1.32.0
I0319 20:47:53.637325   14863 binaries.go:44] Found k8s binaries, skipping transfer
I0319 20:47:53.637413   14863 ssh_runner.go:195] Run: sudo mkdir -p /etc/systemd/system/kubelet.service.d /lib/systemd/system /var/tmp/minikube
I0319 20:47:53.651929   14863 ssh_runner.go:362] scp memory --> /etc/systemd/system/kubelet.service.d/10-kubeadm.conf (307 bytes)
I0319 20:47:53.679622   14863 ssh_runner.go:362] scp memory --> /lib/systemd/system/kubelet.service (352 bytes)
I0319 20:47:53.720461   14863 ssh_runner.go:362] scp memory --> /var/tmp/minikube/kubeadm.yaml.new (2285 bytes)
I0319 20:47:53.759014   14863 ssh_runner.go:195] Run: grep 192.168.49.2	control-plane.minikube.internal$ /etc/hosts
I0319 20:47:53.764145   14863 ssh_runner.go:195] Run: /bin/bash -c "{ grep -v $'\tcontrol-plane.minikube.internal$' "/etc/hosts"; echo "192.168.49.2	control-plane.minikube.internal"; } > /tmp/h.$$; sudo cp /tmp/h.$$ "/etc/hosts""
I0319 20:47:53.780645   14863 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0319 20:47:53.893430   14863 ssh_runner.go:195] Run: sudo systemctl start kubelet
I0319 20:47:53.921824   14863 certs.go:68] Setting up /home/abhay/.minikube/profiles/minikube for IP: 192.168.49.2
I0319 20:47:53.921836   14863 certs.go:194] generating shared ca certs ...
I0319 20:47:53.921854   14863 certs.go:226] acquiring lock for ca certs: {Name:mk772364c0c520130bc925f397489b1a415b8365 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0319 20:47:53.922161   14863 certs.go:235] skipping valid "minikubeCA" ca cert: /home/abhay/.minikube/ca.key
I0319 20:47:53.966682   14863 certs.go:235] skipping valid "proxyClientCA" ca cert: /home/abhay/.minikube/proxy-client-ca.key
I0319 20:47:53.966733   14863 certs.go:256] generating profile certs ...
I0319 20:47:53.966923   14863 certs.go:359] skipping valid signed profile cert regeneration for "minikube-user": /home/abhay/.minikube/profiles/minikube/client.key
I0319 20:47:53.967327   14863 certs.go:359] skipping valid signed profile cert regeneration for "minikube": /home/abhay/.minikube/profiles/minikube/apiserver.key.7fb57e3c
I0319 20:47:53.967556   14863 certs.go:359] skipping valid signed profile cert regeneration for "aggregator": /home/abhay/.minikube/profiles/minikube/proxy-client.key
I0319 20:47:53.967755   14863 certs.go:484] found cert: /home/abhay/.minikube/certs/ca-key.pem (1675 bytes)
I0319 20:47:53.967790   14863 certs.go:484] found cert: /home/abhay/.minikube/certs/ca.pem (1074 bytes)
I0319 20:47:53.967825   14863 certs.go:484] found cert: /home/abhay/.minikube/certs/cert.pem (1119 bytes)
I0319 20:47:53.967854   14863 certs.go:484] found cert: /home/abhay/.minikube/certs/key.pem (1675 bytes)
I0319 20:47:53.968580   14863 ssh_runner.go:362] scp /home/abhay/.minikube/ca.crt --> /var/lib/minikube/certs/ca.crt (1111 bytes)
I0319 20:47:54.009592   14863 ssh_runner.go:362] scp /home/abhay/.minikube/ca.key --> /var/lib/minikube/certs/ca.key (1679 bytes)
I0319 20:47:54.050325   14863 ssh_runner.go:362] scp /home/abhay/.minikube/proxy-client-ca.crt --> /var/lib/minikube/certs/proxy-client-ca.crt (1119 bytes)
I0319 20:47:54.085284   14863 ssh_runner.go:362] scp /home/abhay/.minikube/proxy-client-ca.key --> /var/lib/minikube/certs/proxy-client-ca.key (1675 bytes)
I0319 20:47:54.128577   14863 ssh_runner.go:362] scp /home/abhay/.minikube/profiles/minikube/apiserver.crt --> /var/lib/minikube/certs/apiserver.crt (1411 bytes)
I0319 20:47:54.164247   14863 ssh_runner.go:362] scp /home/abhay/.minikube/profiles/minikube/apiserver.key --> /var/lib/minikube/certs/apiserver.key (1675 bytes)
I0319 20:47:54.206186   14863 ssh_runner.go:362] scp /home/abhay/.minikube/profiles/minikube/proxy-client.crt --> /var/lib/minikube/certs/proxy-client.crt (1147 bytes)
I0319 20:47:54.245582   14863 ssh_runner.go:362] scp /home/abhay/.minikube/profiles/minikube/proxy-client.key --> /var/lib/minikube/certs/proxy-client.key (1675 bytes)
I0319 20:47:54.280205   14863 ssh_runner.go:362] scp /home/abhay/.minikube/ca.crt --> /usr/share/ca-certificates/minikubeCA.pem (1111 bytes)
I0319 20:47:54.385713   14863 ssh_runner.go:362] scp memory --> /var/lib/minikube/kubeconfig (738 bytes)
I0319 20:47:54.431501   14863 ssh_runner.go:195] Run: openssl version
I0319 20:47:54.578632   14863 ssh_runner.go:195] Run: sudo /bin/bash -c "test -s /usr/share/ca-certificates/minikubeCA.pem && ln -fs /usr/share/ca-certificates/minikubeCA.pem /etc/ssl/certs/minikubeCA.pem"
I0319 20:47:54.749341   14863 ssh_runner.go:195] Run: ls -la /usr/share/ca-certificates/minikubeCA.pem
I0319 20:47:54.755924   14863 certs.go:528] hashing: -rw-r--r-- 1 root root 1111 Mar 18 16:51 /usr/share/ca-certificates/minikubeCA.pem
I0319 20:47:54.756017   14863 ssh_runner.go:195] Run: openssl x509 -hash -noout -in /usr/share/ca-certificates/minikubeCA.pem
I0319 20:47:54.776904   14863 ssh_runner.go:195] Run: sudo /bin/bash -c "test -L /etc/ssl/certs/b5213941.0 || ln -fs /etc/ssl/certs/minikubeCA.pem /etc/ssl/certs/b5213941.0"
I0319 20:47:54.792571   14863 ssh_runner.go:195] Run: stat /var/lib/minikube/certs/apiserver-kubelet-client.crt
I0319 20:47:54.799518   14863 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/apiserver-etcd-client.crt -checkend 86400
I0319 20:47:54.833838   14863 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/apiserver-kubelet-client.crt -checkend 86400
I0319 20:47:54.844240   14863 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/etcd/server.crt -checkend 86400
I0319 20:47:54.963068   14863 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/etcd/healthcheck-client.crt -checkend 86400
I0319 20:47:54.976125   14863 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/etcd/peer.crt -checkend 86400
I0319 20:47:54.985376   14863 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/front-proxy-client.crt -checkend 86400
I0319 20:47:54.996517   14863 kubeadm.go:392] StartCluster: {Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.46@sha256:fd2d445ddcc33ebc5c6b68a17e6219ea207ce63c005095ea1525296da2d1a279 Memory:3700 CPUs:2 DiskSize:20000 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.32.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.32.0 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[default-storageclass:true storage-provisioner:true] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:/home/abhay:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s}
I0319 20:47:54.996752   14863 ssh_runner.go:195] Run: docker ps --filter status=paused --filter=name=k8s_.*_(kube-system)_ --format={{.ID}}
I0319 20:47:55.030901   14863 ssh_runner.go:195] Run: sudo ls /var/lib/kubelet/kubeadm-flags.env /var/lib/kubelet/config.yaml /var/lib/minikube/etcd
I0319 20:47:55.062951   14863 kubeadm.go:408] found existing configuration files, will attempt cluster restart
I0319 20:47:55.062962   14863 kubeadm.go:593] restartPrimaryControlPlane start ...
I0319 20:47:55.063041   14863 ssh_runner.go:195] Run: sudo test -d /data/minikube
I0319 20:47:55.113144   14863 kubeadm.go:130] /data/minikube skipping compat symlinks: sudo test -d /data/minikube: Process exited with status 1
stdout:

stderr:
I0319 20:47:55.124666   14863 kubeconfig.go:125] found "minikube" server: "https://192.168.49.2:8443"
I0319 20:47:55.489412   14863 ssh_runner.go:195] Run: sudo diff -u /var/tmp/minikube/kubeadm.yaml /var/tmp/minikube/kubeadm.yaml.new
I0319 20:47:55.508262   14863 kubeadm.go:630] The running cluster does not require reconfiguration: 192.168.49.2
I0319 20:47:55.508286   14863 kubeadm.go:597] duration metric: took 445.318601ms to restartPrimaryControlPlane
I0319 20:47:55.508296   14863 kubeadm.go:394] duration metric: took 511.789384ms to StartCluster
I0319 20:47:55.508313   14863 settings.go:142] acquiring lock: {Name:mk12140390357e22b33ea2cb9d9b893aa10e9991 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0319 20:47:55.508463   14863 settings.go:150] Updating kubeconfig:  /home/abhay/.kube/config
I0319 20:47:55.509298   14863 lock.go:35] WriteFile acquiring /home/abhay/.kube/config: {Name:mk7eaeb990f3e70f38d8b4096d4b93fe2631e92c Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0319 20:47:55.509671   14863 start.go:235] Will wait 6m0s for node &{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.32.0 ContainerRuntime:docker ControlPlane:true Worker:true}
I0319 20:47:55.509945   14863 config.go:182] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.32.0
I0319 20:47:55.509974   14863 addons.go:511] enable addons start: toEnable=map[ambassador:false amd-gpu-device-plugin:false auto-pause:false cloud-spanner:false csi-hostpath-driver:false dashboard:false default-storageclass:true efk:false freshpod:false gcp-auth:false gvisor:false headlamp:false inaccel:false ingress:false ingress-dns:false inspektor-gadget:false istio:false istio-provisioner:false kong:false kubeflow:false kubevirt:false logviewer:false metallb:false metrics-server:false nvidia-device-plugin:false nvidia-driver-installer:false nvidia-gpu-device-plugin:false olm:false pod-security-policy:false portainer:false registry:false registry-aliases:false registry-creds:false storage-provisioner:true storage-provisioner-gluster:false storage-provisioner-rancher:false volcano:false volumesnapshots:false yakd:false]
I0319 20:47:55.510059   14863 addons.go:69] Setting storage-provisioner=true in profile "minikube"
I0319 20:47:55.510091   14863 addons.go:238] Setting addon storage-provisioner=true in "minikube"
W0319 20:47:55.510099   14863 addons.go:247] addon storage-provisioner should already be in state true
I0319 20:47:55.510129   14863 host.go:66] Checking if "minikube" exists ...
I0319 20:47:55.510887   14863 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0319 20:47:55.510969   14863 addons.go:69] Setting default-storageclass=true in profile "minikube"
I0319 20:47:55.510986   14863 addons_storage_classes.go:33] enableOrDisableStorageClasses default-storageclass=true on "minikube"
I0319 20:47:55.511451   14863 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0319 20:47:55.545282   14863 addons.go:238] Setting addon default-storageclass=true in "minikube"
W0319 20:47:55.545293   14863 addons.go:247] addon default-storageclass should already be in state true
I0319 20:47:55.545315   14863 host.go:66] Checking if "minikube" exists ...
I0319 20:47:55.545742   14863 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0319 20:47:55.566957   14863 addons.go:435] installing /etc/kubernetes/addons/storageclass.yaml
I0319 20:47:55.566971   14863 ssh_runner.go:362] scp storageclass/storageclass.yaml --> /etc/kubernetes/addons/storageclass.yaml (271 bytes)
I0319 20:47:55.567067   14863 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0319 20:47:55.586185   14863 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:32772 SSHKeyPath:/home/abhay/.minikube/machines/minikube/id_rsa Username:docker}
I0319 20:47:55.649004   14863 out.go:177]     ▪ Using image gcr.io/k8s-minikube/storage-provisioner:v5
I0319 20:47:55.649002   14863 out.go:177] 🔎  Verifying Kubernetes components...
I0319 20:47:55.693964   14863 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0319 20:47:55.694093   14863 addons.go:435] installing /etc/kubernetes/addons/storage-provisioner.yaml
I0319 20:47:55.694101   14863 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/storage-provisioner.yaml (2676 bytes)
I0319 20:47:55.694199   14863 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0319 20:47:55.718511   14863 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:32772 SSHKeyPath:/home/abhay/.minikube/machines/minikube/id_rsa Username:docker}
I0319 20:47:55.751346   14863 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.32.0/kubectl apply -f /etc/kubernetes/addons/storageclass.yaml
I0319 20:47:55.806059   14863 ssh_runner.go:195] Run: sudo systemctl start kubelet
I0319 20:47:55.845288   14863 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.32.0/kubectl apply -f /etc/kubernetes/addons/storage-provisioner.yaml
I0319 20:47:59.498348   14863 ssh_runner.go:235] Completed: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.32.0/kubectl apply -f /etc/kubernetes/addons/storageclass.yaml: (3.746946193s)
W0319 20:47:59.498376   14863 addons.go:461] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.32.0/kubectl apply -f /etc/kubernetes/addons/storageclass.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storageclass.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0319 20:47:59.498419   14863 retry.go:31] will retry after 353.467611ms: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.32.0/kubectl apply -f /etc/kubernetes/addons/storageclass.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storageclass.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0319 20:47:59.498501   14863 ssh_runner.go:235] Completed: sudo systemctl start kubelet: (3.692423845s)
I0319 20:47:59.498535   14863 api_server.go:52] waiting for apiserver process to appear ...
I0319 20:47:59.498626   14863 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0319 20:47:59.501222   14863 ssh_runner.go:235] Completed: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.32.0/kubectl apply -f /etc/kubernetes/addons/storage-provisioner.yaml: (3.655907205s)
W0319 20:47:59.501251   14863 addons.go:461] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.32.0/kubectl apply -f /etc/kubernetes/addons/storage-provisioner.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storage-provisioner.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0319 20:47:59.501286   14863 retry.go:31] will retry after 167.677699ms: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.32.0/kubectl apply -f /etc/kubernetes/addons/storage-provisioner.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storage-provisioner.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0319 20:47:59.670098   14863 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.32.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml
W0319 20:47:59.790533   14863 addons.go:461] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.32.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storage-provisioner.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0319 20:47:59.790557   14863 retry.go:31] will retry after 196.556697ms: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.32.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storage-provisioner.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0319 20:47:59.852718   14863 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.32.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml
W0319 20:47:59.931718   14863 addons.go:461] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.32.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storageclass.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0319 20:47:59.931744   14863 retry.go:31] will retry after 427.004278ms: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.32.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storageclass.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0319 20:47:59.988040   14863 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.32.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml
I0319 20:47:59.999337   14863 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0319 20:48:00.075244   14863 addons.go:461] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.32.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storage-provisioner.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0319 20:48:00.075269   14863 retry.go:31] will retry after 311.846736ms: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.32.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storage-provisioner.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0319 20:48:00.359809   14863 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.32.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml
I0319 20:48:00.387584   14863 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.32.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml
W0319 20:48:00.451222   14863 addons.go:461] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.32.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storageclass.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0319 20:48:00.451245   14863 retry.go:31] will retry after 581.14861ms: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.32.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storageclass.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
W0319 20:48:00.470017   14863 addons.go:461] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.32.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storage-provisioner.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0319 20:48:00.470038   14863 retry.go:31] will retry after 955.82635ms: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.32.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storage-provisioner.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0319 20:48:00.499314   14863 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0319 20:48:00.998744   14863 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0319 20:48:01.032730   14863 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.32.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml
W0319 20:48:01.288676   14863 addons.go:461] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.32.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storageclass.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0319 20:48:01.288710   14863 retry.go:31] will retry after 812.980741ms: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.32.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storageclass.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0319 20:48:01.427022   14863 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.32.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml
I0319 20:48:01.499275   14863 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0319 20:48:01.513687   14863 addons.go:461] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.32.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storage-provisioner.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0319 20:48:01.513726   14863 retry.go:31] will retry after 1.02868071s: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.32.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storage-provisioner.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0319 20:48:01.998879   14863 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0319 20:48:02.101903   14863 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.32.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml
W0319 20:48:02.226471   14863 addons.go:461] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.32.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storageclass.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0319 20:48:02.226493   14863 retry.go:31] will retry after 1.408975862s: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.32.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storageclass.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0319 20:48:02.498888   14863 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0319 20:48:02.543518   14863 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.32.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml
W0319 20:48:02.686945   14863 addons.go:461] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.32.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storage-provisioner.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0319 20:48:02.686972   14863 retry.go:31] will retry after 1.971383624s: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.32.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storage-provisioner.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0319 20:48:02.999426   14863 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0319 20:48:03.498928   14863 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0319 20:48:03.635811   14863 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.32.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml
W0319 20:48:03.719019   14863 addons.go:461] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.32.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storageclass.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0319 20:48:03.719046   14863 retry.go:31] will retry after 2.108446815s: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.32.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storageclass.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0319 20:48:03.999490   14863 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0319 20:48:04.498998   14863 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0319 20:48:04.658598   14863 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.32.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml
W0319 20:48:04.740347   14863 addons.go:461] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.32.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storage-provisioner.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0319 20:48:04.740366   14863 retry.go:31] will retry after 2.016169764s: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.32.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storage-provisioner.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0319 20:48:04.999841   14863 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0319 20:48:05.498804   14863 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0319 20:48:05.827892   14863 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.32.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml
W0319 20:48:05.911294   14863 addons.go:461] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.32.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storageclass.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0319 20:48:05.911320   14863 retry.go:31] will retry after 2.749666774s: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.32.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storageclass.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0319 20:48:05.999643   14863 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0319 20:48:06.499169   14863 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0319 20:48:06.757437   14863 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.32.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml
W0319 20:48:06.821217   14863 addons.go:461] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.32.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storage-provisioner.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0319 20:48:06.821237   14863 retry.go:31] will retry after 3.995919106s: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.32.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storage-provisioner.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0319 20:48:06.999515   14863 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0319 20:48:07.027863   14863 api_server.go:72] duration metric: took 11.51815242s to wait for apiserver process to appear ...
I0319 20:48:07.027880   14863 api_server.go:88] waiting for apiserver healthz status ...
I0319 20:48:07.027902   14863 api_server.go:253] Checking apiserver healthz at https://192.168.49.2:8443/healthz ...
I0319 20:48:07.028231   14863 api_server.go:269] stopped: https://192.168.49.2:8443/healthz: Get "https://192.168.49.2:8443/healthz": dial tcp 192.168.49.2:8443: connect: connection refused
I0319 20:48:07.528916   14863 api_server.go:253] Checking apiserver healthz at https://192.168.49.2:8443/healthz ...
I0319 20:48:07.529537   14863 api_server.go:269] stopped: https://192.168.49.2:8443/healthz: Get "https://192.168.49.2:8443/healthz": dial tcp 192.168.49.2:8443: connect: connection refused
I0319 20:48:08.028849   14863 api_server.go:253] Checking apiserver healthz at https://192.168.49.2:8443/healthz ...
I0319 20:48:08.029405   14863 api_server.go:269] stopped: https://192.168.49.2:8443/healthz: Get "https://192.168.49.2:8443/healthz": dial tcp 192.168.49.2:8443: connect: connection refused
I0319 20:48:08.528870   14863 api_server.go:253] Checking apiserver healthz at https://192.168.49.2:8443/healthz ...
I0319 20:48:08.529675   14863 api_server.go:269] stopped: https://192.168.49.2:8443/healthz: Get "https://192.168.49.2:8443/healthz": dial tcp 192.168.49.2:8443: connect: connection refused
I0319 20:48:08.662113   14863 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.32.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml
I0319 20:48:09.028803   14863 api_server.go:253] Checking apiserver healthz at https://192.168.49.2:8443/healthz ...
I0319 20:48:09.029425   14863 api_server.go:269] stopped: https://192.168.49.2:8443/healthz: Get "https://192.168.49.2:8443/healthz": dial tcp 192.168.49.2:8443: connect: connection refused
W0319 20:48:09.072523   14863 addons.go:461] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.32.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storageclass.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0319 20:48:09.072546   14863 retry.go:31] will retry after 4.580900651s: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.32.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storageclass.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0319 20:48:09.528048   14863 api_server.go:253] Checking apiserver healthz at https://192.168.49.2:8443/healthz ...
I0319 20:48:09.528568   14863 api_server.go:269] stopped: https://192.168.49.2:8443/healthz: Get "https://192.168.49.2:8443/healthz": dial tcp 192.168.49.2:8443: connect: connection refused
I0319 20:48:10.028094   14863 api_server.go:253] Checking apiserver healthz at https://192.168.49.2:8443/healthz ...
I0319 20:48:10.028628   14863 api_server.go:269] stopped: https://192.168.49.2:8443/healthz: Get "https://192.168.49.2:8443/healthz": dial tcp 192.168.49.2:8443: connect: connection refused
I0319 20:48:10.528840   14863 api_server.go:253] Checking apiserver healthz at https://192.168.49.2:8443/healthz ...
I0319 20:48:10.529382   14863 api_server.go:269] stopped: https://192.168.49.2:8443/healthz: Get "https://192.168.49.2:8443/healthz": dial tcp 192.168.49.2:8443: connect: connection refused
I0319 20:48:10.817962   14863 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.32.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml
W0319 20:48:10.902220   14863 addons.go:461] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.32.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storage-provisioner.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0319 20:48:10.902242   14863 retry.go:31] will retry after 5.610427674s: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.32.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storage-provisioner.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0319 20:48:11.028429   14863 api_server.go:253] Checking apiserver healthz at https://192.168.49.2:8443/healthz ...
I0319 20:48:11.028980   14863 api_server.go:269] stopped: https://192.168.49.2:8443/healthz: Get "https://192.168.49.2:8443/healthz": dial tcp 192.168.49.2:8443: connect: connection refused
I0319 20:48:11.528755   14863 api_server.go:253] Checking apiserver healthz at https://192.168.49.2:8443/healthz ...
I0319 20:48:11.529365   14863 api_server.go:269] stopped: https://192.168.49.2:8443/healthz: Get "https://192.168.49.2:8443/healthz": dial tcp 192.168.49.2:8443: connect: connection refused
I0319 20:48:12.028992   14863 api_server.go:253] Checking apiserver healthz at https://192.168.49.2:8443/healthz ...
I0319 20:48:12.029537   14863 api_server.go:269] stopped: https://192.168.49.2:8443/healthz: Get "https://192.168.49.2:8443/healthz": dial tcp 192.168.49.2:8443: connect: connection refused
I0319 20:48:12.528853   14863 api_server.go:253] Checking apiserver healthz at https://192.168.49.2:8443/healthz ...
I0319 20:48:12.529464   14863 api_server.go:269] stopped: https://192.168.49.2:8443/healthz: Get "https://192.168.49.2:8443/healthz": dial tcp 192.168.49.2:8443: connect: connection refused
I0319 20:48:13.028850   14863 api_server.go:253] Checking apiserver healthz at https://192.168.49.2:8443/healthz ...
I0319 20:48:13.029369   14863 api_server.go:269] stopped: https://192.168.49.2:8443/healthz: Get "https://192.168.49.2:8443/healthz": dial tcp 192.168.49.2:8443: connect: connection refused
I0319 20:48:13.528998   14863 api_server.go:253] Checking apiserver healthz at https://192.168.49.2:8443/healthz ...
I0319 20:48:13.529661   14863 api_server.go:269] stopped: https://192.168.49.2:8443/healthz: Get "https://192.168.49.2:8443/healthz": dial tcp 192.168.49.2:8443: connect: connection refused
I0319 20:48:13.653975   14863 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.32.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml
W0319 20:48:13.734508   14863 addons.go:461] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.32.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storageclass.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0319 20:48:13.734532   14863 retry.go:31] will retry after 8.156109758s: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.32.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storageclass.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0319 20:48:14.028819   14863 api_server.go:253] Checking apiserver healthz at https://192.168.49.2:8443/healthz ...
I0319 20:48:14.029377   14863 api_server.go:269] stopped: https://192.168.49.2:8443/healthz: Get "https://192.168.49.2:8443/healthz": dial tcp 192.168.49.2:8443: connect: connection refused
I0319 20:48:14.528841   14863 api_server.go:253] Checking apiserver healthz at https://192.168.49.2:8443/healthz ...
I0319 20:48:14.529420   14863 api_server.go:269] stopped: https://192.168.49.2:8443/healthz: Get "https://192.168.49.2:8443/healthz": dial tcp 192.168.49.2:8443: connect: connection refused
I0319 20:48:15.028052   14863 api_server.go:253] Checking apiserver healthz at https://192.168.49.2:8443/healthz ...
I0319 20:48:15.028623   14863 api_server.go:269] stopped: https://192.168.49.2:8443/healthz: Get "https://192.168.49.2:8443/healthz": dial tcp 192.168.49.2:8443: connect: connection refused
I0319 20:48:15.528191   14863 api_server.go:253] Checking apiserver healthz at https://192.168.49.2:8443/healthz ...
I0319 20:48:15.528636   14863 api_server.go:269] stopped: https://192.168.49.2:8443/healthz: Get "https://192.168.49.2:8443/healthz": dial tcp 192.168.49.2:8443: connect: connection refused
I0319 20:48:16.028838   14863 api_server.go:253] Checking apiserver healthz at https://192.168.49.2:8443/healthz ...
I0319 20:48:16.029400   14863 api_server.go:269] stopped: https://192.168.49.2:8443/healthz: Get "https://192.168.49.2:8443/healthz": dial tcp 192.168.49.2:8443: connect: connection refused
I0319 20:48:16.513071   14863 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.32.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml
I0319 20:48:16.528412   14863 api_server.go:253] Checking apiserver healthz at https://192.168.49.2:8443/healthz ...
I0319 20:48:16.528892   14863 api_server.go:269] stopped: https://192.168.49.2:8443/healthz: Get "https://192.168.49.2:8443/healthz": dial tcp 192.168.49.2:8443: connect: connection refused
W0319 20:48:16.833027   14863 addons.go:461] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.32.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storage-provisioner.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0319 20:48:16.833049   14863 retry.go:31] will retry after 8.229880114s: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.32.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storage-provisioner.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0319 20:48:17.028281   14863 api_server.go:253] Checking apiserver healthz at https://192.168.49.2:8443/healthz ...
I0319 20:48:17.028944   14863 api_server.go:269] stopped: https://192.168.49.2:8443/healthz: Get "https://192.168.49.2:8443/healthz": dial tcp 192.168.49.2:8443: connect: connection refused
I0319 20:48:17.528638   14863 api_server.go:253] Checking apiserver healthz at https://192.168.49.2:8443/healthz ...
I0319 20:48:17.529221   14863 api_server.go:269] stopped: https://192.168.49.2:8443/healthz: Get "https://192.168.49.2:8443/healthz": dial tcp 192.168.49.2:8443: connect: connection refused
I0319 20:48:18.028837   14863 api_server.go:253] Checking apiserver healthz at https://192.168.49.2:8443/healthz ...
I0319 20:48:18.029377   14863 api_server.go:269] stopped: https://192.168.49.2:8443/healthz: Get "https://192.168.49.2:8443/healthz": dial tcp 192.168.49.2:8443: connect: connection refused
I0319 20:48:18.528845   14863 api_server.go:253] Checking apiserver healthz at https://192.168.49.2:8443/healthz ...
I0319 20:48:18.529441   14863 api_server.go:269] stopped: https://192.168.49.2:8443/healthz: Get "https://192.168.49.2:8443/healthz": dial tcp 192.168.49.2:8443: connect: connection refused
I0319 20:48:19.028007   14863 api_server.go:253] Checking apiserver healthz at https://192.168.49.2:8443/healthz ...
I0319 20:48:19.028592   14863 api_server.go:269] stopped: https://192.168.49.2:8443/healthz: Get "https://192.168.49.2:8443/healthz": dial tcp 192.168.49.2:8443: connect: connection refused
I0319 20:48:19.528023   14863 api_server.go:253] Checking apiserver healthz at https://192.168.49.2:8443/healthz ...
I0319 20:48:19.528551   14863 api_server.go:269] stopped: https://192.168.49.2:8443/healthz: Get "https://192.168.49.2:8443/healthz": dial tcp 192.168.49.2:8443: connect: connection refused
I0319 20:48:20.028806   14863 api_server.go:253] Checking apiserver healthz at https://192.168.49.2:8443/healthz ...
I0319 20:48:21.890842   14863 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.32.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml
I0319 20:48:22.775866   14863 api_server.go:279] https://192.168.49.2:8443/healthz returned 403:
{"kind":"Status","apiVersion":"v1","metadata":{},"status":"Failure","message":"forbidden: User \"system:anonymous\" cannot get path \"/healthz\"","reason":"Forbidden","details":{},"code":403}
W0319 20:48:22.775888   14863 api_server.go:103] status: https://192.168.49.2:8443/healthz returned error 403:
{"kind":"Status","apiVersion":"v1","metadata":{},"status":"Failure","message":"forbidden: User \"system:anonymous\" cannot get path \"/healthz\"","reason":"Forbidden","details":{},"code":403}
I0319 20:48:22.775923   14863 api_server.go:253] Checking apiserver healthz at https://192.168.49.2:8443/healthz ...
I0319 20:48:22.785715   14863 api_server.go:279] https://192.168.49.2:8443/healthz returned 403:
{"kind":"Status","apiVersion":"v1","metadata":{},"status":"Failure","message":"forbidden: User \"system:anonymous\" cannot get path \"/healthz\"","reason":"Forbidden","details":{},"code":403}
W0319 20:48:22.785733   14863 api_server.go:103] status: https://192.168.49.2:8443/healthz returned error 403:
{"kind":"Status","apiVersion":"v1","metadata":{},"status":"Failure","message":"forbidden: User \"system:anonymous\" cannot get path \"/healthz\"","reason":"Forbidden","details":{},"code":403}
I0319 20:48:23.028028   14863 api_server.go:253] Checking apiserver healthz at https://192.168.49.2:8443/healthz ...
I0319 20:48:23.182984   14863 api_server.go:279] https://192.168.49.2:8443/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: reason withheld
[+]poststarthook/priority-and-fairness-config-producer ok
[-]poststarthook/bootstrap-controller failed: reason withheld
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
W0319 20:48:23.183019   14863 api_server.go:103] status: https://192.168.49.2:8443/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: reason withheld
[+]poststarthook/priority-and-fairness-config-producer ok
[-]poststarthook/bootstrap-controller failed: reason withheld
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
I0319 20:48:23.528513   14863 api_server.go:253] Checking apiserver healthz at https://192.168.49.2:8443/healthz ...
I0319 20:48:23.537287   14863 api_server.go:279] https://192.168.49.2:8443/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: reason withheld
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
W0319 20:48:23.537314   14863 api_server.go:103] status: https://192.168.49.2:8443/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: reason withheld
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
I0319 20:48:23.750475   14863 ssh_runner.go:235] Completed: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.32.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml: (1.859601868s)
I0319 20:48:24.028022   14863 api_server.go:253] Checking apiserver healthz at https://192.168.49.2:8443/healthz ...
I0319 20:48:24.036015   14863 api_server.go:279] https://192.168.49.2:8443/healthz returned 200:
ok
I0319 20:48:24.037232   14863 api_server.go:141] control plane version: v1.32.0
I0319 20:48:24.037255   14863 api_server.go:131] duration metric: took 17.009366069s to wait for apiserver health ...
I0319 20:48:24.037267   14863 system_pods.go:43] waiting for kube-system pods to appear ...
I0319 20:48:24.119437   14863 system_pods.go:59] 7 kube-system pods found
I0319 20:48:24.119455   14863 system_pods.go:61] "coredns-668d6bf9bc-r94c6" [d6dd87bb-0633-4abb-8c6f-57354c724a43] Running
I0319 20:48:24.119460   14863 system_pods.go:61] "etcd-minikube" [d50d1660-04a6-4c0d-97d0-11c93626ba24] Running
I0319 20:48:24.119464   14863 system_pods.go:61] "kube-apiserver-minikube" [2e8c4ef9-2b0f-49ca-aeb0-0fa7ad49fd7a] Running
I0319 20:48:24.119468   14863 system_pods.go:61] "kube-controller-manager-minikube" [1df99cdc-5343-4535-9e65-e6ab59292d35] Running
I0319 20:48:24.119472   14863 system_pods.go:61] "kube-proxy-knzzt" [a7bdc103-5645-40af-a058-2ba0396710bd] Running
I0319 20:48:24.119475   14863 system_pods.go:61] "kube-scheduler-minikube" [3f755c68-2da5-420f-8fbc-f0dfe26d5c85] Running
I0319 20:48:24.119479   14863 system_pods.go:61] "storage-provisioner" [4506078e-8db0-4868-bf29-47640c2571c4] Running
I0319 20:48:24.119484   14863 system_pods.go:74] duration metric: took 82.211695ms to wait for pod list to return data ...
I0319 20:48:24.119494   14863 kubeadm.go:582] duration metric: took 28.609791536s to wait for: map[apiserver:true system_pods:true]
I0319 20:48:24.119505   14863 node_conditions.go:102] verifying NodePressure condition ...
I0319 20:48:24.149008   14863 node_conditions.go:122] node storage ephemeral capacity is 189690860Ki
I0319 20:48:24.149033   14863 node_conditions.go:123] node cpu capacity is 4
I0319 20:48:24.149047   14863 node_conditions.go:105] duration metric: took 29.536927ms to run NodePressure ...
I0319 20:48:24.149066   14863 start.go:241] waiting for startup goroutines ...
I0319 20:48:25.063363   14863 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.32.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml
I0319 20:48:26.101934   14863 ssh_runner.go:235] Completed: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.32.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml: (1.038529017s)
I0319 20:48:26.178626   14863 out.go:177] 🌟  Enabled addons: default-storageclass, storage-provisioner
I0319 20:48:26.223886   14863 addons.go:514] duration metric: took 30.713867972s for enable addons: enabled=[default-storageclass storage-provisioner]
I0319 20:48:26.223934   14863 start.go:246] waiting for cluster config update ...
I0319 20:48:26.223953   14863 start.go:255] writing updated cluster config ...
I0319 20:48:26.224540   14863 ssh_runner.go:195] Run: rm -f paused
I0319 20:48:26.416149   14863 start.go:600] kubectl: 1.32.3, cluster: 1.32.0 (minor skew: 0)
I0319 20:48:26.491783   14863 out.go:177] 🏄  Done! kubectl is now configured to use "minikube" cluster and "default" namespace by default


==> Docker <==
Mar 19 15:38:58 minikube dockerd[748]: time="2025-03-19T15:38:58.195061084Z" level=info msg="ignoring event" container=945fdf24169550824abd616e7717616e9db667d943c751ed2a7021bc5133e7cd module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Mar 19 15:38:58 minikube dockerd[748]: time="2025-03-19T15:38:58.586391483Z" level=info msg="ignoring event" container=3c6f22fc1a829a8a0babed41e3f5505c2ccccb197f81d6a6b61d6b5d4d279485 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Mar 23 15:27:14 minikube cri-dockerd[1038]: time="2025-03-23T15:27:14Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/2ee17dd8e12aeef85e1f36f80a9aa785d39088b064ed0e99b868bce24c725cb5/resolv.conf as [nameserver 10.96.0.10 search ingress-nginx.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Mar 23 15:27:14 minikube cri-dockerd[1038]: time="2025-03-23T15:27:14Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/fd083f77d6c5120970aac15190a568c577f5a5a4311a3f32ac315b0a04b89e0d/resolv.conf as [nameserver 10.96.0.10 search ingress-nginx.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Mar 23 15:27:15 minikube dockerd[748]: time="2025-03-23T15:27:15.346469356Z" level=warning msg="reference for unknown type: " digest="sha256:a9f03b34a3cbfbb26d103a14046ab2c5130a80c3d69d526ff8063d2b37b9fd3f" remote="registry.k8s.io/ingress-nginx/kube-webhook-certgen@sha256:a9f03b34a3cbfbb26d103a14046ab2c5130a80c3d69d526ff8063d2b37b9fd3f"
Mar 23 15:27:27 minikube cri-dockerd[1038]: time="2025-03-23T15:27:27Z" level=info msg="Pulling image registry.k8s.io/ingress-nginx/kube-webhook-certgen:v1.4.4@sha256:a9f03b34a3cbfbb26d103a14046ab2c5130a80c3d69d526ff8063d2b37b9fd3f: d711cc0b3a79: Downloading [>                                                  ]  253.9kB/25.36MB"
Mar 23 15:27:37 minikube cri-dockerd[1038]: time="2025-03-23T15:27:37Z" level=info msg="Pulling image registry.k8s.io/ingress-nginx/kube-webhook-certgen:v1.4.4@sha256:a9f03b34a3cbfbb26d103a14046ab2c5130a80c3d69d526ff8063d2b37b9fd3f: d711cc0b3a79: Downloading [====>                                              ]  2.081MB/25.36MB"
Mar 23 15:27:47 minikube cri-dockerd[1038]: time="2025-03-23T15:27:47Z" level=info msg="Pulling image registry.k8s.io/ingress-nginx/kube-webhook-certgen:v1.4.4@sha256:a9f03b34a3cbfbb26d103a14046ab2c5130a80c3d69d526ff8063d2b37b9fd3f: d711cc0b3a79: Downloading [=======>                                           ]  3.647MB/25.36MB"
Mar 23 15:27:57 minikube cri-dockerd[1038]: time="2025-03-23T15:27:57Z" level=info msg="Pulling image registry.k8s.io/ingress-nginx/kube-webhook-certgen:v1.4.4@sha256:a9f03b34a3cbfbb26d103a14046ab2c5130a80c3d69d526ff8063d2b37b9fd3f: d711cc0b3a79: Downloading [============>                                      ]   6.52MB/25.36MB"
Mar 23 15:28:07 minikube cri-dockerd[1038]: time="2025-03-23T15:28:07Z" level=info msg="Pulling image registry.k8s.io/ingress-nginx/kube-webhook-certgen:v1.4.4@sha256:a9f03b34a3cbfbb26d103a14046ab2c5130a80c3d69d526ff8063d2b37b9fd3f: d711cc0b3a79: Downloading [==================>                                ]  9.141MB/25.36MB"
Mar 23 15:28:17 minikube cri-dockerd[1038]: time="2025-03-23T15:28:17Z" level=info msg="Pulling image registry.k8s.io/ingress-nginx/kube-webhook-certgen:v1.4.4@sha256:a9f03b34a3cbfbb26d103a14046ab2c5130a80c3d69d526ff8063d2b37b9fd3f: d711cc0b3a79: Downloading [=======================>                           ]  11.76MB/25.36MB"
Mar 23 15:28:27 minikube cri-dockerd[1038]: time="2025-03-23T15:28:27Z" level=info msg="Pulling image registry.k8s.io/ingress-nginx/kube-webhook-certgen:v1.4.4@sha256:a9f03b34a3cbfbb26d103a14046ab2c5130a80c3d69d526ff8063d2b37b9fd3f: d711cc0b3a79: Downloading [==================================>                ]  17.53MB/25.36MB"
Mar 23 15:28:37 minikube cri-dockerd[1038]: time="2025-03-23T15:28:37Z" level=info msg="Pulling image registry.k8s.io/ingress-nginx/kube-webhook-certgen:v1.4.4@sha256:a9f03b34a3cbfbb26d103a14046ab2c5130a80c3d69d526ff8063d2b37b9fd3f: d711cc0b3a79: Downloading [========================================>          ]  20.41MB/25.36MB"
Mar 23 15:28:47 minikube cri-dockerd[1038]: time="2025-03-23T15:28:47Z" level=info msg="Pulling image registry.k8s.io/ingress-nginx/kube-webhook-certgen:v1.4.4@sha256:a9f03b34a3cbfbb26d103a14046ab2c5130a80c3d69d526ff8063d2b37b9fd3f: d711cc0b3a79: Downloading [==============================================>    ]  23.56MB/25.36MB"
Mar 23 15:28:53 minikube cri-dockerd[1038]: time="2025-03-23T15:28:53Z" level=info msg="Stop pulling image registry.k8s.io/ingress-nginx/kube-webhook-certgen:v1.4.4@sha256:a9f03b34a3cbfbb26d103a14046ab2c5130a80c3d69d526ff8063d2b37b9fd3f: Status: Downloaded newer image for registry.k8s.io/ingress-nginx/kube-webhook-certgen@sha256:a9f03b34a3cbfbb26d103a14046ab2c5130a80c3d69d526ff8063d2b37b9fd3f"
Mar 23 15:28:55 minikube cri-dockerd[1038]: time="2025-03-23T15:28:55Z" level=info msg="Stop pulling image registry.k8s.io/ingress-nginx/kube-webhook-certgen:v1.4.4@sha256:a9f03b34a3cbfbb26d103a14046ab2c5130a80c3d69d526ff8063d2b37b9fd3f: Status: Image is up to date for registry.k8s.io/ingress-nginx/kube-webhook-certgen@sha256:a9f03b34a3cbfbb26d103a14046ab2c5130a80c3d69d526ff8063d2b37b9fd3f"
Mar 23 15:28:55 minikube dockerd[748]: time="2025-03-23T15:28:55.839622559Z" level=info msg="ignoring event" container=8c43a1e79633f7fdce93528047e82fc8ceb58c7bb439a5937694191039639920 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Mar 23 15:28:56 minikube dockerd[748]: time="2025-03-23T15:28:56.323241620Z" level=info msg="ignoring event" container=16a7983a744bc50506c4af2bee93c6c781e0256947a003c2434d6bddd3b4bcc8 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Mar 23 15:28:58 minikube dockerd[748]: time="2025-03-23T15:28:58.594069450Z" level=info msg="ignoring event" container=2ee17dd8e12aeef85e1f36f80a9aa785d39088b064ed0e99b868bce24c725cb5 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Mar 23 15:28:58 minikube dockerd[748]: time="2025-03-23T15:28:58.613184121Z" level=info msg="ignoring event" container=fd083f77d6c5120970aac15190a568c577f5a5a4311a3f32ac315b0a04b89e0d module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Mar 23 15:29:31 minikube cri-dockerd[1038]: time="2025-03-23T15:29:31Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/7f07a18c8bdb4726dad38af8e458a2b5ec006715c356983863156afa2819869f/resolv.conf as [nameserver 10.96.0.10 search ingress-nginx.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Mar 23 15:29:31 minikube dockerd[748]: time="2025-03-23T15:29:31.983466012Z" level=warning msg="reference for unknown type: " digest="sha256:d56f135b6462cfc476447cfe564b83a45e8bb7da2774963b00d12161112270b7" remote="registry.k8s.io/ingress-nginx/controller@sha256:d56f135b6462cfc476447cfe564b83a45e8bb7da2774963b00d12161112270b7"
Mar 23 15:29:44 minikube cri-dockerd[1038]: time="2025-03-23T15:29:44Z" level=info msg="Pulling image registry.k8s.io/ingress-nginx/controller:v1.11.3@sha256:d56f135b6462cfc476447cfe564b83a45e8bb7da2774963b00d12161112270b7: 43c4264eed91: Downloading [=======>                                           ]  520.7kB/3.624MB"
Mar 23 15:29:54 minikube cri-dockerd[1038]: time="2025-03-23T15:29:54Z" level=info msg="Pulling image registry.k8s.io/ingress-nginx/controller:v1.11.3@sha256:d56f135b6462cfc476447cfe564b83a45e8bb7da2774963b00d12161112270b7: 43c4264eed91: Downloading [=================>                                 ]  1.304MB/3.624MB"
Mar 23 15:30:04 minikube cri-dockerd[1038]: time="2025-03-23T15:30:04Z" level=info msg="Pulling image registry.k8s.io/ingress-nginx/controller:v1.11.3@sha256:d56f135b6462cfc476447cfe564b83a45e8bb7da2774963b00d12161112270b7: 3adb5d6ffffa: Downloading [===============>                                   ]  1.513MB/4.975MB"
Mar 23 15:30:14 minikube cri-dockerd[1038]: time="2025-03-23T15:30:14Z" level=info msg="Pulling image registry.k8s.io/ingress-nginx/controller:v1.11.3@sha256:d56f135b6462cfc476447cfe564b83a45e8bb7da2774963b00d12161112270b7: 43c4264eed91: Downloading [===============================>                   ]  2.296MB/3.624MB"
Mar 23 15:30:24 minikube cri-dockerd[1038]: time="2025-03-23T15:30:24Z" level=info msg="Pulling image registry.k8s.io/ingress-nginx/controller:v1.11.3@sha256:d56f135b6462cfc476447cfe564b83a45e8bb7da2774963b00d12161112270b7: 43c4264eed91: Downloading [==============================================>    ]  3.393MB/3.624MB"
Mar 23 15:30:34 minikube cri-dockerd[1038]: time="2025-03-23T15:30:34Z" level=info msg="Pulling image registry.k8s.io/ingress-nginx/controller:v1.11.3@sha256:d56f135b6462cfc476447cfe564b83a45e8bb7da2774963b00d12161112270b7: 3adb5d6ffffa: Downloading [=================================================> ]  4.948MB/4.975MB"
Mar 23 15:30:41 minikube dockerd[748]: time="2025-03-23T15:30:41.138193000Z" level=info msg="Download failed, retrying (1/5): dial tcp: lookup registry.k8s.io on 192.168.49.1:53: server misbehaving"
Mar 23 15:30:44 minikube cri-dockerd[1038]: time="2025-03-23T15:30:44Z" level=info msg="Pulling image registry.k8s.io/ingress-nginx/controller:v1.11.3@sha256:d56f135b6462cfc476447cfe564b83a45e8bb7da2774963b00d12161112270b7: 5070b1918f8e: Downloading [============>                                      ]  8.971MB/34.91MB"
Mar 23 15:30:49 minikube dockerd[748]: time="2025-03-23T15:30:49.142645209Z" level=info msg="Download failed, retrying (1/5): dial tcp: lookup registry.k8s.io on 192.168.49.1:53: server misbehaving"
Mar 23 15:30:54 minikube cri-dockerd[1038]: time="2025-03-23T15:30:54Z" level=info msg="Pulling image registry.k8s.io/ingress-nginx/controller:v1.11.3@sha256:d56f135b6462cfc476447cfe564b83a45e8bb7da2774963b00d12161112270b7: 77b70db29aee: Retrying in 1 second "
Mar 23 15:31:04 minikube cri-dockerd[1038]: time="2025-03-23T15:31:04Z" level=info msg="Pulling image registry.k8s.io/ingress-nginx/controller:v1.11.3@sha256:d56f135b6462cfc476447cfe564b83a45e8bb7da2774963b00d12161112270b7: 5070b1918f8e: Downloading [=====================>                             ]  14.74MB/34.91MB"
Mar 23 15:31:14 minikube cri-dockerd[1038]: time="2025-03-23T15:31:14Z" level=info msg="Pulling image registry.k8s.io/ingress-nginx/controller:v1.11.3@sha256:d56f135b6462cfc476447cfe564b83a45e8bb7da2774963b00d12161112270b7: 5070b1918f8e: Downloading [==========================>                        ]  18.34MB/34.91MB"
Mar 23 15:31:24 minikube cri-dockerd[1038]: time="2025-03-23T15:31:24Z" level=info msg="Pulling image registry.k8s.io/ingress-nginx/controller:v1.11.3@sha256:d56f135b6462cfc476447cfe564b83a45e8bb7da2774963b00d12161112270b7: 5070b1918f8e: Downloading [==============================>                    ]  21.59MB/34.91MB"
Mar 23 15:31:30 minikube dockerd[748]: time="2025-03-23T15:31:30.850736494Z" level=info msg="Download failed, retrying (2/5): dial tcp: lookup registry.k8s.io on 192.168.49.1:53: server misbehaving"
Mar 23 15:31:34 minikube cri-dockerd[1038]: time="2025-03-23T15:31:34Z" level=info msg="Pulling image registry.k8s.io/ingress-nginx/controller:v1.11.3@sha256:d56f135b6462cfc476447cfe564b83a45e8bb7da2774963b00d12161112270b7: 77b70db29aee: Retrying in 7 seconds "
Mar 23 15:31:44 minikube cri-dockerd[1038]: time="2025-03-23T15:31:44Z" level=info msg="Pulling image registry.k8s.io/ingress-nginx/controller:v1.11.3@sha256:d56f135b6462cfc476447cfe564b83a45e8bb7da2774963b00d12161112270b7: 5070b1918f8e: Downloading [=====================================>             ]  25.91MB/34.91MB"
Mar 23 15:31:54 minikube cri-dockerd[1038]: time="2025-03-23T15:31:54Z" level=info msg="Pulling image registry.k8s.io/ingress-nginx/controller:v1.11.3@sha256:d56f135b6462cfc476447cfe564b83a45e8bb7da2774963b00d12161112270b7: 5070b1918f8e: Downloading [=========================================>         ]  29.16MB/34.91MB"
Mar 23 15:32:04 minikube cri-dockerd[1038]: time="2025-03-23T15:32:04Z" level=info msg="Pulling image registry.k8s.io/ingress-nginx/controller:v1.11.3@sha256:d56f135b6462cfc476447cfe564b83a45e8bb7da2774963b00d12161112270b7: 5070b1918f8e: Downloading [==============================================>    ]  32.76MB/34.91MB"
Mar 23 15:32:14 minikube cri-dockerd[1038]: time="2025-03-23T15:32:14Z" level=info msg="Pulling image registry.k8s.io/ingress-nginx/controller:v1.11.3@sha256:d56f135b6462cfc476447cfe564b83a45e8bb7da2774963b00d12161112270b7: 77b70db29aee: Downloading [===========>                                       ]  200.7kB/860.9kB"
Mar 23 15:32:24 minikube cri-dockerd[1038]: time="2025-03-23T15:32:24Z" level=info msg="Pulling image registry.k8s.io/ingress-nginx/controller:v1.11.3@sha256:d56f135b6462cfc476447cfe564b83a45e8bb7da2774963b00d12161112270b7: e39a383da384: Downloading [===>                                               ]  1.454MB/19.87MB"
Mar 23 15:32:34 minikube cri-dockerd[1038]: time="2025-03-23T15:32:34Z" level=info msg="Pulling image registry.k8s.io/ingress-nginx/controller:v1.11.3@sha256:d56f135b6462cfc476447cfe564b83a45e8bb7da2774963b00d12161112270b7: 196f4b856d59: Downloading [===================>                               ]  68.07kB/175.2kB"
Mar 23 15:32:44 minikube cri-dockerd[1038]: time="2025-03-23T15:32:44Z" level=info msg="Pulling image registry.k8s.io/ingress-nginx/controller:v1.11.3@sha256:d56f135b6462cfc476447cfe564b83a45e8bb7da2774963b00d12161112270b7: e39a383da384: Downloading [=====================>                             ]  8.656MB/19.87MB"
Mar 23 15:32:54 minikube cri-dockerd[1038]: time="2025-03-23T15:32:54Z" level=info msg="Pulling image registry.k8s.io/ingress-nginx/controller:v1.11.3@sha256:d56f135b6462cfc476447cfe564b83a45e8bb7da2774963b00d12161112270b7: e39a383da384: Downloading [=================================>                 ]  13.13MB/19.87MB"
Mar 23 15:33:04 minikube cri-dockerd[1038]: time="2025-03-23T15:33:04Z" level=info msg="Pulling image registry.k8s.io/ingress-nginx/controller:v1.11.3@sha256:d56f135b6462cfc476447cfe564b83a45e8bb7da2774963b00d12161112270b7: e39a383da384: Downloading [========================================>          ]  16.11MB/19.87MB"
Mar 23 15:33:14 minikube cri-dockerd[1038]: time="2025-03-23T15:33:14Z" level=info msg="Pulling image registry.k8s.io/ingress-nginx/controller:v1.11.3@sha256:d56f135b6462cfc476447cfe564b83a45e8bb7da2774963b00d12161112270b7: e39a383da384: Downloading [=================================================> ]  19.73MB/19.87MB"
Mar 23 15:33:24 minikube cri-dockerd[1038]: time="2025-03-23T15:33:24Z" level=info msg="Pulling image registry.k8s.io/ingress-nginx/controller:v1.11.3@sha256:d56f135b6462cfc476447cfe564b83a45e8bb7da2774963b00d12161112270b7: d10fa5065d2e: Downloading [========================================>          ]  1.184MB/1.46MB"
Mar 23 15:33:34 minikube cri-dockerd[1038]: time="2025-03-23T15:33:34Z" level=info msg="Pulling image registry.k8s.io/ingress-nginx/controller:v1.11.3@sha256:d56f135b6462cfc476447cfe564b83a45e8bb7da2774963b00d12161112270b7: 1fadc9c48afa: Downloading [==============>                                    ]   4.18MB/14.65MB"
Mar 23 15:33:44 minikube cri-dockerd[1038]: time="2025-03-23T15:33:44Z" level=info msg="Pulling image registry.k8s.io/ingress-nginx/controller:v1.11.3@sha256:d56f135b6462cfc476447cfe564b83a45e8bb7da2774963b00d12161112270b7: 1fadc9c48afa: Downloading [============================>                      ]  8.309MB/14.65MB"
Mar 23 15:33:54 minikube cri-dockerd[1038]: time="2025-03-23T15:33:54Z" level=info msg="Pulling image registry.k8s.io/ingress-nginx/controller:v1.11.3@sha256:d56f135b6462cfc476447cfe564b83a45e8bb7da2774963b00d12161112270b7: 1fadc9c48afa: Downloading [========================================>          ]  11.85MB/14.65MB"
Mar 23 15:34:04 minikube cri-dockerd[1038]: time="2025-03-23T15:34:04Z" level=info msg="Pulling image registry.k8s.io/ingress-nginx/controller:v1.11.3@sha256:d56f135b6462cfc476447cfe564b83a45e8bb7da2774963b00d12161112270b7: e05b61fcb166: Pull complete "
Mar 23 15:34:14 minikube cri-dockerd[1038]: time="2025-03-23T15:34:14Z" level=info msg="Pulling image registry.k8s.io/ingress-nginx/controller:v1.11.3@sha256:d56f135b6462cfc476447cfe564b83a45e8bb7da2774963b00d12161112270b7: 024e3a95c093: Downloading [=======>                                           ]  3.264MB/20.98MB"
Mar 23 15:34:24 minikube cri-dockerd[1038]: time="2025-03-23T15:34:24Z" level=info msg="Pulling image registry.k8s.io/ingress-nginx/controller:v1.11.3@sha256:d56f135b6462cfc476447cfe564b83a45e8bb7da2774963b00d12161112270b7: 024e3a95c093: Downloading [===============>                                   ]   6.68MB/20.98MB"
Mar 23 15:34:34 minikube cri-dockerd[1038]: time="2025-03-23T15:34:34Z" level=info msg="Pulling image registry.k8s.io/ingress-nginx/controller:v1.11.3@sha256:d56f135b6462cfc476447cfe564b83a45e8bb7da2774963b00d12161112270b7: 024e3a95c093: Downloading [======================>                            ]  9.235MB/20.98MB"
Mar 23 15:34:44 minikube cri-dockerd[1038]: time="2025-03-23T15:34:44Z" level=info msg="Pulling image registry.k8s.io/ingress-nginx/controller:v1.11.3@sha256:d56f135b6462cfc476447cfe564b83a45e8bb7da2774963b00d12161112270b7: 024e3a95c093: Downloading [===========================>                       ]  11.58MB/20.98MB"
Mar 23 15:34:54 minikube cri-dockerd[1038]: time="2025-03-23T15:34:54Z" level=info msg="Pulling image registry.k8s.io/ingress-nginx/controller:v1.11.3@sha256:d56f135b6462cfc476447cfe564b83a45e8bb7da2774963b00d12161112270b7: 024e3a95c093: Downloading [=================================>                 ]  14.13MB/20.98MB"
Mar 23 15:35:04 minikube cri-dockerd[1038]: time="2025-03-23T15:35:04Z" level=info msg="Pulling image registry.k8s.io/ingress-nginx/controller:v1.11.3@sha256:d56f135b6462cfc476447cfe564b83a45e8bb7da2774963b00d12161112270b7: 024e3a95c093: Downloading [======================================>            ]  16.05MB/20.98MB"
Mar 23 15:35:14 minikube cri-dockerd[1038]: time="2025-03-23T15:35:14Z" level=info msg="Pulling image registry.k8s.io/ingress-nginx/controller:v1.11.3@sha256:d56f135b6462cfc476447cfe564b83a45e8bb7da2774963b00d12161112270b7: 024e3a95c093: Downloading [=============================================>     ]  19.25MB/20.98MB"
Mar 23 15:35:19 minikube cri-dockerd[1038]: time="2025-03-23T15:35:19Z" level=info msg="Stop pulling image registry.k8s.io/ingress-nginx/controller:v1.11.3@sha256:d56f135b6462cfc476447cfe564b83a45e8bb7da2774963b00d12161112270b7: Status: Downloaded newer image for registry.k8s.io/ingress-nginx/controller@sha256:d56f135b6462cfc476447cfe564b83a45e8bb7da2774963b00d12161112270b7"


==> container status <==
CONTAINER           IMAGE                                                                                                                        CREATED              STATE               NAME                      ATTEMPT             POD ID              POD
2c5b360894ca7       registry.k8s.io/ingress-nginx/controller@sha256:d56f135b6462cfc476447cfe564b83a45e8bb7da2774963b00d12161112270b7             About a minute ago   Running             controller                0                   7f07a18c8bdb4       ingress-nginx-controller-56d7c84fd4-dccrg
16a7983a744bc       registry.k8s.io/ingress-nginx/kube-webhook-certgen@sha256:a9f03b34a3cbfbb26d103a14046ab2c5130a80c3d69d526ff8063d2b37b9fd3f   7 minutes ago        Exited              patch                     0                   2ee17dd8e12ae       ingress-nginx-admission-patch-qz5hf
8c43a1e79633f       registry.k8s.io/ingress-nginx/kube-webhook-certgen@sha256:a9f03b34a3cbfbb26d103a14046ab2c5130a80c3d69d526ff8063d2b37b9fd3f   7 minutes ago        Exited              create                    0                   fd083f77d6c51       ingress-nginx-admission-create-74zdf
c1f4e26fcb626       abhay4617/authentication@sha256:391b543b5fc60db6fed5485dc9f5c2c281389f9567675a3a2067135832fe4784                             4 days ago           Running             authentication            0                   d538627d00b8d       authentication-5857596898-w5pdq
9510fa5e0a4c8       abhay4617/authentication@sha256:391b543b5fc60db6fed5485dc9f5c2c281389f9567675a3a2067135832fe4784                             4 days ago           Running             authentication            0                   b3d137a6d3041       authentication-5857596898-ctrdg
67738fa2de9fb       6e38f40d628db                                                                                                                4 days ago           Running             storage-provisioner       3                   795f36e0a0c85       storage-provisioner
e82e4334104b4       c69fa2e9cbf5f                                                                                                                4 days ago           Running             coredns                   1                   40b319e4df1c5       coredns-668d6bf9bc-r94c6
3b219b59b7f8a       040f9f8aac8cd                                                                                                                4 days ago           Running             kube-proxy                1                   ee94648fe093d       kube-proxy-knzzt
d7d187c1ca727       6e38f40d628db                                                                                                                4 days ago           Exited              storage-provisioner       2                   795f36e0a0c85       storage-provisioner
ca3afca9853ae       a9e7e6b294baf                                                                                                                4 days ago           Running             etcd                      1                   8f88a7873a049       etcd-minikube
0940d270f94dc       a389e107f4ff1                                                                                                                4 days ago           Running             kube-scheduler            1                   b5074882402b9       kube-scheduler-minikube
9a068c46f83dc       c2e17b8d0f4a3                                                                                                                4 days ago           Running             kube-apiserver            1                   e06b17aebe99f       kube-apiserver-minikube
30d988a34d1a5       8cab3d2a8bd0f                                                                                                                4 days ago           Running             kube-controller-manager   1                   e47c06e4153ea       kube-controller-manager-minikube
2aa9984c28e34       c69fa2e9cbf5f                                                                                                                4 days ago           Exited              coredns                   0                   4529f596f8c4a       coredns-668d6bf9bc-r94c6
0b96888287529       040f9f8aac8cd                                                                                                                4 days ago           Exited              kube-proxy                0                   49abee188a2ed       kube-proxy-knzzt
a8724693c5ff9       a389e107f4ff1                                                                                                                4 days ago           Exited              kube-scheduler            0                   bb9dc4ae61aa0       kube-scheduler-minikube
2896e3a4a8476       c2e17b8d0f4a3                                                                                                                4 days ago           Exited              kube-apiserver            0                   d2adbd107f67e       kube-apiserver-minikube
e483351a5c158       a9e7e6b294baf                                                                                                                4 days ago           Exited              etcd                      0                   5137deba5bdb8       etcd-minikube
78e7bf7b330a9       8cab3d2a8bd0f                                                                                                                4 days ago           Exited              kube-controller-manager   0                   788156f6f89ee       kube-controller-manager-minikube


==> coredns [2aa9984c28e3] <==
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[WARNING] plugin/kubernetes: starting server with unsynced Kubernetes API
.:53
[INFO] plugin/reload: Running configuration SHA512 = 9e2996f8cb67ac53e0259ab1f8d615d07d1beb0bd07e6a1e39769c3bf486a905bb991cc47f8d2f14d0d3a90a87dfc625a0b4c524fed169d8158c40657c0694b1
CoreDNS-1.11.3
linux/amd64, go1.21.11, a6338e9
[INFO] 127.0.0.1:49673 - 13571 "HINFO IN 1723756747281028269.1228585039306879059. udp 57 false 512" NXDOMAIN qr,rd,ra 132 0.078246943s
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.29.3/tools/cache/reflector.go:229: failed to list *v1.Namespace: Get "https://10.96.0.1:443/api/v1/namespaces?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: i/o timeout
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.29.3/tools/cache/reflector.go:229: failed to list *v1.EndpointSlice: Get "https://10.96.0.1:443/apis/discovery.k8s.io/v1/endpointslices?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: i/o timeout
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.29.3/tools/cache/reflector.go:229: failed to list *v1.Service: Get "https://10.96.0.1:443/api/v1/services?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: i/o timeout
[INFO] plugin/kubernetes: Trace[1596101126]: "Reflector ListAndWatch" name:pkg/mod/k8s.io/client-go@v0.29.3/tools/cache/reflector.go:229 (18-Mar-2025 16:52:28.383) (total time: 30077ms):
Trace[1596101126]: ---"Objects listed" error:Get "https://10.96.0.1:443/api/v1/namespaces?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: i/o timeout 30046ms (16:52:58.429)
Trace[1596101126]: [30.077453157s] [30.077453157s] END
[ERROR] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.29.3/tools/cache/reflector.go:229: Failed to watch *v1.Namespace: failed to list *v1.Namespace: Get "https://10.96.0.1:443/api/v1/namespaces?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: i/o timeout
[INFO] plugin/kubernetes: Trace[877377387]: "Reflector ListAndWatch" name:pkg/mod/k8s.io/client-go@v0.29.3/tools/cache/reflector.go:229 (18-Mar-2025 16:52:28.416) (total time: 30044ms):
Trace[877377387]: ---"Objects listed" error:Get "https://10.96.0.1:443/api/v1/services?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: i/o timeout 30013ms (16:52:58.429)
Trace[877377387]: [30.044291571s] [30.044291571s] END
[ERROR] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.29.3/tools/cache/reflector.go:229: Failed to watch *v1.Service: failed to list *v1.Service: Get "https://10.96.0.1:443/api/v1/services?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: i/o timeout
[INFO] plugin/kubernetes: Trace[1696013068]: "Reflector ListAndWatch" name:pkg/mod/k8s.io/client-go@v0.29.3/tools/cache/reflector.go:229 (18-Mar-2025 16:52:28.420) (total time: 30040ms):
Trace[1696013068]: ---"Objects listed" error:Get "https://10.96.0.1:443/apis/discovery.k8s.io/v1/endpointslices?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: i/o timeout 30008ms (16:52:58.429)
Trace[1696013068]: [30.040087664s] [30.040087664s] END
[ERROR] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.29.3/tools/cache/reflector.go:229: Failed to watch *v1.EndpointSlice: failed to list *v1.EndpointSlice: Get "https://10.96.0.1:443/apis/discovery.k8s.io/v1/endpointslices?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: i/o timeout
[INFO] SIGTERM: Shutting down servers then terminating
[INFO] plugin/health: Going into lameduck mode for 5s


==> coredns [e82e4334104b] <==
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[WARNING] plugin/kubernetes: starting server with unsynced Kubernetes API
.:53
[INFO] plugin/reload: Running configuration SHA512 = 9e2996f8cb67ac53e0259ab1f8d615d07d1beb0bd07e6a1e39769c3bf486a905bb991cc47f8d2f14d0d3a90a87dfc625a0b4c524fed169d8158c40657c0694b1
CoreDNS-1.11.3
linux/amd64, go1.21.11, a6338e9
[INFO] 127.0.0.1:39463 - 12939 "HINFO IN 2069477410118241529.1868859165548566297. udp 57 false 512" NXDOMAIN qr,rd,ra 132 0.737504097s
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.29.3/tools/cache/reflector.go:229: failed to list *v1.Service: Get "https://10.96.0.1:443/api/v1/services?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: connect: no route to host
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.29.3/tools/cache/reflector.go:229: failed to list *v1.Namespace: Get "https://10.96.0.1:443/api/v1/namespaces?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: connect: no route to host
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.29.3/tools/cache/reflector.go:229: failed to list *v1.EndpointSlice: Get "https://10.96.0.1:443/apis/discovery.k8s.io/v1/endpointslices?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: connect: no route to host
[INFO] plugin/kubernetes: Trace[1792869510]: "Reflector ListAndWatch" name:pkg/mod/k8s.io/client-go@v0.29.3/tools/cache/reflector.go:229 (19-Mar-2025 15:19:00.685) (total time: 13713ms):
Trace[1792869510]: ---"Objects listed" error:Get "https://10.96.0.1:443/apis/discovery.k8s.io/v1/endpointslices?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: connect: no route to host 13713ms (15:19:14.398)
Trace[1792869510]: [13.713441904s] [13.713441904s] END
[ERROR] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.29.3/tools/cache/reflector.go:229: Failed to watch *v1.EndpointSlice: failed to list *v1.EndpointSlice: Get "https://10.96.0.1:443/apis/discovery.k8s.io/v1/endpointslices?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: connect: no route to host
[INFO] plugin/kubernetes: Trace[452040540]: "Reflector ListAndWatch" name:pkg/mod/k8s.io/client-go@v0.29.3/tools/cache/reflector.go:229 (19-Mar-2025 15:19:00.649) (total time: 13749ms):
Trace[452040540]: ---"Objects listed" error:Get "https://10.96.0.1:443/api/v1/namespaces?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: connect: no route to host 13749ms (15:19:14.398)
Trace[452040540]: [13.749667959s] [13.749667959s] END
[ERROR] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.29.3/tools/cache/reflector.go:229: Failed to watch *v1.Namespace: failed to list *v1.Namespace: Get "https://10.96.0.1:443/api/v1/namespaces?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: connect: no route to host
[INFO] plugin/kubernetes: Trace[761040026]: "Reflector ListAndWatch" name:pkg/mod/k8s.io/client-go@v0.29.3/tools/cache/reflector.go:229 (19-Mar-2025 15:19:00.649) (total time: 13749ms):
Trace[761040026]: ---"Objects listed" error:Get "https://10.96.0.1:443/api/v1/services?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: connect: no route to host 13749ms (15:19:14.398)
Trace[761040026]: [13.749353829s] [13.749353829s] END
[ERROR] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.29.3/tools/cache/reflector.go:229: Failed to watch *v1.Service: failed to list *v1.Service: Get "https://10.96.0.1:443/api/v1/services?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: connect: no route to host


==> describe nodes <==
Name:               minikube
Roles:              control-plane
Labels:             beta.kubernetes.io/arch=amd64
                    beta.kubernetes.io/os=linux
                    kubernetes.io/arch=amd64
                    kubernetes.io/hostname=minikube
                    kubernetes.io/os=linux
                    minikube.k8s.io/commit=dd5d320e41b5451cdf3c01891bc4e13d189586ed-dirty
                    minikube.k8s.io/name=minikube
                    minikube.k8s.io/primary=true
                    minikube.k8s.io/updated_at=2025_03_18T22_22_13_0700
                    minikube.k8s.io/version=v1.35.0
                    node-role.kubernetes.io/control-plane=
                    node.kubernetes.io/exclude-from-external-load-balancers=
Annotations:        kubeadm.alpha.kubernetes.io/cri-socket: unix:///var/run/cri-dockerd.sock
                    node.alpha.kubernetes.io/ttl: 0
                    volumes.kubernetes.io/controller-managed-attach-detach: true
CreationTimestamp:  Tue, 18 Mar 2025 16:52:09 +0000
Taints:             <none>
Unschedulable:      false
Lease:
  HolderIdentity:  minikube
  AcquireTime:     <unset>
  RenewTime:       Sun, 23 Mar 2025 15:36:39 +0000
Conditions:
  Type             Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message
  ----             ------  -----------------                 ------------------                ------                       -------
  MemoryPressure   False   Sun, 23 Mar 2025 15:35:42 +0000   Tue, 18 Mar 2025 16:52:09 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available
  DiskPressure     False   Sun, 23 Mar 2025 15:35:42 +0000   Tue, 18 Mar 2025 16:52:09 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure
  PIDPressure      False   Sun, 23 Mar 2025 15:35:42 +0000   Tue, 18 Mar 2025 16:52:09 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available
  Ready            True    Sun, 23 Mar 2025 15:35:42 +0000   Tue, 18 Mar 2025 16:52:09 +0000   KubeletReady                 kubelet is posting ready status
Addresses:
  InternalIP:  192.168.49.2
  Hostname:    minikube
Capacity:
  cpu:                4
  ephemeral-storage:  189690860Ki
  hugepages-1Gi:      0
  hugepages-2Mi:      0
  memory:             15232072Ki
  pods:               110
Allocatable:
  cpu:                4
  ephemeral-storage:  189690860Ki
  hugepages-1Gi:      0
  hugepages-2Mi:      0
  memory:             15232072Ki
  pods:               110
System Info:
  Machine ID:                 c8352cd0ca8845f1924a3c17d6936d01
  System UUID:                9f8f1fd9-9465-4aee-a9cb-540f602afcbf
  Boot ID:                    f8aced3f-6591-45e9-af9a-b023f23edd88
  Kernel Version:             6.8.0-52-generic
  OS Image:                   Ubuntu 22.04.5 LTS
  Operating System:           linux
  Architecture:               amd64
  Container Runtime Version:  docker://27.4.1
  Kubelet Version:            v1.32.0
  Kube-Proxy Version:         v1.32.0
PodCIDR:                      10.244.0.0/24
PodCIDRs:                     10.244.0.0/24
Non-terminated Pods:          (10 in total)
  Namespace                   Name                                         CPU Requests  CPU Limits  Memory Requests  Memory Limits  Age
  ---------                   ----                                         ------------  ----------  ---------------  -------------  ---
  default                     authentication-5857596898-ctrdg              0 (0%)        0 (0%)      0 (0%)           0 (0%)         3d23h
  default                     authentication-5857596898-w5pdq              0 (0%)        0 (0%)      0 (0%)           0 (0%)         3d23h
  ingress-nginx               ingress-nginx-controller-56d7c84fd4-dccrg    100m (2%)     0 (0%)      90Mi (0%)        0 (0%)         9m33s
  kube-system                 coredns-668d6bf9bc-r94c6                     100m (2%)     0 (0%)      70Mi (0%)        170Mi (1%)     4d22h
  kube-system                 etcd-minikube                                100m (2%)     0 (0%)      100Mi (0%)       0 (0%)         4d22h
  kube-system                 kube-apiserver-minikube                      250m (6%)     0 (0%)      0 (0%)           0 (0%)         4d22h
  kube-system                 kube-controller-manager-minikube             200m (5%)     0 (0%)      0 (0%)           0 (0%)         4d22h
  kube-system                 kube-proxy-knzzt                             0 (0%)        0 (0%)      0 (0%)           0 (0%)         4d22h
  kube-system                 kube-scheduler-minikube                      100m (2%)     0 (0%)      0 (0%)           0 (0%)         4d22h
  kube-system                 storage-provisioner                          0 (0%)        0 (0%)      0 (0%)           0 (0%)         4d22h
Allocated resources:
  (Total limits may be over 100 percent, i.e., overcommitted.)
  Resource           Requests    Limits
  --------           --------    ------
  cpu                850m (21%)  0 (0%)
  memory             260Mi (1%)  170Mi (1%)
  ephemeral-storage  0 (0%)      0 (0%)
  hugepages-1Gi      0 (0%)      0 (0%)
  hugepages-2Mi      0 (0%)      0 (0%)
Events:              <none>


==> dmesg <==
[  +0.000005] pcieport 0000:00:01.7:    [ 6] BadTLP                
[  +0.000009] ath10k_pci 0000:03:00.0: PCIe Bus Error: severity=Correctable, type=Physical Layer, (Transmitter ID)
[  +0.000003] ath10k_pci 0000:03:00.0:   device [168c:0042] error status/mask=00001101/00006000
[  +0.000004] ath10k_pci 0000:03:00.0:    [ 0] RxErr                  (First)
[  +0.000004] ath10k_pci 0000:03:00.0:    [ 8] Rollover              
[  +0.000004] ath10k_pci 0000:03:00.0:    [12] Timeout               
[  +0.029244] pcieport 0000:00:01.7: PCIe Bus Error: severity=Correctable, type=Data Link Layer, (Receiver ID)
[  +0.000004] pcieport 0000:00:01.7:   device [1022:15d3] error status/mask=00000040/00006000
[  +0.000009] pcieport 0000:00:01.7:    [ 6] BadTLP                
[  +0.000014] ath10k_pci 0000:03:00.0: PCIe Bus Error: severity=Correctable, type=Physical Layer, (Transmitter ID)
[  +0.000003] ath10k_pci 0000:03:00.0:   device [168c:0042] error status/mask=00001181/00006000
[  +0.000009] ath10k_pci 0000:03:00.0:    [ 0] RxErr                  (First)
[  +0.000003] ath10k_pci 0000:03:00.0:    [ 7] BadDLLP               
[  +0.000003] ath10k_pci 0000:03:00.0:    [ 8] Rollover              
[  +0.000003] ath10k_pci 0000:03:00.0:    [12] Timeout               
[  +1.463693] kauditd_printk_skb: 8 callbacks suppressed
[Mar23 00:55] ath10k_pci 0000:03:00.0: failed to flush transmit queue (skip 0 ar-state 1): 0
[Mar23 14:04] done.
[  +0.377064] Bluetooth: hci0: HCI Enhanced Setup Synchronous Connection command is advertised, but not supported.
[  +0.726170] pcieport 0000:00:01.7: PCIe Bus Error: severity=Correctable, type=Data Link Layer, (Receiver ID)
[  +0.000006] pcieport 0000:00:01.7:   device [1022:15d3] error status/mask=00000040/00006000
[  +0.000007] pcieport 0000:00:01.7:    [ 6] BadTLP                
[  +0.000010] ath10k_pci 0000:03:00.0: PCIe Bus Error: severity=Correctable, type=Physical Layer, (Transmitter ID)
[  +0.000006] ath10k_pci 0000:03:00.0:   device [168c:0042] error status/mask=00001101/00006000
[  +0.000006] ath10k_pci 0000:03:00.0:    [ 0] RxErr                  (First)
[  +0.000006] ath10k_pci 0000:03:00.0:    [ 8] Rollover              
[  +0.000005] ath10k_pci 0000:03:00.0:    [12] Timeout               
[  +0.029996] pcieport 0000:00:01.7: PCIe Bus Error: severity=Correctable, type=Data Link Layer, (Transmitter ID)
[  +0.000007] pcieport 0000:00:01.7:   device [1022:15d3] error status/mask=000010c0/00006000
[  +0.000011] pcieport 0000:00:01.7:    [ 6] BadTLP                
[  +0.000007] pcieport 0000:00:01.7:    [ 7] BadDLLP               
[  +0.000005] pcieport 0000:00:01.7:    [12] Timeout               
[  +0.000011] ath10k_pci 0000:03:00.0: PCIe Bus Error: severity=Correctable, type=Physical Layer, (Transmitter ID)
[  +0.000006] ath10k_pci 0000:03:00.0:   device [168c:0042] error status/mask=00001181/00006000
[  +0.000006] ath10k_pci 0000:03:00.0:    [ 0] RxErr                  (First)
[  +0.000005] ath10k_pci 0000:03:00.0:    [ 7] BadDLLP               
[  +0.000005] ath10k_pci 0000:03:00.0:    [ 8] Rollover              
[  +0.000005] ath10k_pci 0000:03:00.0:    [12] Timeout               
[Mar23 14:42] kauditd_printk_skb: 8 callbacks suppressed
[Mar23 14:48] ath10k_pci 0000:03:00.0: AMD-Vi: Event logged [IO_PAGE_FAULT domain=0x000c address=0xfede35d0 flags=0x0070]
[  +3.524495] pcieport 0000:00:01.7: PCIe Bus Error: severity=Correctable, type=Data Link Layer, (Receiver ID)
[  +0.000009] pcieport 0000:00:01.7:   device [1022:15d3] error status/mask=00000040/00006000
[  +0.000010] pcieport 0000:00:01.7:    [ 6] BadTLP                
[  +0.000012] ath10k_pci 0000:03:00.0: PCIe Bus Error: severity=Correctable, type=Physical Layer, (Transmitter ID)
[  +0.000008] ath10k_pci 0000:03:00.0:   device [168c:0042] error status/mask=00001101/00006000
[  +0.000009] ath10k_pci 0000:03:00.0:    [ 0] RxErr                  (First)
[  +0.000008] ath10k_pci 0000:03:00.0:    [ 8] Rollover              
[  +0.000007] ath10k_pci 0000:03:00.0:    [12] Timeout               
[  +0.031595] pcieport 0000:00:01.7: PCIe Bus Error: severity=Correctable, type=Data Link Layer, (Receiver ID)
[  +0.000015] pcieport 0000:00:01.7:   device [1022:15d3] error status/mask=00000040/00006000
[  +0.000009] pcieport 0000:00:01.7:    [ 6] BadTLP                
[  +0.000012] ath10k_pci 0000:03:00.0: PCIe Bus Error: severity=Correctable, type=Physical Layer, (Transmitter ID)
[  +0.000065] ath10k_pci 0000:03:00.0:   device [168c:0042] error status/mask=00001181/00006000
[  +0.000011] ath10k_pci 0000:03:00.0:    [ 0] RxErr                  (First)
[  +0.000008] ath10k_pci 0000:03:00.0:    [ 7] BadDLLP               
[  +0.000009] ath10k_pci 0000:03:00.0:    [ 8] Rollover              
[  +0.000008] ath10k_pci 0000:03:00.0:    [12] Timeout               
[Mar23 14:49] kauditd_printk_skb: 9 callbacks suppressed
[ +16.394427] kauditd_printk_skb: 8 callbacks suppressed
[Mar23 15:01] kauditd_printk_skb: 8 callbacks suppressed


==> etcd [ca3afca9853a] <==
{"level":"warn","ts":"2025-03-23T15:21:53.525021Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"122.481202ms","expected-duration":"100ms","prefix":"","request":"header:<ID:8128036006278917374 > lease_revoke:<id:70cc95aefbd6ecac>","response":"size:30"}
{"level":"info","ts":"2025-03-23T15:21:57.137652Z","caller":"traceutil/trace.go:171","msg":"trace[2129924170] transaction","detail":"{read_only:false; response_revision:53948; number_of_response:1; }","duration":"136.034511ms","start":"2025-03-23T15:21:57.001588Z","end":"2025-03-23T15:21:57.137622Z","steps":["trace[2129924170] 'process raft request'  (duration: 135.791903ms)"],"step_count":1}
{"level":"warn","ts":"2025-03-23T15:23:38.128260Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"165.478909ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/masterleases/192.168.49.2\" limit:1 ","response":"range_response_count:1 size:135"}
{"level":"info","ts":"2025-03-23T15:23:38.128645Z","caller":"traceutil/trace.go:171","msg":"trace[1976808431] range","detail":"{range_begin:/registry/masterleases/192.168.49.2; range_end:; response_count:1; response_revision:54028; }","duration":"165.650752ms","start":"2025-03-23T15:23:37.962697Z","end":"2025-03-23T15:23:38.128348Z","steps":["trace[1976808431] 'range keys from in-memory index tree'  (duration: 165.252646ms)"],"step_count":1}
{"level":"info","ts":"2025-03-23T15:24:33.014386Z","caller":"traceutil/trace.go:171","msg":"trace[962730049] transaction","detail":"{read_only:false; response_revision:54073; number_of_response:1; }","duration":"101.28175ms","start":"2025-03-23T15:24:32.913072Z","end":"2025-03-23T15:24:33.014354Z","steps":["trace[962730049] 'process raft request'  (duration: 101.102642ms)"],"step_count":1}
{"level":"info","ts":"2025-03-23T15:24:52.525218Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":53851}
{"level":"info","ts":"2025-03-23T15:24:52.548189Z","caller":"mvcc/kvstore_compaction.go:72","msg":"finished scheduled compaction","compact-revision":53851,"took":"22.666171ms","hash":1478290787,"current-db-size-bytes":3407872,"current-db-size":"3.4 MB","current-db-size-in-use-bytes":1404928,"current-db-size-in-use":"1.4 MB"}
{"level":"info","ts":"2025-03-23T15:24:52.548260Z","caller":"mvcc/hash.go:151","msg":"storing new hash","hash":1478290787,"revision":53851,"compact-revision":53614}
{"level":"info","ts":"2025-03-23T15:25:03.621970Z","caller":"traceutil/trace.go:171","msg":"trace[1170608069] transaction","detail":"{read_only:false; response_revision:54097; number_of_response:1; }","duration":"121.809529ms","start":"2025-03-23T15:25:03.500126Z","end":"2025-03-23T15:25:03.621935Z","steps":["trace[1170608069] 'process raft request'  (duration: 121.638013ms)"],"step_count":1}
{"level":"info","ts":"2025-03-23T15:27:12.005489Z","caller":"traceutil/trace.go:171","msg":"trace[1592974847] transaction","detail":"{read_only:false; response_revision:54245; number_of_response:1; }","duration":"100.149946ms","start":"2025-03-23T15:27:11.905268Z","end":"2025-03-23T15:27:12.005418Z","steps":["trace[1592974847] 'process raft request'  (duration: 99.944698ms)"],"step_count":1}
{"level":"info","ts":"2025-03-23T15:27:12.095985Z","caller":"traceutil/trace.go:171","msg":"trace[194844581] transaction","detail":"{read_only:false; response_revision:54246; number_of_response:1; }","duration":"134.567995ms","start":"2025-03-23T15:27:11.961375Z","end":"2025-03-23T15:27:12.095943Z","steps":["trace[194844581] 'process raft request'  (duration: 99.964172ms)","trace[194844581] 'compare'  (duration: 34.37051ms)"],"step_count":2}
{"level":"info","ts":"2025-03-23T15:27:12.096197Z","caller":"traceutil/trace.go:171","msg":"trace[1173897178] transaction","detail":"{read_only:false; response_revision:54247; number_of_response:1; }","duration":"134.727572ms","start":"2025-03-23T15:27:11.961453Z","end":"2025-03-23T15:27:12.096181Z","steps":["trace[1173897178] 'process raft request'  (duration: 134.43442ms)"],"step_count":1}
{"level":"info","ts":"2025-03-23T15:27:12.096305Z","caller":"traceutil/trace.go:171","msg":"trace[611305595] transaction","detail":"{read_only:false; response_revision:54248; number_of_response:1; }","duration":"134.813352ms","start":"2025-03-23T15:27:11.961481Z","end":"2025-03-23T15:27:12.096294Z","steps":["trace[611305595] 'process raft request'  (duration: 134.450812ms)"],"step_count":1}
{"level":"info","ts":"2025-03-23T15:27:12.096736Z","caller":"traceutil/trace.go:171","msg":"trace[1193914403] transaction","detail":"{read_only:false; response_revision:54249; number_of_response:1; }","duration":"127.137601ms","start":"2025-03-23T15:27:11.969583Z","end":"2025-03-23T15:27:12.096720Z","steps":["trace[1193914403] 'process raft request'  (duration: 126.382702ms)"],"step_count":1}
{"level":"info","ts":"2025-03-23T15:27:12.724293Z","caller":"traceutil/trace.go:171","msg":"trace[196935473] transaction","detail":"{read_only:false; response_revision:54254; number_of_response:1; }","duration":"167.797463ms","start":"2025-03-23T15:27:12.556474Z","end":"2025-03-23T15:27:12.724272Z","steps":["trace[196935473] 'process raft request'  (duration: 167.593998ms)"],"step_count":1}
{"level":"warn","ts":"2025-03-23T15:27:12.724484Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"155.181735ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/pods\" limit:1 ","response":"range_response_count:0 size:6"}
{"level":"info","ts":"2025-03-23T15:27:12.724263Z","caller":"traceutil/trace.go:171","msg":"trace[1581340891] linearizableReadLoop","detail":"{readStateIndex:67641; appliedIndex:67640; }","duration":"154.92791ms","start":"2025-03-23T15:27:12.569304Z","end":"2025-03-23T15:27:12.724232Z","steps":["trace[1581340891] 'read index received'  (duration: 154.743855ms)","trace[1581340891] 'applied index is now lower than readState.Index'  (duration: 183.073µs)"],"step_count":2}
{"level":"info","ts":"2025-03-23T15:27:12.724524Z","caller":"traceutil/trace.go:171","msg":"trace[770421922] range","detail":"{range_begin:/registry/pods; range_end:; response_count:0; response_revision:54254; }","duration":"155.313266ms","start":"2025-03-23T15:27:12.569200Z","end":"2025-03-23T15:27:12.724513Z","steps":["trace[770421922] 'agreement among raft nodes before linearized reading'  (duration: 155.12402ms)"],"step_count":1}
{"level":"info","ts":"2025-03-23T15:27:13.371565Z","caller":"traceutil/trace.go:171","msg":"trace[289644689] linearizableReadLoop","detail":"{readStateIndex:67642; appliedIndex:67641; }","duration":"100.242357ms","start":"2025-03-23T15:27:13.271261Z","end":"2025-03-23T15:27:13.371503Z","steps":["trace[289644689] 'read index received'  (duration: 7.949866ms)","trace[289644689] 'applied index is now lower than readState.Index'  (duration: 92.290056ms)"],"step_count":2}
{"level":"warn","ts":"2025-03-23T15:27:13.371845Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"100.542784ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" limit:1 ","response":"range_response_count:1 size:1112"}
{"level":"info","ts":"2025-03-23T15:27:13.371922Z","caller":"traceutil/trace.go:171","msg":"trace[1456441293] range","detail":"{range_begin:/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath; range_end:; response_count:1; response_revision:54254; }","duration":"100.695847ms","start":"2025-03-23T15:27:13.271200Z","end":"2025-03-23T15:27:13.371895Z","steps":["trace[1456441293] 'agreement among raft nodes before linearized reading'  (duration: 100.421342ms)"],"step_count":1}
{"level":"info","ts":"2025-03-23T15:27:29.871967Z","caller":"traceutil/trace.go:171","msg":"trace[934282451] transaction","detail":"{read_only:false; response_revision:54275; number_of_response:1; }","duration":"103.488752ms","start":"2025-03-23T15:27:29.768427Z","end":"2025-03-23T15:27:29.871916Z","steps":["trace[934282451] 'process raft request'  (duration: 103.213107ms)"],"step_count":1}
{"level":"info","ts":"2025-03-23T15:27:44.304116Z","caller":"traceutil/trace.go:171","msg":"trace[1451598680] transaction","detail":"{read_only:false; response_revision:54286; number_of_response:1; }","duration":"112.987458ms","start":"2025-03-23T15:27:44.191099Z","end":"2025-03-23T15:27:44.304087Z","steps":["trace[1451598680] 'process raft request'  (duration: 112.760381ms)"],"step_count":1}
{"level":"info","ts":"2025-03-23T15:28:01.003841Z","caller":"traceutil/trace.go:171","msg":"trace[1023871738] transaction","detail":"{read_only:false; response_revision:54300; number_of_response:1; }","duration":"100.379276ms","start":"2025-03-23T15:28:00.903424Z","end":"2025-03-23T15:28:01.003804Z","steps":["trace[1023871738] 'process raft request'  (duration: 100.182068ms)"],"step_count":1}
{"level":"warn","ts":"2025-03-23T15:28:54.161833Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"300.95229ms","expected-duration":"100ms","prefix":"","request":"header:<ID:8128036006278919724 > lease_revoke:<id:70cc95aefbd6f5c0>","response":"size:30"}
{"level":"info","ts":"2025-03-23T15:28:54.161967Z","caller":"traceutil/trace.go:171","msg":"trace[2067301355] linearizableReadLoop","detail":"{readStateIndex:67752; appliedIndex:67751; }","duration":"593.798194ms","start":"2025-03-23T15:28:53.568145Z","end":"2025-03-23T15:28:54.161944Z","steps":["trace[2067301355] 'read index received'  (duration: 292.77705ms)","trace[2067301355] 'applied index is now lower than readState.Index'  (duration: 301.019621ms)"],"step_count":2}
{"level":"warn","ts":"2025-03-23T15:28:54.162141Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"593.981171ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/pods\" limit:1 ","response":"range_response_count:0 size:6"}
{"level":"warn","ts":"2025-03-23T15:28:54.162149Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"459.403749ms","expected-duration":"100ms","prefix":"read-only range ","request":"limit:1 keys_only:true ","response":"range_response_count:0 size:6"}
{"level":"info","ts":"2025-03-23T15:28:54.162181Z","caller":"traceutil/trace.go:171","msg":"trace[709154994] range","detail":"{range_begin:/registry/pods; range_end:; response_count:0; response_revision:54343; }","duration":"594.076218ms","start":"2025-03-23T15:28:53.568090Z","end":"2025-03-23T15:28:54.162166Z","steps":["trace[709154994] 'agreement among raft nodes before linearized reading'  (duration: 593.992765ms)"],"step_count":1}
{"level":"info","ts":"2025-03-23T15:28:54.162213Z","caller":"traceutil/trace.go:171","msg":"trace[2128433726] range","detail":"{range_begin:; range_end:; response_count:0; response_revision:54343; }","duration":"459.467073ms","start":"2025-03-23T15:28:53.702727Z","end":"2025-03-23T15:28:54.162194Z","steps":["trace[2128433726] 'agreement among raft nodes before linearized reading'  (duration: 459.38389ms)"],"step_count":1}
{"level":"warn","ts":"2025-03-23T15:28:54.162224Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2025-03-23T15:28:53.568060Z","time spent":"594.152226ms","remote":"127.0.0.1:54716","response type":"/etcdserverpb.KV/Range","request count":0,"request size":18,"response count":0,"response size":30,"request content":"key:\"/registry/pods\" limit:1 "}
{"level":"info","ts":"2025-03-23T15:28:54.713522Z","caller":"traceutil/trace.go:171","msg":"trace[2015153732] transaction","detail":"{read_only:false; response_revision:54344; number_of_response:1; }","duration":"112.134541ms","start":"2025-03-23T15:28:54.601353Z","end":"2025-03-23T15:28:54.713488Z","steps":["trace[2015153732] 'process raft request'  (duration: 111.825126ms)"],"step_count":1}
{"level":"info","ts":"2025-03-23T15:28:55.795889Z","caller":"traceutil/trace.go:171","msg":"trace[1320507590] transaction","detail":"{read_only:false; response_revision:54347; number_of_response:1; }","duration":"104.492942ms","start":"2025-03-23T15:28:55.691344Z","end":"2025-03-23T15:28:55.795837Z","steps":["trace[1320507590] 'process raft request'  (duration: 104.290376ms)"],"step_count":1}
{"level":"info","ts":"2025-03-23T15:29:00.508033Z","caller":"traceutil/trace.go:171","msg":"trace[255451822] transaction","detail":"{read_only:false; response_revision:54365; number_of_response:1; }","duration":"155.676925ms","start":"2025-03-23T15:29:00.352310Z","end":"2025-03-23T15:29:00.507987Z","steps":["trace[255451822] 'process raft request'  (duration: 144.329033ms)","trace[255451822] 'compare'  (duration: 11.110658ms)"],"step_count":2}
{"level":"info","ts":"2025-03-23T15:29:00.508087Z","caller":"traceutil/trace.go:171","msg":"trace[1958569579] transaction","detail":"{read_only:false; response_revision:54366; number_of_response:1; }","duration":"129.757623ms","start":"2025-03-23T15:29:00.378304Z","end":"2025-03-23T15:29:00.508062Z","steps":["trace[1958569579] 'process raft request'  (duration: 129.620485ms)"],"step_count":1}
{"level":"info","ts":"2025-03-23T15:29:52.568801Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":54088}
{"level":"info","ts":"2025-03-23T15:29:52.591691Z","caller":"mvcc/kvstore_compaction.go:72","msg":"finished scheduled compaction","compact-revision":54088,"took":"21.943975ms","hash":2699443287,"current-db-size-bytes":3407872,"current-db-size":"3.4 MB","current-db-size-in-use-bytes":1748992,"current-db-size-in-use":"1.7 MB"}
{"level":"info","ts":"2025-03-23T15:29:52.591783Z","caller":"mvcc/hash.go:151","msg":"storing new hash","hash":2699443287,"revision":54088,"compact-revision":53851}
{"level":"warn","ts":"2025-03-23T15:29:58.995622Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"154.016576ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/runtimeclasses/\" range_end:\"/registry/runtimeclasses0\" count_only:true ","response":"range_response_count:0 size:6"}
{"level":"info","ts":"2025-03-23T15:29:58.995789Z","caller":"traceutil/trace.go:171","msg":"trace[2055892308] range","detail":"{range_begin:/registry/runtimeclasses/; range_end:/registry/runtimeclasses0; response_count:0; response_revision:54417; }","duration":"154.206858ms","start":"2025-03-23T15:29:58.841539Z","end":"2025-03-23T15:29:58.995746Z","steps":["trace[2055892308] 'count revisions from in-memory index tree'  (duration: 153.887016ms)"],"step_count":1}
{"level":"info","ts":"2025-03-23T15:30:15.270807Z","caller":"traceutil/trace.go:171","msg":"trace[180991832] transaction","detail":"{read_only:false; response_revision:54429; number_of_response:1; }","duration":"161.524712ms","start":"2025-03-23T15:30:15.109244Z","end":"2025-03-23T15:30:15.270768Z","steps":["trace[180991832] 'process raft request'  (duration: 161.265226ms)"],"step_count":1}
{"level":"info","ts":"2025-03-23T15:30:27.658527Z","caller":"traceutil/trace.go:171","msg":"trace[395456674] transaction","detail":"{read_only:false; response_revision:54438; number_of_response:1; }","duration":"168.859627ms","start":"2025-03-23T15:30:27.489627Z","end":"2025-03-23T15:30:27.658487Z","steps":["trace[395456674] 'process raft request'  (duration: 168.645104ms)"],"step_count":1}
{"level":"info","ts":"2025-03-23T15:31:11.128091Z","caller":"traceutil/trace.go:171","msg":"trace[1505650561] transaction","detail":"{read_only:false; response_revision:54473; number_of_response:1; }","duration":"108.715962ms","start":"2025-03-23T15:31:11.019338Z","end":"2025-03-23T15:31:11.128054Z","steps":["trace[1505650561] 'process raft request'  (duration: 85.811305ms)","trace[1505650561] 'compare'  (duration: 22.711357ms)"],"step_count":2}
{"level":"info","ts":"2025-03-23T15:31:11.939614Z","caller":"traceutil/trace.go:171","msg":"trace[1720913346] transaction","detail":"{read_only:false; response_revision:54474; number_of_response:1; }","duration":"134.48394ms","start":"2025-03-23T15:31:11.805102Z","end":"2025-03-23T15:31:11.939586Z","steps":["trace[1720913346] 'process raft request'  (duration: 134.252492ms)"],"step_count":1}
{"level":"info","ts":"2025-03-23T15:31:27.859359Z","caller":"traceutil/trace.go:171","msg":"trace[1885859819] linearizableReadLoop","detail":"{readStateIndex:67926; appliedIndex:67925; }","duration":"157.077584ms","start":"2025-03-23T15:31:27.702251Z","end":"2025-03-23T15:31:27.859329Z","steps":["trace[1885859819] 'read index received'  (duration: 156.892216ms)","trace[1885859819] 'applied index is now lower than readState.Index'  (duration: 184.046µs)"],"step_count":2}
{"level":"info","ts":"2025-03-23T15:31:27.859837Z","caller":"traceutil/trace.go:171","msg":"trace[566476632] transaction","detail":"{read_only:false; response_revision:54485; number_of_response:1; }","duration":"278.875209ms","start":"2025-03-23T15:31:27.580942Z","end":"2025-03-23T15:31:27.859817Z","steps":["trace[566476632] 'process raft request'  (duration: 278.175732ms)"],"step_count":1}
{"level":"warn","ts":"2025-03-23T15:31:27.859910Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"130.173265ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/health\" ","response":"range_response_count:0 size:6"}
{"level":"info","ts":"2025-03-23T15:31:27.859981Z","caller":"traceutil/trace.go:171","msg":"trace[1091011615] range","detail":"{range_begin:/registry/health; range_end:; response_count:0; response_revision:54485; }","duration":"130.29954ms","start":"2025-03-23T15:31:27.729662Z","end":"2025-03-23T15:31:27.859961Z","steps":["trace[1091011615] 'agreement among raft nodes before linearized reading'  (duration: 130.174396ms)"],"step_count":1}
{"level":"warn","ts":"2025-03-23T15:31:27.860264Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"158.010454ms","expected-duration":"100ms","prefix":"read-only range ","request":"limit:1 keys_only:true ","response":"range_response_count:0 size:6"}
{"level":"info","ts":"2025-03-23T15:31:27.860311Z","caller":"traceutil/trace.go:171","msg":"trace[75678644] range","detail":"{range_begin:; range_end:; response_count:0; response_revision:54485; }","duration":"158.058798ms","start":"2025-03-23T15:31:27.702238Z","end":"2025-03-23T15:31:27.860297Z","steps":["trace[75678644] 'agreement among raft nodes before linearized reading'  (duration: 157.992485ms)"],"step_count":1}
{"level":"info","ts":"2025-03-23T15:32:38.036513Z","caller":"traceutil/trace.go:171","msg":"trace[654640717] transaction","detail":"{read_only:false; response_revision:54540; number_of_response:1; }","duration":"112.189359ms","start":"2025-03-23T15:32:37.924288Z","end":"2025-03-23T15:32:38.036477Z","steps":["trace[654640717] 'process raft request'  (duration: 111.90872ms)"],"step_count":1}
{"level":"info","ts":"2025-03-23T15:33:24.913585Z","caller":"traceutil/trace.go:171","msg":"trace[737232438] transaction","detail":"{read_only:false; response_revision:54576; number_of_response:1; }","duration":"137.174128ms","start":"2025-03-23T15:33:24.776373Z","end":"2025-03-23T15:33:24.913547Z","steps":["trace[737232438] 'process raft request'  (duration: 136.873661ms)"],"step_count":1}
{"level":"info","ts":"2025-03-23T15:33:25.357136Z","caller":"traceutil/trace.go:171","msg":"trace[745916419] transaction","detail":"{read_only:false; response_revision:54577; number_of_response:1; }","duration":"181.277086ms","start":"2025-03-23T15:33:25.175823Z","end":"2025-03-23T15:33:25.357100Z","steps":["trace[745916419] 'process raft request'  (duration: 181.065645ms)"],"step_count":1}
{"level":"info","ts":"2025-03-23T15:34:10.530887Z","caller":"traceutil/trace.go:171","msg":"trace[384406181] transaction","detail":"{read_only:false; response_revision:54613; number_of_response:1; }","duration":"139.717423ms","start":"2025-03-23T15:34:10.391142Z","end":"2025-03-23T15:34:10.530859Z","steps":["trace[384406181] 'process raft request'  (duration: 139.550291ms)"],"step_count":1}
{"level":"info","ts":"2025-03-23T15:34:45.608163Z","caller":"traceutil/trace.go:171","msg":"trace[1366520959] transaction","detail":"{read_only:false; response_revision:54639; number_of_response:1; }","duration":"164.825435ms","start":"2025-03-23T15:34:45.443299Z","end":"2025-03-23T15:34:45.608124Z","steps":["trace[1366520959] 'process raft request'  (duration: 164.619978ms)"],"step_count":1}
{"level":"info","ts":"2025-03-23T15:34:52.628845Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":54412}
{"level":"info","ts":"2025-03-23T15:34:52.663678Z","caller":"mvcc/kvstore_compaction.go:72","msg":"finished scheduled compaction","compact-revision":54412,"took":"34.487079ms","hash":1205999695,"current-db-size-bytes":3407872,"current-db-size":"3.4 MB","current-db-size-in-use-bytes":1814528,"current-db-size-in-use":"1.8 MB"}
{"level":"info","ts":"2025-03-23T15:34:52.663767Z","caller":"mvcc/hash.go:151","msg":"storing new hash","hash":1205999695,"revision":54412,"compact-revision":54088}
{"level":"info","ts":"2025-03-23T15:35:20.441421Z","caller":"traceutil/trace.go:171","msg":"trace[1843947703] transaction","detail":"{read_only:false; response_revision:54670; number_of_response:1; }","duration":"101.96523ms","start":"2025-03-23T15:35:20.339425Z","end":"2025-03-23T15:35:20.441390Z","steps":["trace[1843947703] 'process raft request'  (duration: 101.748962ms)"],"step_count":1}
{"level":"info","ts":"2025-03-23T15:36:40.322306Z","caller":"traceutil/trace.go:171","msg":"trace[657515251] transaction","detail":"{read_only:false; response_revision:54763; number_of_response:1; }","duration":"110.733354ms","start":"2025-03-23T15:36:40.211543Z","end":"2025-03-23T15:36:40.322277Z","steps":["trace[657515251] 'process raft request'  (duration: 110.56304ms)"],"step_count":1}


==> etcd [e483351a5c15] <==
{"level":"info","ts":"2025-03-18T18:25:40.481854Z","caller":"traceutil/trace.go:171","msg":"trace[1233941875] range","detail":"{range_begin:/registry/mutatingwebhookconfigurations/; range_end:/registry/mutatingwebhookconfigurations0; response_count:0; response_revision:6342; }","duration":"120.619587ms","start":"2025-03-18T18:25:40.361212Z","end":"2025-03-18T18:25:40.481832Z","steps":["trace[1233941875] 'agreement among raft nodes before linearized reading'  (duration: 120.47089ms)"],"step_count":1}
{"level":"info","ts":"2025-03-18T18:26:24.610382Z","caller":"traceutil/trace.go:171","msg":"trace[1322683714] transaction","detail":"{read_only:false; response_revision:6377; number_of_response:1; }","duration":"125.577645ms","start":"2025-03-18T18:26:24.484758Z","end":"2025-03-18T18:26:24.610336Z","steps":["trace[1322683714] 'process raft request'  (duration: 125.337953ms)"],"step_count":1}
{"level":"info","ts":"2025-03-18T18:27:06.492075Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":6169}
{"level":"info","ts":"2025-03-18T18:27:06.526273Z","caller":"mvcc/kvstore_compaction.go:72","msg":"finished scheduled compaction","compact-revision":6169,"took":"33.483686ms","hash":3431428438,"current-db-size-bytes":3407872,"current-db-size":"3.4 MB","current-db-size-in-use-bytes":1449984,"current-db-size-in-use":"1.4 MB"}
{"level":"info","ts":"2025-03-18T18:27:06.526346Z","caller":"mvcc/hash.go:151","msg":"storing new hash","hash":3431428438,"revision":6169,"compact-revision":5919}
{"level":"warn","ts":"2025-03-18T18:27:13.862666Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"111.342163ms","expected-duration":"100ms","prefix":"read-only range ","request":"limit:1 serializable:true keys_only:true ","response":"range_response_count:0 size:5"}
{"level":"info","ts":"2025-03-18T18:27:13.862761Z","caller":"traceutil/trace.go:171","msg":"trace[986267246] range","detail":"{range_begin:; range_end:; response_count:0; response_revision:6421; }","duration":"111.450803ms","start":"2025-03-18T18:27:13.751287Z","end":"2025-03-18T18:27:13.862737Z","steps":["trace[986267246] 'range keys from in-memory index tree'  (duration: 111.321273ms)"],"step_count":1}
{"level":"info","ts":"2025-03-18T18:27:38.384948Z","caller":"traceutil/trace.go:171","msg":"trace[1033852566] transaction","detail":"{read_only:false; response_revision:6440; number_of_response:1; }","duration":"193.109516ms","start":"2025-03-18T18:27:38.191810Z","end":"2025-03-18T18:27:38.384919Z","steps":["trace[1033852566] 'process raft request'  (duration: 192.876657ms)"],"step_count":1}
{"level":"info","ts":"2025-03-18T18:29:16.673979Z","caller":"traceutil/trace.go:171","msg":"trace[1438862377] linearizableReadLoop","detail":"{readStateIndex:7767; appliedIndex:7766; }","duration":"160.724908ms","start":"2025-03-18T18:29:16.513222Z","end":"2025-03-18T18:29:16.673947Z","steps":["trace[1438862377] 'read index received'  (duration: 160.410722ms)","trace[1438862377] 'applied index is now lower than readState.Index'  (duration: 312.422µs)"],"step_count":2}
{"level":"warn","ts":"2025-03-18T18:29:16.676264Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"162.979244ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/health\" ","response":"range_response_count:0 size:5"}
{"level":"info","ts":"2025-03-18T18:29:16.676428Z","caller":"traceutil/trace.go:171","msg":"trace[2000165376] range","detail":"{range_begin:/registry/health; range_end:; response_count:0; response_revision:6519; }","duration":"163.233033ms","start":"2025-03-18T18:29:16.513170Z","end":"2025-03-18T18:29:16.676403Z","steps":["trace[2000165376] 'agreement among raft nodes before linearized reading'  (duration: 162.948625ms)"],"step_count":1}
{"level":"info","ts":"2025-03-18T18:29:16.678252Z","caller":"traceutil/trace.go:171","msg":"trace[1056651891] transaction","detail":"{read_only:false; response_revision:6519; number_of_response:1; }","duration":"196.601487ms","start":"2025-03-18T18:29:16.481613Z","end":"2025-03-18T18:29:16.678214Z","steps":["trace[1056651891] 'process raft request'  (duration: 192.1265ms)"],"step_count":1}
{"level":"warn","ts":"2025-03-18T18:29:37.434831Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"111.533843ms","expected-duration":"100ms","prefix":"","request":"header:<ID:8128035985601662213 > lease_revoke:<id:70cc95aa2b610cb4>","response":"size:29"}
{"level":"info","ts":"2025-03-18T18:30:22.210800Z","caller":"traceutil/trace.go:171","msg":"trace[2053162655] transaction","detail":"{read_only:false; response_revision:6575; number_of_response:1; }","duration":"164.372119ms","start":"2025-03-18T18:30:22.046397Z","end":"2025-03-18T18:30:22.210769Z","steps":["trace[2053162655] 'process raft request'  (duration: 141.891813ms)","trace[2053162655] 'compare'  (duration: 22.262856ms)"],"step_count":2}
{"level":"info","ts":"2025-03-18T18:30:52.910695Z","caller":"traceutil/trace.go:171","msg":"trace[1062362169] transaction","detail":"{read_only:false; response_revision:6600; number_of_response:1; }","duration":"129.995702ms","start":"2025-03-18T18:30:52.780666Z","end":"2025-03-18T18:30:52.910662Z","steps":["trace[1062362169] 'process raft request'  (duration: 129.757943ms)"],"step_count":1}
{"level":"info","ts":"2025-03-18T18:31:58.519184Z","caller":"traceutil/trace.go:171","msg":"trace[1574775905] transaction","detail":"{read_only:false; response_revision:6656; number_of_response:1; }","duration":"107.769105ms","start":"2025-03-18T18:31:58.411384Z","end":"2025-03-18T18:31:58.519153Z","steps":["trace[1574775905] 'process raft request'  (duration: 107.582005ms)"],"step_count":1}
{"level":"info","ts":"2025-03-18T18:32:02.692695Z","caller":"traceutil/trace.go:171","msg":"trace[1520148171] transaction","detail":"{read_only:false; response_revision:6661; number_of_response:1; }","duration":"127.549988ms","start":"2025-03-18T18:32:02.565115Z","end":"2025-03-18T18:32:02.692665Z","steps":["trace[1520148171] 'process raft request'  (duration: 127.238677ms)"],"step_count":1}
{"level":"info","ts":"2025-03-18T18:32:06.576763Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":6414}
{"level":"info","ts":"2025-03-18T18:32:06.599739Z","caller":"mvcc/kvstore_compaction.go:72","msg":"finished scheduled compaction","compact-revision":6414,"took":"22.308464ms","hash":2385852616,"current-db-size-bytes":3407872,"current-db-size":"3.4 MB","current-db-size-in-use-bytes":1421312,"current-db-size-in-use":"1.4 MB"}
{"level":"info","ts":"2025-03-18T18:32:06.599807Z","caller":"mvcc/hash.go:151","msg":"storing new hash","hash":2385852616,"revision":6414,"compact-revision":6169}
{"level":"info","ts":"2025-03-18T18:32:25.603123Z","caller":"traceutil/trace.go:171","msg":"trace[988963382] transaction","detail":"{read_only:false; response_revision:6680; number_of_response:1; }","duration":"185.151495ms","start":"2025-03-18T18:32:25.417943Z","end":"2025-03-18T18:32:25.603095Z","steps":["trace[988963382] 'process raft request'  (duration: 184.975646ms)"],"step_count":1}
{"level":"info","ts":"2025-03-18T18:32:33.881601Z","caller":"traceutil/trace.go:171","msg":"trace[1411064146] transaction","detail":"{read_only:false; response_revision:6686; number_of_response:1; }","duration":"104.526275ms","start":"2025-03-18T18:32:33.777039Z","end":"2025-03-18T18:32:33.881565Z","steps":["trace[1411064146] 'process raft request'  (duration: 104.339595ms)"],"step_count":1}
{"level":"info","ts":"2025-03-18T18:35:29.864637Z","caller":"traceutil/trace.go:171","msg":"trace[316594516] transaction","detail":"{read_only:false; response_revision:6830; number_of_response:1; }","duration":"111.597596ms","start":"2025-03-18T18:35:29.753003Z","end":"2025-03-18T18:35:29.864601Z","steps":["trace[316594516] 'process raft request'  (duration: 111.342244ms)"],"step_count":1}
{"level":"info","ts":"2025-03-18T18:36:01.703005Z","caller":"traceutil/trace.go:171","msg":"trace[603482474] transaction","detail":"{read_only:false; response_revision:6857; number_of_response:1; }","duration":"100.011261ms","start":"2025-03-18T18:36:01.602953Z","end":"2025-03-18T18:36:01.702964Z","steps":["trace[603482474] 'process raft request'  (duration: 99.735079ms)"],"step_count":1}
{"level":"info","ts":"2025-03-18T18:36:27.517578Z","caller":"traceutil/trace.go:171","msg":"trace[1189394432] transaction","detail":"{read_only:false; response_revision:6878; number_of_response:1; }","duration":"114.614912ms","start":"2025-03-18T18:36:27.402932Z","end":"2025-03-18T18:36:27.517547Z","steps":["trace[1189394432] 'process raft request'  (duration: 114.355412ms)"],"step_count":1}
{"level":"info","ts":"2025-03-18T18:37:06.630807Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":6665}
{"level":"info","ts":"2025-03-18T18:37:06.698433Z","caller":"mvcc/kvstore_compaction.go:72","msg":"finished scheduled compaction","compact-revision":6665,"took":"48.077348ms","hash":1127183314,"current-db-size-bytes":3407872,"current-db-size":"3.4 MB","current-db-size-in-use-bytes":1429504,"current-db-size-in-use":"1.4 MB"}
{"level":"info","ts":"2025-03-18T18:37:06.698522Z","caller":"mvcc/hash.go:151","msg":"storing new hash","hash":1127183314,"revision":6665,"compact-revision":6414}
{"level":"info","ts":"2025-03-18T18:37:12.592815Z","caller":"traceutil/trace.go:171","msg":"trace[1484719435] transaction","detail":"{read_only:false; response_revision:6919; number_of_response:1; }","duration":"132.571677ms","start":"2025-03-18T18:37:12.460209Z","end":"2025-03-18T18:37:12.592781Z","steps":["trace[1484719435] 'process raft request'  (duration: 132.360771ms)"],"step_count":1}
{"level":"info","ts":"2025-03-18T18:37:37.282974Z","caller":"traceutil/trace.go:171","msg":"trace[517701584] transaction","detail":"{read_only:false; response_revision:6939; number_of_response:1; }","duration":"158.174722ms","start":"2025-03-18T18:37:37.124770Z","end":"2025-03-18T18:37:37.282945Z","steps":["trace[517701584] 'process raft request'  (duration: 157.989675ms)"],"step_count":1}
{"level":"info","ts":"2025-03-18T18:37:53.703698Z","caller":"traceutil/trace.go:171","msg":"trace[1691665768] transaction","detail":"{read_only:false; response_revision:6952; number_of_response:1; }","duration":"107.995ms","start":"2025-03-18T18:37:53.595667Z","end":"2025-03-18T18:37:53.703662Z","steps":["trace[1691665768] 'process raft request'  (duration: 107.613896ms)"],"step_count":1}
{"level":"info","ts":"2025-03-18T18:38:16.328936Z","caller":"traceutil/trace.go:171","msg":"trace[1104549898] transaction","detail":"{read_only:false; response_revision:6970; number_of_response:1; }","duration":"143.004088ms","start":"2025-03-18T18:38:16.185889Z","end":"2025-03-18T18:38:16.328893Z","steps":["trace[1104549898] 'process raft request'  (duration: 142.81323ms)"],"step_count":1}
{"level":"info","ts":"2025-03-18T18:38:34.764591Z","caller":"traceutil/trace.go:171","msg":"trace[1621863850] transaction","detail":"{read_only:false; response_revision:6985; number_of_response:1; }","duration":"120.362652ms","start":"2025-03-18T18:38:34.644195Z","end":"2025-03-18T18:38:34.764557Z","steps":["trace[1621863850] 'process raft request'  (duration: 120.168838ms)"],"step_count":1}
{"level":"info","ts":"2025-03-18T18:39:52.452244Z","caller":"traceutil/trace.go:171","msg":"trace[172024414] transaction","detail":"{read_only:false; response_revision:7046; number_of_response:1; }","duration":"180.717708ms","start":"2025-03-18T18:39:52.271494Z","end":"2025-03-18T18:39:52.452212Z","steps":["trace[172024414] 'process raft request'  (duration: 109.527525ms)","trace[172024414] 'compare'  (duration: 71.051115ms)"],"step_count":2}
{"level":"info","ts":"2025-03-18T18:42:06.697306Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":6914}
{"level":"info","ts":"2025-03-18T18:42:06.731558Z","caller":"mvcc/kvstore_compaction.go:72","msg":"finished scheduled compaction","compact-revision":6914,"took":"33.610601ms","hash":3604982287,"current-db-size-bytes":3407872,"current-db-size":"3.4 MB","current-db-size-in-use-bytes":1323008,"current-db-size-in-use":"1.3 MB"}
{"level":"info","ts":"2025-03-18T18:42:06.731651Z","caller":"mvcc/hash.go:151","msg":"storing new hash","hash":3604982287,"revision":6914,"compact-revision":6665}
{"level":"info","ts":"2025-03-18T18:42:30.153083Z","caller":"traceutil/trace.go:171","msg":"trace[14935887] transaction","detail":"{read_only:false; response_revision:7182; number_of_response:1; }","duration":"148.764281ms","start":"2025-03-18T18:42:30.004289Z","end":"2025-03-18T18:42:30.153053Z","steps":["trace[14935887] 'process raft request'  (duration: 148.532825ms)"],"step_count":1}
{"level":"info","ts":"2025-03-18T18:42:36.414580Z","caller":"traceutil/trace.go:171","msg":"trace[1094804565] transaction","detail":"{read_only:false; response_revision:7188; number_of_response:1; }","duration":"176.354507ms","start":"2025-03-18T18:42:36.238194Z","end":"2025-03-18T18:42:36.414548Z","steps":["trace[1094804565] 'process raft request'  (duration: 176.172426ms)"],"step_count":1}
{"level":"warn","ts":"2025-03-18T18:44:22.429663Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"122.646856ms","expected-duration":"100ms","prefix":"","request":"header:<ID:8128035985601666526 username:\"kube-apiserver-etcd-client\" auth_revision:1 > txn:<compare:<target:MOD key:\"/registry/masterleases/192.168.49.2\" mod_revision:7265 > success:<request_put:<key:\"/registry/masterleases/192.168.49.2\" value_size:66 lease:8128035985601666523 >> failure:<request_range:<key:\"/registry/masterleases/192.168.49.2\" > >>","response":"size:16"}
{"level":"info","ts":"2025-03-18T18:44:22.429898Z","caller":"traceutil/trace.go:171","msg":"trace[1347633560] transaction","detail":"{read_only:false; response_revision:7272; number_of_response:1; }","duration":"132.82743ms","start":"2025-03-18T18:44:22.297007Z","end":"2025-03-18T18:44:22.429834Z","steps":["trace[1347633560] 'compare'  (duration: 122.512106ms)"],"step_count":1}
{"level":"info","ts":"2025-03-18T18:44:51.068691Z","caller":"traceutil/trace.go:171","msg":"trace[1297030041] transaction","detail":"{read_only:false; response_revision:7295; number_of_response:1; }","duration":"118.537523ms","start":"2025-03-18T18:44:50.950118Z","end":"2025-03-18T18:44:51.068655Z","steps":["trace[1297030041] 'process raft request'  (duration: 118.347767ms)"],"step_count":1}
{"level":"info","ts":"2025-03-18T18:45:23.815510Z","caller":"traceutil/trace.go:171","msg":"trace[634382296] transaction","detail":"{read_only:false; response_revision:7325; number_of_response:1; }","duration":"123.508827ms","start":"2025-03-18T18:45:23.691970Z","end":"2025-03-18T18:45:23.815479Z","steps":["trace[634382296] 'process raft request'  (duration: 123.304092ms)"],"step_count":1}
{"level":"info","ts":"2025-03-18T18:46:12.432652Z","caller":"traceutil/trace.go:171","msg":"trace[1628176801] transaction","detail":"{read_only:false; response_revision:7364; number_of_response:1; }","duration":"132.653826ms","start":"2025-03-18T18:46:12.299966Z","end":"2025-03-18T18:46:12.432620Z","steps":["trace[1628176801] 'process raft request'  (duration: 110.104718ms)","trace[1628176801] 'compare'  (duration: 22.421091ms)"],"step_count":2}
{"level":"info","ts":"2025-03-18T18:47:06.756379Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":7163}
{"level":"info","ts":"2025-03-18T18:47:06.790799Z","caller":"mvcc/kvstore_compaction.go:72","msg":"finished scheduled compaction","compact-revision":7163,"took":"33.773936ms","hash":3009914710,"current-db-size-bytes":3407872,"current-db-size":"3.4 MB","current-db-size-in-use-bytes":1249280,"current-db-size-in-use":"1.2 MB"}
{"level":"info","ts":"2025-03-18T18:47:06.790899Z","caller":"mvcc/hash.go:151","msg":"storing new hash","hash":3009914710,"revision":7163,"compact-revision":6914}
{"level":"info","ts":"2025-03-18T18:47:18.178877Z","caller":"traceutil/trace.go:171","msg":"trace[1015836710] transaction","detail":"{read_only:false; response_revision:7422; number_of_response:1; }","duration":"130.109961ms","start":"2025-03-18T18:47:18.048724Z","end":"2025-03-18T18:47:18.178834Z","steps":["trace[1015836710] 'process raft request'  (duration: 129.908633ms)"],"step_count":1}
{"level":"info","ts":"2025-03-18T18:48:00.778313Z","caller":"osutil/interrupt_unix.go:64","msg":"received signal; shutting down","signal":"terminated"}
{"level":"info","ts":"2025-03-18T18:48:01.850970Z","caller":"embed/etcd.go:378","msg":"closing etcd server","name":"minikube","data-dir":"/var/lib/minikube/etcd","advertise-peer-urls":["https://192.168.49.2:2380"],"advertise-client-urls":["https://192.168.49.2:2379"]}
{"level":"warn","ts":"2025-03-18T18:48:02.149550Z","caller":"embed/serve.go:212","msg":"stopping secure grpc server due to error","error":"accept tcp 127.0.0.1:2379: use of closed network connection"}
{"level":"warn","ts":"2025-03-18T18:48:02.149740Z","caller":"embed/serve.go:214","msg":"stopped secure grpc server due to error","error":"accept tcp 127.0.0.1:2379: use of closed network connection"}
{"level":"warn","ts":"2025-03-18T18:48:02.778287Z","caller":"v3rpc/watch.go:460","msg":"failed to send watch response to gRPC stream","error":"rpc error: code = Unavailable desc = transport is closing"}
{"level":"warn","ts":"2025-03-18T18:48:02.779499Z","caller":"v3rpc/watch.go:460","msg":"failed to send watch response to gRPC stream","error":"rpc error: code = Unavailable desc = transport is closing"}
{"level":"warn","ts":"2025-03-18T18:48:03.817493Z","caller":"embed/serve.go:212","msg":"stopping secure grpc server due to error","error":"accept tcp 192.168.49.2:2379: use of closed network connection"}
{"level":"warn","ts":"2025-03-18T18:48:03.817575Z","caller":"embed/serve.go:214","msg":"stopped secure grpc server due to error","error":"accept tcp 192.168.49.2:2379: use of closed network connection"}
{"level":"info","ts":"2025-03-18T18:48:04.138278Z","caller":"etcdserver/server.go:1543","msg":"skipped leadership transfer for single voting member cluster","local-member-id":"aec36adc501070cc","current-leader-member-id":"aec36adc501070cc"}
{"level":"info","ts":"2025-03-18T18:48:05.931808Z","caller":"embed/etcd.go:582","msg":"stopping serving peer traffic","address":"192.168.49.2:2380"}
{"level":"info","ts":"2025-03-18T18:48:06.050106Z","caller":"embed/etcd.go:587","msg":"stopped serving peer traffic","address":"192.168.49.2:2380"}
{"level":"info","ts":"2025-03-18T18:48:06.050175Z","caller":"embed/etcd.go:380","msg":"closed etcd server","name":"minikube","data-dir":"/var/lib/minikube/etcd","advertise-peer-urls":["https://192.168.49.2:2380"],"advertise-client-urls":["https://192.168.49.2:2379"]}


==> kernel <==
 15:36:44 up 4 days, 42 min,  0 users,  load average: 1.45, 1.28, 1.33
Linux minikube 6.8.0-52-generic #53~22.04.1-Ubuntu SMP PREEMPT_DYNAMIC Wed Jan 15 19:18:46 UTC 2 x86_64 x86_64 x86_64 GNU/Linux
PRETTY_NAME="Ubuntu 22.04.5 LTS"


==> kube-apiserver [2896e3a4a847] <==
W0318 18:48:05.916908       1 logging.go:55] [core] [Channel #61 SubChannel #62]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0318 18:48:05.933963       1 logging.go:55] [core] [Channel #91 SubChannel #92]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0318 18:48:05.959665       1 logging.go:55] [core] [Channel #142 SubChannel #143]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0318 18:48:07.552637       1 logging.go:55] [core] [Channel #40 SubChannel #41]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0318 18:48:07.638206       1 logging.go:55] [core] [Channel #178 SubChannel #179]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0318 18:48:07.665169       1 logging.go:55] [core] [Channel #55 SubChannel #56]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0318 18:48:07.697652       1 logging.go:55] [core] [Channel #10 SubChannel #11]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0318 18:48:07.701105       1 logging.go:55] [core] [Channel #133 SubChannel #134]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0318 18:48:07.777159       1 logging.go:55] [core] [Channel #163 SubChannel #164]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0318 18:48:07.784856       1 logging.go:55] [core] [Channel #46 SubChannel #47]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0318 18:48:07.795637       1 logging.go:55] [core] [Channel #73 SubChannel #74]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0318 18:48:07.812296       1 logging.go:55] [core] [Channel #52 SubChannel #53]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0318 18:48:07.878547       1 logging.go:55] [core] [Channel #58 SubChannel #59]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0318 18:48:07.891961       1 logging.go:55] [core] [Channel #3 SubChannel #4]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0318 18:48:07.915474       1 logging.go:55] [core] [Channel #136 SubChannel #137]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0318 18:48:07.921061       1 logging.go:55] [core] [Channel #82 SubChannel #83]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0318 18:48:07.972596       1 logging.go:55] [core] [Channel #121 SubChannel #122]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0318 18:48:07.974028       1 logging.go:55] [core] [Channel #157 SubChannel #158]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0318 18:48:08.027400       1 logging.go:55] [core] [Channel #106 SubChannel #107]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0318 18:48:08.027400       1 logging.go:55] [core] [Channel #61 SubChannel #62]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0318 18:48:08.043146       1 logging.go:55] [core] [Channel #22 SubChannel #23]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0318 18:48:08.050737       1 logging.go:55] [core] [Channel #91 SubChannel #92]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0318 18:48:08.050737       1 logging.go:55] [core] [Channel #31 SubChannel #32]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0318 18:48:08.072561       1 logging.go:55] [core] [Channel #127 SubChannel #128]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0318 18:48:08.073877       1 logging.go:55] [core] [Channel #34 SubChannel #35]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0318 18:48:08.075692       1 logging.go:55] [core] [Channel #142 SubChannel #143]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0318 18:48:08.149065       1 logging.go:55] [core] [Channel #64 SubChannel #65]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0318 18:48:08.172983       1 logging.go:55] [core] [Channel #115 SubChannel #116]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0318 18:48:08.198738       1 logging.go:55] [core] [Channel #166 SubChannel #167]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0318 18:48:08.200197       1 logging.go:55] [core] [Channel #139 SubChannel #140]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0318 18:48:08.205975       1 logging.go:55] [core] [Channel #79 SubChannel #80]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0318 18:48:08.205988       1 logging.go:55] [core] [Channel #118 SubChannel #119]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0318 18:48:08.222022       1 logging.go:55] [core] [Channel #25 SubChannel #26]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0318 18:48:08.236090       1 logging.go:55] [core] [Channel #109 SubChannel #110]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0318 18:48:08.238769       1 logging.go:55] [core] [Channel #94 SubChannel #95]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0318 18:48:08.242548       1 logging.go:55] [core] [Channel #97 SubChannel #98]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0318 18:48:08.245103       1 logging.go:55] [core] [Channel #76 SubChannel #77]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0318 18:48:08.281435       1 logging.go:55] [core] [Channel #145 SubChannel #146]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0318 18:48:08.299806       1 logging.go:55] [core] [Channel #148 SubChannel #149]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0318 18:48:08.319560       1 logging.go:55] [core] [Channel #103 SubChannel #104]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0318 18:48:08.327235       1 logging.go:55] [core] [Channel #100 SubChannel #101]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0318 18:48:08.349224       1 logging.go:55] [core] [Channel #85 SubChannel #86]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0318 18:48:08.351632       1 logging.go:55] [core] [Channel #17 SubChannel #18]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0318 18:48:08.374359       1 logging.go:55] [core] [Channel #175 SubChannel #176]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0318 18:48:08.393817       1 logging.go:55] [core] [Channel #151 SubChannel #152]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0318 18:48:08.402475       1 logging.go:55] [core] [Channel #124 SubChannel #125]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0318 18:48:08.409152       1 logging.go:55] [core] [Channel #112 SubChannel #113]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0318 18:48:08.415577       1 logging.go:55] [core] [Channel #172 SubChannel #173]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0318 18:48:08.442620       1 logging.go:55] [core] [Channel #88 SubChannel #89]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0318 18:48:08.445093       1 logging.go:55] [core] [Channel #1 SubChannel #2]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0318 18:48:08.446429       1 logging.go:55] [core] [Channel #49 SubChannel #50]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0318 18:48:08.450851       1 logging.go:55] [core] [Channel #5 SubChannel #6]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0318 18:48:08.462474       1 logging.go:55] [core] [Channel #37 SubChannel #38]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0318 18:48:08.483155       1 logging.go:55] [core] [Channel #154 SubChannel #155]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0318 18:48:08.588281       1 logging.go:55] [core] [Channel #28 SubChannel #29]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0318 18:48:08.656909       1 logging.go:55] [core] [Channel #169 SubChannel #170]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0318 18:48:08.747300       1 logging.go:55] [core] [Channel #130 SubChannel #131]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0318 18:48:08.774913       1 logging.go:55] [core] [Channel #70 SubChannel #71]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0318 18:48:08.826071       1 logging.go:55] [core] [Channel #67 SubChannel #68]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0318 18:48:08.946535       1 logging.go:55] [core] [Channel #160 SubChannel #161]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"


==> kube-apiserver [9a068c46f83d] <==
I0319 15:18:22.747817       1 system_namespaces_controller.go:66] Starting system namespaces controller
I0319 15:18:22.749107       1 cluster_authentication_trust_controller.go:462] Starting cluster_authentication_trust_controller controller
I0319 15:18:22.749124       1 shared_informer.go:313] Waiting for caches to sync for cluster_authentication_trust_controller
I0319 15:18:22.763365       1 gc_controller.go:78] Starting apiserver lease garbage collector
I0319 15:18:22.763428       1 controller.go:80] Starting OpenAPI V3 AggregationController
I0319 15:18:22.765148       1 crdregistration_controller.go:114] Starting crd-autoregister controller
I0319 15:18:22.765165       1 shared_informer.go:313] Waiting for caches to sync for crd-autoregister
I0319 15:18:22.746832       1 tlsconfig.go:243] "Starting DynamicServingCertificateController"
I0319 15:18:22.746836       1 controller.go:78] Starting OpenAPI AggregationController
I0319 15:18:22.765828       1 dynamic_serving_content.go:135] "Starting controller" name="aggregator-proxy-cert::/var/lib/minikube/certs/front-proxy-client.crt::/var/lib/minikube/certs/front-proxy-client.key"
I0319 15:18:22.746870       1 remote_available_controller.go:411] Starting RemoteAvailability controller
I0319 15:18:22.766170       1 cache.go:32] Waiting for caches to sync for RemoteAvailability controller
I0319 15:18:22.767183       1 customresource_discovery_controller.go:292] Starting DiscoveryController
I0319 15:18:22.767233       1 controller.go:142] Starting OpenAPI controller
I0319 15:18:22.768084       1 controller.go:90] Starting OpenAPI V3 controller
I0319 15:18:22.768385       1 naming_controller.go:294] Starting NamingConditionController
I0319 15:18:22.768640       1 establishing_controller.go:81] Starting EstablishingController
I0319 15:18:22.768773       1 nonstructuralschema_controller.go:195] Starting NonStructuralSchemaConditionController
I0319 15:18:22.768792       1 apiapproval_controller.go:189] Starting KubernetesAPIApprovalPolicyConformantConditionController
I0319 15:18:22.768805       1 crd_finalizer.go:269] Starting CRDFinalizer
I0319 15:18:22.772124       1 dynamic_cafile_content.go:161] "Starting controller" name="client-ca-bundle::/var/lib/minikube/certs/ca.crt"
I0319 15:18:22.772629       1 dynamic_cafile_content.go:161] "Starting controller" name="request-header::/var/lib/minikube/certs/front-proxy-ca.crt"
I0319 15:18:22.773163       1 apf_controller.go:377] Starting API Priority and Fairness config controller
I0319 15:18:22.849105       1 cache.go:39] Caches are synced for LocalAvailability controller
I0319 15:18:22.849129       1 cache.go:39] Caches are synced for APIServiceRegistrationController controller
I0319 15:18:22.849147       1 shared_informer.go:320] Caches are synced for cluster_authentication_trust_controller
I0319 15:18:22.849340       1 shared_informer.go:320] Caches are synced for node_authorizer
I0319 15:18:22.849358       1 shared_informer.go:320] Caches are synced for configmaps
I0319 15:18:22.854848       1 handler_discovery.go:451] Starting ResourceDiscoveryManager
I0319 15:18:22.854861       1 shared_informer.go:320] Caches are synced for *generic.policySource[*k8s.io/api/admissionregistration/v1.ValidatingAdmissionPolicy,*k8s.io/api/admissionregistration/v1.ValidatingAdmissionPolicyBinding,k8s.io/apiserver/pkg/admission/plugin/policy/validating.Validator]
I0319 15:18:22.854893       1 policy_source.go:240] refreshing policies
I0319 15:18:22.865219       1 shared_informer.go:320] Caches are synced for crd-autoregister
I0319 15:18:22.865255       1 aggregator.go:171] initial CRD sync complete...
I0319 15:18:22.865262       1 autoregister_controller.go:144] Starting autoregister controller
I0319 15:18:22.865270       1 cache.go:32] Waiting for caches to sync for autoregister controller
I0319 15:18:22.865277       1 cache.go:39] Caches are synced for autoregister controller
I0319 15:18:22.866373       1 cache.go:39] Caches are synced for RemoteAvailability controller
I0319 15:18:22.873948       1 apf_controller.go:382] Running API Priority and Fairness config worker
I0319 15:18:22.873963       1 apf_controller.go:385] Running API Priority and Fairness periodic rebalancing process
I0319 15:18:22.929938       1 controller.go:615] quota admission added evaluator for: leases.coordination.k8s.io
E0319 15:18:23.220104       1 controller.go:97] Error removing old endpoints from kubernetes service: no API server IP addresses were listed in storage, refusing to erase all endpoints for the kubernetes Service
I0319 15:18:23.754829       1 storage_scheduling.go:111] all system priority classes are created successfully or already exist.
I0319 15:18:25.353448       1 controller.go:615] quota admission added evaluator for: serviceaccounts
I0319 15:18:25.852791       1 controller.go:615] quota admission added evaluator for: endpoints
I0319 15:18:32.404793       1 controller.go:615] quota admission added evaluator for: endpointslices.discovery.k8s.io
I0319 15:18:41.231558       1 controller.go:615] quota admission added evaluator for: replicasets.apps
I0319 15:18:41.434548       1 controller.go:615] quota admission added evaluator for: deployments.apps
I0319 15:18:42.843518       1 controller.go:615] quota admission added evaluator for: daemonsets.apps
I0319 15:18:49.792958       1 alloc.go:330] "allocated clusterIPs" service="default/authentication" clusterIPs={"IPv4":"10.100.116.97"}
E0319 15:45:25.421382       1 conn.go:339] Error on socket receive: read tcp 192.168.49.2:8443->192.168.49.1:35880: use of closed network connection
E0320 13:35:10.447763       1 authentication.go:74] "Unable to authenticate the request" err="[invalid bearer token, service account token has expired]"
E0320 13:35:10.448018       1 authentication.go:74] "Unable to authenticate the request" err="[invalid bearer token, service account token has expired]"
E0323 14:04:31.528184       1 authentication.go:74] "Unable to authenticate the request" err="[invalid bearer token, service account token has expired]"
E0323 14:04:31.533750       1 authentication.go:74] "Unable to authenticate the request" err="[invalid bearer token, service account token has expired]"
I0323 15:27:10.897810       1 controller.go:615] quota admission added evaluator for: namespaces
I0323 15:27:11.041641       1 controller.go:615] quota admission added evaluator for: roles.rbac.authorization.k8s.io
I0323 15:27:11.155403       1 controller.go:615] quota admission added evaluator for: rolebindings.rbac.authorization.k8s.io
I0323 15:27:11.482731       1 alloc.go:330] "allocated clusterIPs" service="ingress-nginx/ingress-nginx-controller" clusterIPs={"IPv4":"10.98.221.40"}
I0323 15:27:11.584840       1 alloc.go:330] "allocated clusterIPs" service="ingress-nginx/ingress-nginx-controller-admission" clusterIPs={"IPv4":"10.99.35.12"}
I0323 15:27:11.668839       1 controller.go:615] quota admission added evaluator for: jobs.batch


==> kube-controller-manager [30d988a34d1a] <==
I0321 01:39:07.276784       1 range_allocator.go:247] "Successfully synced" logger="node-ipam-controller" key="minikube"
I0321 01:44:12.662556       1 range_allocator.go:247] "Successfully synced" logger="node-ipam-controller" key="minikube"
I0321 01:49:17.996736       1 range_allocator.go:247] "Successfully synced" logger="node-ipam-controller" key="minikube"
I0321 01:54:24.539281       1 range_allocator.go:247] "Successfully synced" logger="node-ipam-controller" key="minikube"
I0321 01:59:30.156696       1 range_allocator.go:247] "Successfully synced" logger="node-ipam-controller" key="minikube"
I0321 02:04:35.840492       1 range_allocator.go:247] "Successfully synced" logger="node-ipam-controller" key="minikube"
I0321 02:09:42.021569       1 range_allocator.go:247] "Successfully synced" logger="node-ipam-controller" key="minikube"
I0321 02:14:48.181303       1 range_allocator.go:247] "Successfully synced" logger="node-ipam-controller" key="minikube"
I0321 02:19:54.038369       1 range_allocator.go:247] "Successfully synced" logger="node-ipam-controller" key="minikube"
I0321 02:24:57.761846       1 range_allocator.go:247] "Successfully synced" logger="node-ipam-controller" key="minikube"
I0321 02:30:03.905293       1 range_allocator.go:247] "Successfully synced" logger="node-ipam-controller" key="minikube"
I0321 02:35:10.657466       1 range_allocator.go:247] "Successfully synced" logger="node-ipam-controller" key="minikube"
I0321 02:40:16.333584       1 range_allocator.go:247] "Successfully synced" logger="node-ipam-controller" key="minikube"
I0321 02:45:22.969391       1 range_allocator.go:247] "Successfully synced" logger="node-ipam-controller" key="minikube"
I0323 14:04:31.529922       1 garbagecollector.go:789] "failed to discover preferred resources" logger="garbage-collector-controller" error="the server has asked for the client to provide credentials"
E0323 14:04:31.536699       1 resource_quota_controller.go:446] "Unhandled Error" err="failed to discover resources: the server has asked for the client to provide credentials" logger="UnhandledError"
I0323 14:07:10.827744       1 range_allocator.go:247] "Successfully synced" logger="node-ipam-controller" key="minikube"
I0323 14:12:16.600549       1 range_allocator.go:247] "Successfully synced" logger="node-ipam-controller" key="minikube"
I0323 14:17:21.527043       1 range_allocator.go:247] "Successfully synced" logger="node-ipam-controller" key="minikube"
I0323 14:22:27.798920       1 range_allocator.go:247] "Successfully synced" logger="node-ipam-controller" key="minikube"
I0323 14:27:34.652437       1 range_allocator.go:247] "Successfully synced" logger="node-ipam-controller" key="minikube"
I0323 14:32:40.593824       1 range_allocator.go:247] "Successfully synced" logger="node-ipam-controller" key="minikube"
I0323 14:37:45.193054       1 range_allocator.go:247] "Successfully synced" logger="node-ipam-controller" key="minikube"
I0323 14:42:50.720165       1 range_allocator.go:247] "Successfully synced" logger="node-ipam-controller" key="minikube"
I0323 14:47:56.206935       1 range_allocator.go:247] "Successfully synced" logger="node-ipam-controller" key="minikube"
I0323 14:53:02.739750       1 range_allocator.go:247] "Successfully synced" logger="node-ipam-controller" key="minikube"
I0323 14:58:08.066867       1 range_allocator.go:247] "Successfully synced" logger="node-ipam-controller" key="minikube"
I0323 15:03:14.853044       1 range_allocator.go:247] "Successfully synced" logger="node-ipam-controller" key="minikube"
I0323 15:08:20.792458       1 range_allocator.go:247] "Successfully synced" logger="node-ipam-controller" key="minikube"
I0323 15:13:26.171407       1 range_allocator.go:247] "Successfully synced" logger="node-ipam-controller" key="minikube"
I0323 15:18:32.050873       1 range_allocator.go:247] "Successfully synced" logger="node-ipam-controller" key="minikube"
I0323 15:23:37.766291       1 range_allocator.go:247] "Successfully synced" logger="node-ipam-controller" key="minikube"
I0323 15:27:11.687931       1 job_controller.go:598] "enqueueing job" logger="job-controller" key="ingress-nginx/ingress-nginx-admission-create" delay="0s"
I0323 15:27:11.757157       1 job_controller.go:598] "enqueueing job" logger="job-controller" key="ingress-nginx/ingress-nginx-admission-patch" delay="0s"
I0323 15:27:11.827297       1 job_controller.go:598] "enqueueing job" logger="job-controller" key="ingress-nginx/ingress-nginx-admission-patch" delay="1s"
I0323 15:27:11.828108       1 job_controller.go:598] "enqueueing job" logger="job-controller" key="ingress-nginx/ingress-nginx-admission-create" delay="1s"
I0323 15:27:11.882982       1 job_controller.go:598] "enqueueing job" logger="job-controller" key="ingress-nginx/ingress-nginx-admission-patch" delay="1s"
I0323 15:27:11.883122       1 job_controller.go:598] "enqueueing job" logger="job-controller" key="ingress-nginx/ingress-nginx-admission-create" delay="1s"
I0323 15:27:11.884087       1 job_controller.go:598] "enqueueing job" logger="job-controller" key="ingress-nginx/ingress-nginx-admission-patch" delay="1s"
I0323 15:27:11.884238       1 job_controller.go:598] "enqueueing job" logger="job-controller" key="ingress-nginx/ingress-nginx-admission-create" delay="1s"
I0323 15:27:11.890051       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="ingress-nginx/ingress-nginx-controller-56d7c84fd4" duration="174.820824ms"
I0323 15:27:12.008037       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="ingress-nginx/ingress-nginx-controller-56d7c84fd4" duration="116.959854ms"
I0323 15:27:12.008184       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="ingress-nginx/ingress-nginx-controller-56d7c84fd4" duration="76.793µs"
I0323 15:27:12.101724       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="ingress-nginx/ingress-nginx-controller-56d7c84fd4" duration="132.273µs"
I0323 15:27:12.144124       1 job_controller.go:598] "enqueueing job" logger="job-controller" key="ingress-nginx/ingress-nginx-admission-create" delay="1s"
I0323 15:27:12.177189       1 job_controller.go:598] "enqueueing job" logger="job-controller" key="ingress-nginx/ingress-nginx-admission-patch" delay="1s"
I0323 15:28:43.848922       1 range_allocator.go:247] "Successfully synced" logger="node-ipam-controller" key="minikube"
I0323 15:28:57.355965       1 job_controller.go:598] "enqueueing job" logger="job-controller" key="ingress-nginx/ingress-nginx-admission-patch" delay="1s"
I0323 15:28:57.401788       1 job_controller.go:598] "enqueueing job" logger="job-controller" key="ingress-nginx/ingress-nginx-admission-create" delay="1s"
I0323 15:28:59.227391       1 job_controller.go:598] "enqueueing job" logger="job-controller" key="ingress-nginx/ingress-nginx-admission-patch" delay="1s"
I0323 15:28:59.272627       1 job_controller.go:598] "enqueueing job" logger="job-controller" key="ingress-nginx/ingress-nginx-admission-create" delay="1s"
I0323 15:29:00.286175       1 job_controller.go:598] "enqueueing job" logger="job-controller" key="ingress-nginx/ingress-nginx-admission-patch" delay="1s"
I0323 15:29:00.345227       1 job_controller.go:598] "enqueueing job" logger="job-controller" key="ingress-nginx/ingress-nginx-admission-create" delay="1s"
I0323 15:29:00.345227       1 job_controller.go:598] "enqueueing job" logger="job-controller" key="ingress-nginx/ingress-nginx-admission-patch" delay="1s"
I0323 15:29:00.511758       1 job_controller.go:598] "enqueueing job" logger="job-controller" key="ingress-nginx/ingress-nginx-admission-create" delay="1s"
I0323 15:29:14.296221       1 range_allocator.go:247] "Successfully synced" logger="node-ipam-controller" key="minikube"
I0323 15:35:22.591250       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="ingress-nginx/ingress-nginx-controller-56d7c84fd4" duration="182.781µs"
I0323 15:35:34.738426       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="ingress-nginx/ingress-nginx-controller-56d7c84fd4" duration="77.777911ms"
I0323 15:35:34.738805       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="ingress-nginx/ingress-nginx-controller-56d7c84fd4" duration="294.881µs"
I0323 15:35:42.802508       1 range_allocator.go:247] "Successfully synced" logger="node-ipam-controller" key="minikube"


==> kube-controller-manager [78e7bf7b330a] <==
I0318 17:50:50.657766       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/authentication-58bbb849c8" duration="62.841µs"
I0318 17:51:03.647376       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/authentication-58bbb849c8" duration="145.411µs"
I0318 17:51:18.692945       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/authentication-58bbb849c8" duration="114.21µs"
I0318 17:51:32.681082       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/authentication-58bbb849c8" duration="149.92µs"
I0318 17:52:17.025389       1 cleaner.go:175] "Cleaning CSR as it is more than approvedExpiration duration old and approved." logger="certificatesigningrequest-cleaner-controller" csr="csr-9tk22" approvedExpiration="1h0m0s"
I0318 17:53:37.683326       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/authentication-58bbb849c8" duration="84.934µs"
I0318 17:53:48.633727       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/authentication-58bbb849c8" duration="91.056µs"
I0318 17:54:13.659095       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/authentication-58bbb849c8" duration="88.13µs"
I0318 17:54:28.654190       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/authentication-58bbb849c8" duration="90.064µs"
I0318 17:55:03.803595       1 range_allocator.go:247] "Successfully synced" logger="node-ipam-controller" key="minikube"
I0318 17:58:50.649404       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/authentication-58bbb849c8" duration="155.89µs"
I0318 17:59:02.648295       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/authentication-58bbb849c8" duration="98.369µs"
I0318 17:59:19.621305       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/authentication-58bbb849c8" duration="158.615µs"
I0318 17:59:30.642014       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/authentication-58bbb849c8" duration="192.32µs"
I0318 18:00:10.236946       1 range_allocator.go:247] "Successfully synced" logger="node-ipam-controller" key="minikube"
I0318 18:03:58.647709       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/authentication-58bbb849c8" duration="192.952µs"
I0318 18:04:09.641713       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/authentication-58bbb849c8" duration="170.689µs"
I0318 18:04:23.658857       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/authentication-58bbb849c8" duration="162.773µs"
I0318 18:04:38.645381       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/authentication-58bbb849c8" duration="85.805µs"
I0318 18:05:16.747730       1 range_allocator.go:247] "Successfully synced" logger="node-ipam-controller" key="minikube"
I0318 18:08:57.642398       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/authentication-58bbb849c8" duration="131.924µs"
I0318 18:09:08.648962       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/authentication-58bbb849c8" duration="206.629µs"
I0318 18:09:28.631902       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/authentication-58bbb849c8" duration="83.662µs"
I0318 18:09:39.650651       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/authentication-58bbb849c8" duration="86.026µs"
I0318 18:10:22.105346       1 range_allocator.go:247] "Successfully synced" logger="node-ipam-controller" key="minikube"
I0318 18:14:05.648490       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/authentication-58bbb849c8" duration="128.839µs"
I0318 18:14:16.641568       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/authentication-58bbb849c8" duration="82.099µs"
I0318 18:14:28.651364       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/authentication-58bbb849c8" duration="171.801µs"
I0318 18:14:39.658214       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/authentication-58bbb849c8" duration="679.37µs"
I0318 18:15:28.164431       1 range_allocator.go:247] "Successfully synced" logger="node-ipam-controller" key="minikube"
I0318 18:19:23.665960       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/authentication-58bbb849c8" duration="150.791µs"
I0318 18:19:32.653103       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/authentication-58bbb849c8" duration="140.26µs"
I0318 18:19:37.680505       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/authentication-58bbb849c8" duration="85.454µs"
I0318 18:19:46.650365       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/authentication-58bbb849c8" duration="106.615µs"
I0318 18:20:34.612705       1 range_allocator.go:247] "Successfully synced" logger="node-ipam-controller" key="minikube"
I0318 18:24:30.659738       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/authentication-58bbb849c8" duration="147.474µs"
I0318 18:24:42.644324       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/authentication-58bbb849c8" duration="136.383µs"
I0318 18:24:43.634456       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/authentication-58bbb849c8" duration="145.431µs"
I0318 18:24:58.697564       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/authentication-58bbb849c8" duration="82.298µs"
I0318 18:25:40.484427       1 range_allocator.go:247] "Successfully synced" logger="node-ipam-controller" key="minikube"
I0318 18:29:35.643276       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/authentication-58bbb849c8" duration="104.171µs"
I0318 18:29:47.651451       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/authentication-58bbb849c8" duration="192.601µs"
I0318 18:29:49.642400       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/authentication-58bbb849c8" duration="85.104µs"
I0318 18:30:02.641737       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/authentication-58bbb849c8" duration="86.317µs"
I0318 18:30:45.746785       1 range_allocator.go:247] "Successfully synced" logger="node-ipam-controller" key="minikube"
I0318 18:34:41.650617       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/authentication-58bbb849c8" duration="144.529µs"
I0318 18:34:53.624506       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/authentication-58bbb849c8" duration="107.377µs"
I0318 18:35:06.649736       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/authentication-58bbb849c8" duration="84.352µs"
I0318 18:35:20.672415       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/authentication-58bbb849c8" duration="164.698µs"
I0318 18:35:51.863714       1 range_allocator.go:247] "Successfully synced" logger="node-ipam-controller" key="minikube"
I0318 18:39:54.649364       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/authentication-58bbb849c8" duration="157.533µs"
I0318 18:40:06.651747       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/authentication-58bbb849c8" duration="89.613µs"
I0318 18:40:08.655319       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/authentication-58bbb849c8" duration="89.242µs"
I0318 18:40:21.624250       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/authentication-58bbb849c8" duration="170.128µs"
I0318 18:40:58.162216       1 range_allocator.go:247] "Successfully synced" logger="node-ipam-controller" key="minikube"
I0318 18:45:05.635476       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/authentication-58bbb849c8" duration="154.789µs"
I0318 18:45:14.622772       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/authentication-58bbb849c8" duration="165.358µs"
I0318 18:45:20.651452       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/authentication-58bbb849c8" duration="79.343µs"
I0318 18:45:28.648251       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/authentication-58bbb849c8" duration="93.45µs"
I0318 18:46:04.621302       1 range_allocator.go:247] "Successfully synced" logger="node-ipam-controller" key="minikube"


==> kube-proxy [0b9688828752] <==
I0318 16:52:28.619485       1 server_linux.go:66] "Using iptables proxy"
I0318 16:52:29.055214       1 server.go:698] "Successfully retrieved node IP(s)" IPs=["192.168.49.2"]
E0318 16:52:29.055467       1 server.go:234] "Kube-proxy configuration may be incomplete or incorrect" err="nodePortAddresses is unset; NodePort connections will be accepted on all local IPs. Consider using `--nodeport-addresses primary`"
I0318 16:52:29.233224       1 server.go:243] "kube-proxy running in dual-stack mode" primary ipFamily="IPv4"
I0318 16:52:29.233596       1 server_linux.go:170] "Using iptables Proxier"
I0318 16:52:29.239903       1 proxier.go:255] "Setting route_localnet=1 to allow node-ports on localhost; to change this either disable iptables.localhostNodePorts (--iptables-localhost-nodeports) or set nodePortAddresses (--nodeport-addresses) to filter loopback addresses" ipFamily="IPv4"
I0318 16:52:29.261031       1 server.go:497] "Version info" version="v1.32.0"
I0318 16:52:29.261125       1 server.go:499] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I0318 16:52:29.294185       1 config.go:199] "Starting service config controller"
I0318 16:52:29.301028       1 shared_informer.go:313] Waiting for caches to sync for service config
I0318 16:52:29.296209       1 config.go:105] "Starting endpoint slice config controller"
I0318 16:52:29.301397       1 shared_informer.go:313] Waiting for caches to sync for endpoint slice config
I0318 16:52:29.300464       1 config.go:329] "Starting node config controller"
I0318 16:52:29.301630       1 shared_informer.go:313] Waiting for caches to sync for node config
I0318 16:52:29.401366       1 shared_informer.go:320] Caches are synced for service config
I0318 16:52:29.402489       1 shared_informer.go:320] Caches are synced for node config
I0318 16:52:29.402511       1 shared_informer.go:320] Caches are synced for endpoint slice config


==> kube-proxy [3b219b59b7f8] <==
I0319 15:19:02.847199       1 server_linux.go:66] "Using iptables proxy"
I0319 15:19:04.808601       1 server.go:698] "Successfully retrieved node IP(s)" IPs=["192.168.49.2"]
E0319 15:19:04.808783       1 server.go:234] "Kube-proxy configuration may be incomplete or incorrect" err="nodePortAddresses is unset; NodePort connections will be accepted on all local IPs. Consider using `--nodeport-addresses primary`"
I0319 15:19:05.273236       1 server.go:243] "kube-proxy running in dual-stack mode" primary ipFamily="IPv4"
I0319 15:19:05.273302       1 server_linux.go:170] "Using iptables Proxier"
I0319 15:19:05.276986       1 proxier.go:255] "Setting route_localnet=1 to allow node-ports on localhost; to change this either disable iptables.localhostNodePorts (--iptables-localhost-nodeports) or set nodePortAddresses (--nodeport-addresses) to filter loopback addresses" ipFamily="IPv4"
I0319 15:19:05.316648       1 server.go:497] "Version info" version="v1.32.0"
I0319 15:19:05.316955       1 server.go:499] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I0319 15:19:05.344323       1 config.go:199] "Starting service config controller"
I0319 15:19:05.344365       1 config.go:105] "Starting endpoint slice config controller"
I0319 15:19:05.344395       1 shared_informer.go:313] Waiting for caches to sync for service config
I0319 15:19:05.344419       1 config.go:329] "Starting node config controller"
I0319 15:19:05.344431       1 shared_informer.go:313] Waiting for caches to sync for node config
I0319 15:19:05.344394       1 shared_informer.go:313] Waiting for caches to sync for endpoint slice config
I0319 15:19:05.444752       1 shared_informer.go:320] Caches are synced for endpoint slice config
I0319 15:19:05.444796       1 shared_informer.go:320] Caches are synced for service config
I0319 15:19:05.445966       1 shared_informer.go:320] Caches are synced for node config


==> kube-scheduler [0940d270f94d] <==
I0319 15:18:18.223273       1 serving.go:386] Generated self-signed cert in-memory
W0319 15:18:22.819939       1 requestheader_controller.go:204] Unable to get configmap/extension-apiserver-authentication in kube-system.  Usually fixed by 'kubectl create rolebinding -n kube-system ROLEBINDING_NAME --role=extension-apiserver-authentication-reader --serviceaccount=YOUR_NS:YOUR_SA'
W0319 15:18:22.819991       1 authentication.go:397] Error looking up in-cluster authentication configuration: configmaps "extension-apiserver-authentication" is forbidden: User "system:kube-scheduler" cannot get resource "configmaps" in API group "" in the namespace "kube-system"
W0319 15:18:22.820007       1 authentication.go:398] Continuing without authentication configuration. This may treat all requests as anonymous.
W0319 15:18:22.820017       1 authentication.go:399] To require authentication configuration lookup to succeed, set --authentication-tolerate-lookup-failure=false
I0319 15:18:23.211588       1 server.go:166] "Starting Kubernetes Scheduler" version="v1.32.0"
I0319 15:18:23.211625       1 server.go:168] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I0319 15:18:23.286867       1 configmap_cafile_content.go:205] "Starting controller" name="client-ca::kube-system::extension-apiserver-authentication::client-ca-file"
I0319 15:18:23.286940       1 shared_informer.go:313] Waiting for caches to sync for client-ca::kube-system::extension-apiserver-authentication::client-ca-file
I0319 15:18:23.287474       1 secure_serving.go:213] Serving securely on 127.0.0.1:10259
I0319 15:18:23.346879       1 tlsconfig.go:243] "Starting DynamicServingCertificateController"
I0319 15:18:23.687173       1 shared_informer.go:320] Caches are synced for client-ca::kube-system::extension-apiserver-authentication::client-ca-file


==> kube-scheduler [a8724693c5ff] <==
E0318 16:52:06.584035       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.CSIDriver: failed to list *v1.CSIDriver: csidrivers.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"csidrivers\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError"
W0318 16:52:06.584127       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.PersistentVolumeClaim: persistentvolumeclaims is forbidden: User "system:kube-scheduler" cannot list resource "persistentvolumeclaims" in API group "" at the cluster scope
E0318 16:52:06.584150       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.PersistentVolumeClaim: failed to list *v1.PersistentVolumeClaim: persistentvolumeclaims is forbidden: User \"system:kube-scheduler\" cannot list resource \"persistentvolumeclaims\" in API group \"\" at the cluster scope" logger="UnhandledError"
W0318 16:52:06.584519       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.StatefulSet: statefulsets.apps is forbidden: User "system:kube-scheduler" cannot list resource "statefulsets" in API group "apps" at the cluster scope
E0318 16:52:06.584542       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.StatefulSet: failed to list *v1.StatefulSet: statefulsets.apps is forbidden: User \"system:kube-scheduler\" cannot list resource \"statefulsets\" in API group \"apps\" at the cluster scope" logger="UnhandledError"
W0318 16:52:06.584720       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.ReplicaSet: replicasets.apps is forbidden: User "system:kube-scheduler" cannot list resource "replicasets" in API group "apps" at the cluster scope
E0318 16:52:06.584929       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.ReplicaSet: failed to list *v1.ReplicaSet: replicasets.apps is forbidden: User \"system:kube-scheduler\" cannot list resource \"replicasets\" in API group \"apps\" at the cluster scope" logger="UnhandledError"
W0318 16:52:06.584739       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.PersistentVolume: persistentvolumes is forbidden: User "system:kube-scheduler" cannot list resource "persistentvolumes" in API group "" at the cluster scope
W0318 16:52:06.584823       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Namespace: namespaces is forbidden: User "system:kube-scheduler" cannot list resource "namespaces" in API group "" at the cluster scope
E0318 16:52:06.585318       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Namespace: failed to list *v1.Namespace: namespaces is forbidden: User \"system:kube-scheduler\" cannot list resource \"namespaces\" in API group \"\" at the cluster scope" logger="UnhandledError"
W0318 16:52:06.584830       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.PodDisruptionBudget: poddisruptionbudgets.policy is forbidden: User "system:kube-scheduler" cannot list resource "poddisruptionbudgets" in API group "policy" at the cluster scope
E0318 16:52:06.585496       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.PodDisruptionBudget: failed to list *v1.PodDisruptionBudget: poddisruptionbudgets.policy is forbidden: User \"system:kube-scheduler\" cannot list resource \"poddisruptionbudgets\" in API group \"policy\" at the cluster scope" logger="UnhandledError"
E0318 16:52:06.585155       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.PersistentVolume: failed to list *v1.PersistentVolume: persistentvolumes is forbidden: User \"system:kube-scheduler\" cannot list resource \"persistentvolumes\" in API group \"\" at the cluster scope" logger="UnhandledError"
W0318 16:52:07.412110       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Service: services is forbidden: User "system:kube-scheduler" cannot list resource "services" in API group "" at the cluster scope
E0318 16:52:07.412179       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Service: failed to list *v1.Service: services is forbidden: User \"system:kube-scheduler\" cannot list resource \"services\" in API group \"\" at the cluster scope" logger="UnhandledError"
W0318 16:52:07.416867       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.StatefulSet: statefulsets.apps is forbidden: User "system:kube-scheduler" cannot list resource "statefulsets" in API group "apps" at the cluster scope
E0318 16:52:07.416962       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.StatefulSet: failed to list *v1.StatefulSet: statefulsets.apps is forbidden: User \"system:kube-scheduler\" cannot list resource \"statefulsets\" in API group \"apps\" at the cluster scope" logger="UnhandledError"
W0318 16:52:07.534986       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Pod: pods is forbidden: User "system:kube-scheduler" cannot list resource "pods" in API group "" at the cluster scope
E0318 16:52:07.535054       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Pod: failed to list *v1.Pod: pods is forbidden: User \"system:kube-scheduler\" cannot list resource \"pods\" in API group \"\" at the cluster scope" logger="UnhandledError"
W0318 16:52:07.540245       1 reflector.go:569] runtime/asm_amd64.s:1700: failed to list *v1.ConfigMap: configmaps "extension-apiserver-authentication" is forbidden: User "system:kube-scheduler" cannot list resource "configmaps" in API group "" in the namespace "kube-system"
E0318 16:52:07.540338       1 reflector.go:166] "Unhandled Error" err="runtime/asm_amd64.s:1700: Failed to watch *v1.ConfigMap: failed to list *v1.ConfigMap: configmaps \"extension-apiserver-authentication\" is forbidden: User \"system:kube-scheduler\" cannot list resource \"configmaps\" in API group \"\" in the namespace \"kube-system\"" logger="UnhandledError"
W0318 16:52:07.624371       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.PodDisruptionBudget: poddisruptionbudgets.policy is forbidden: User "system:kube-scheduler" cannot list resource "poddisruptionbudgets" in API group "policy" at the cluster scope
E0318 16:52:07.624446       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.PodDisruptionBudget: failed to list *v1.PodDisruptionBudget: poddisruptionbudgets.policy is forbidden: User \"system:kube-scheduler\" cannot list resource \"poddisruptionbudgets\" in API group \"policy\" at the cluster scope" logger="UnhandledError"
W0318 16:52:07.724215       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.CSIStorageCapacity: csistoragecapacities.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csistoragecapacities" in API group "storage.k8s.io" at the cluster scope
E0318 16:52:07.724321       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.CSIStorageCapacity: failed to list *v1.CSIStorageCapacity: csistoragecapacities.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"csistoragecapacities\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError"
W0318 16:52:07.732629       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.StorageClass: storageclasses.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "storageclasses" in API group "storage.k8s.io" at the cluster scope
E0318 16:52:07.732699       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.StorageClass: failed to list *v1.StorageClass: storageclasses.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"storageclasses\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError"
W0318 16:52:07.746965       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Node: nodes is forbidden: User "system:kube-scheduler" cannot list resource "nodes" in API group "" at the cluster scope
E0318 16:52:07.747034       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Node: failed to list *v1.Node: nodes is forbidden: User \"system:kube-scheduler\" cannot list resource \"nodes\" in API group \"\" at the cluster scope" logger="UnhandledError"
W0318 16:52:07.796463       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.ReplicationController: replicationcontrollers is forbidden: User "system:kube-scheduler" cannot list resource "replicationcontrollers" in API group "" at the cluster scope
E0318 16:52:07.796582       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.ReplicationController: failed to list *v1.ReplicationController: replicationcontrollers is forbidden: User \"system:kube-scheduler\" cannot list resource \"replicationcontrollers\" in API group \"\" at the cluster scope" logger="UnhandledError"
W0318 16:52:07.844995       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.CSINode: csinodes.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csinodes" in API group "storage.k8s.io" at the cluster scope
E0318 16:52:07.845062       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.CSINode: failed to list *v1.CSINode: csinodes.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"csinodes\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError"
W0318 16:52:08.026619       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.PersistentVolumeClaim: persistentvolumeclaims is forbidden: User "system:kube-scheduler" cannot list resource "persistentvolumeclaims" in API group "" at the cluster scope
E0318 16:52:08.026701       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.PersistentVolumeClaim: failed to list *v1.PersistentVolumeClaim: persistentvolumeclaims is forbidden: User \"system:kube-scheduler\" cannot list resource \"persistentvolumeclaims\" in API group \"\" at the cluster scope" logger="UnhandledError"
W0318 16:52:08.027050       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.VolumeAttachment: volumeattachments.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "volumeattachments" in API group "storage.k8s.io" at the cluster scope
E0318 16:52:08.027086       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.VolumeAttachment: failed to list *v1.VolumeAttachment: volumeattachments.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"volumeattachments\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError"
W0318 16:52:08.027047       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.PersistentVolume: persistentvolumes is forbidden: User "system:kube-scheduler" cannot list resource "persistentvolumes" in API group "" at the cluster scope
E0318 16:52:08.027244       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.PersistentVolume: failed to list *v1.PersistentVolume: persistentvolumes is forbidden: User \"system:kube-scheduler\" cannot list resource \"persistentvolumes\" in API group \"\" at the cluster scope" logger="UnhandledError"
W0318 16:52:08.065263       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.CSIDriver: csidrivers.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csidrivers" in API group "storage.k8s.io" at the cluster scope
E0318 16:52:08.065345       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.CSIDriver: failed to list *v1.CSIDriver: csidrivers.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"csidrivers\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError"
W0318 16:52:08.130202       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.ReplicaSet: replicasets.apps is forbidden: User "system:kube-scheduler" cannot list resource "replicasets" in API group "apps" at the cluster scope
E0318 16:52:08.130269       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.ReplicaSet: failed to list *v1.ReplicaSet: replicasets.apps is forbidden: User \"system:kube-scheduler\" cannot list resource \"replicasets\" in API group \"apps\" at the cluster scope" logger="UnhandledError"
W0318 16:52:08.162781       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Namespace: namespaces is forbidden: User "system:kube-scheduler" cannot list resource "namespaces" in API group "" at the cluster scope
E0318 16:52:08.162850       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Namespace: failed to list *v1.Namespace: namespaces is forbidden: User \"system:kube-scheduler\" cannot list resource \"namespaces\" in API group \"\" at the cluster scope" logger="UnhandledError"
W0318 16:52:09.436719       1 reflector.go:569] runtime/asm_amd64.s:1700: failed to list *v1.ConfigMap: configmaps "extension-apiserver-authentication" is forbidden: User "system:kube-scheduler" cannot list resource "configmaps" in API group "" in the namespace "kube-system"
E0318 16:52:09.436801       1 reflector.go:166] "Unhandled Error" err="runtime/asm_amd64.s:1700: Failed to watch *v1.ConfigMap: failed to list *v1.ConfigMap: configmaps \"extension-apiserver-authentication\" is forbidden: User \"system:kube-scheduler\" cannot list resource \"configmaps\" in API group \"\" in the namespace \"kube-system\"" logger="UnhandledError"
W0318 16:52:09.560473       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.PodDisruptionBudget: poddisruptionbudgets.policy is forbidden: User "system:kube-scheduler" cannot list resource "poddisruptionbudgets" in API group "policy" at the cluster scope
E0318 16:52:09.560549       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.PodDisruptionBudget: failed to list *v1.PodDisruptionBudget: poddisruptionbudgets.policy is forbidden: User \"system:kube-scheduler\" cannot list resource \"poddisruptionbudgets\" in API group \"policy\" at the cluster scope" logger="UnhandledError"
W0318 16:52:09.744988       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Pod: pods is forbidden: User "system:kube-scheduler" cannot list resource "pods" in API group "" at the cluster scope
E0318 16:52:09.745094       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Pod: failed to list *v1.Pod: pods is forbidden: User \"system:kube-scheduler\" cannot list resource \"pods\" in API group \"\" at the cluster scope" logger="UnhandledError"
W0318 16:52:09.805322       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.CSINode: csinodes.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csinodes" in API group "storage.k8s.io" at the cluster scope
E0318 16:52:09.805403       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.CSINode: failed to list *v1.CSINode: csinodes.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"csinodes\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError"
W0318 16:52:09.943209       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.CSIDriver: csidrivers.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csidrivers" in API group "storage.k8s.io" at the cluster scope
E0318 16:52:09.943322       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.CSIDriver: failed to list *v1.CSIDriver: csidrivers.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"csidrivers\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError"
I0318 16:52:12.874517       1 shared_informer.go:320] Caches are synced for client-ca::kube-system::extension-apiserver-authentication::client-ca-file
I0318 18:48:03.052955       1 secure_serving.go:258] Stopped listening on 127.0.0.1:10259
I0318 18:48:03.057411       1 configmap_cafile_content.go:226] "Shutting down controller" name="client-ca::kube-system::extension-apiserver-authentication::client-ca-file"
I0318 18:48:03.057507       1 tlsconfig.go:258] "Shutting down DynamicServingCertificateController"
E0318 18:48:05.015198       1 run.go:72] "command failed" err="finished without leader elect"


==> kubelet <==
Mar 19 15:38:13 minikube kubelet[1243]: I0319 15:38:13.130101    1243 memory_manager.go:355] "RemoveStaleState removing state" podUID="d3fd4514-3161-4b65-a62e-b0840aae2c5b" containerName="authentication"
Mar 19 15:38:13 minikube kubelet[1243]: I0319 15:38:13.174470    1243 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-k9njm\" (UniqueName: \"kubernetes.io/projected/2abf5190-29f8-45c8-b17f-48a18e92353f-kube-api-access-k9njm\") pod \"authentication-5857596898-w5pdq\" (UID: \"2abf5190-29f8-45c8-b17f-48a18e92353f\") " pod="default/authentication-5857596898-w5pdq"
Mar 19 15:38:13 minikube kubelet[1243]: I0319 15:38:13.275294    1243 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-lzr25\" (UniqueName: \"kubernetes.io/projected/d42f6e4b-6b39-489b-8214-62fbe2f16b4c-kube-api-access-lzr25\") pod \"authentication-5857596898-ctrdg\" (UID: \"d42f6e4b-6b39-489b-8214-62fbe2f16b4c\") " pod="default/authentication-5857596898-ctrdg"
Mar 19 15:38:15 minikube kubelet[1243]: I0319 15:38:15.568396    1243 pod_container_deletor.go:80] "Container not found in pod's containers" containerID="d538627d00b8d26f189ee2e9bccd5985d2102e5777325b703b9319038ec35079"
Mar 19 15:38:15 minikube kubelet[1243]: I0319 15:38:15.573409    1243 pod_container_deletor.go:80] "Container not found in pod's containers" containerID="b3d137a6d3041f8a35542aa8d678add80f6b268b330738772773d1e5046208c4"
Mar 19 15:38:22 minikube kubelet[1243]: I0319 15:38:22.855226    1243 pod_startup_latency_tracker.go:104] "Observed pod startup duration" pod="default/authentication-5857596898-ctrdg" podStartSLOduration=4.530627975 podStartE2EDuration="9.855185703s" podCreationTimestamp="2025-03-19 15:38:13 +0000 UTC" firstStartedPulling="2025-03-19 15:38:15.637090882 +0000 UTC m=+1219.971679824" lastFinishedPulling="2025-03-19 15:38:20.96164858 +0000 UTC m=+1225.296237552" observedRunningTime="2025-03-19 15:38:22.742359192 +0000 UTC m=+1227.076948195" watchObservedRunningTime="2025-03-19 15:38:22.855185703 +0000 UTC m=+1227.189774685"
Mar 19 15:38:28 minikube kubelet[1243]: I0319 15:38:28.007622    1243 pod_startup_latency_tracker.go:104] "Observed pod startup duration" pod="default/authentication-5857596898-w5pdq" podStartSLOduration=4.681150098 podStartE2EDuration="15.007586246s" podCreationTimestamp="2025-03-19 15:38:13 +0000 UTC" firstStartedPulling="2025-03-19 15:38:15.637153454 +0000 UTC m=+1219.971742587" lastFinishedPulling="2025-03-19 15:38:25.963589753 +0000 UTC m=+1230.298178735" observedRunningTime="2025-03-19 15:38:27.89356439 +0000 UTC m=+1232.228153332" watchObservedRunningTime="2025-03-19 15:38:28.007586246 +0000 UTC m=+1232.342175228"
Mar 19 15:38:54 minikube kubelet[1243]: I0319 15:38:54.129093    1243 reconciler_common.go:162] "operationExecutor.UnmountVolume started for volume \"kube-api-access-8bmw4\" (UniqueName: \"kubernetes.io/projected/4908e7e0-6b55-4eb0-84b7-6f84cd43f96c-kube-api-access-8bmw4\") pod \"4908e7e0-6b55-4eb0-84b7-6f84cd43f96c\" (UID: \"4908e7e0-6b55-4eb0-84b7-6f84cd43f96c\") "
Mar 19 15:38:54 minikube kubelet[1243]: I0319 15:38:54.132784    1243 operation_generator.go:780] UnmountVolume.TearDown succeeded for volume "kubernetes.io/projected/4908e7e0-6b55-4eb0-84b7-6f84cd43f96c-kube-api-access-8bmw4" (OuterVolumeSpecName: "kube-api-access-8bmw4") pod "4908e7e0-6b55-4eb0-84b7-6f84cd43f96c" (UID: "4908e7e0-6b55-4eb0-84b7-6f84cd43f96c"). InnerVolumeSpecName "kube-api-access-8bmw4". PluginName "kubernetes.io/projected", VolumeGIDValue ""
Mar 19 15:38:54 minikube kubelet[1243]: I0319 15:38:54.183873    1243 scope.go:117] "RemoveContainer" containerID="a1b26d537fffc59fde497aa63e80dc582938793028d5ae41e2b6868ac5210c62"
Mar 19 15:38:54 minikube kubelet[1243]: I0319 15:38:54.229928    1243 reconciler_common.go:299] "Volume detached for volume \"kube-api-access-8bmw4\" (UniqueName: \"kubernetes.io/projected/4908e7e0-6b55-4eb0-84b7-6f84cd43f96c-kube-api-access-8bmw4\") on node \"minikube\" DevicePath \"\""
Mar 19 15:38:54 minikube kubelet[1243]: I0319 15:38:54.289210    1243 scope.go:117] "RemoveContainer" containerID="a1b26d537fffc59fde497aa63e80dc582938793028d5ae41e2b6868ac5210c62"
Mar 19 15:38:54 minikube kubelet[1243]: E0319 15:38:54.290775    1243 log.go:32] "ContainerStatus from runtime service failed" err="rpc error: code = Unknown desc = Error response from daemon: No such container: a1b26d537fffc59fde497aa63e80dc582938793028d5ae41e2b6868ac5210c62" containerID="a1b26d537fffc59fde497aa63e80dc582938793028d5ae41e2b6868ac5210c62"
Mar 19 15:38:54 minikube kubelet[1243]: I0319 15:38:54.290860    1243 pod_container_deletor.go:53] "DeleteContainer returned error" containerID={"Type":"docker","ID":"a1b26d537fffc59fde497aa63e80dc582938793028d5ae41e2b6868ac5210c62"} err="failed to get container status \"a1b26d537fffc59fde497aa63e80dc582938793028d5ae41e2b6868ac5210c62\": rpc error: code = Unknown desc = Error response from daemon: No such container: a1b26d537fffc59fde497aa63e80dc582938793028d5ae41e2b6868ac5210c62"
Mar 19 15:38:55 minikube kubelet[1243]: I0319 15:38:55.013112    1243 kubelet_volumes.go:163] "Cleaned up orphaned pod volumes dir" podUID="4908e7e0-6b55-4eb0-84b7-6f84cd43f96c" path="/var/lib/kubelet/pods/4908e7e0-6b55-4eb0-84b7-6f84cd43f96c/volumes"
Mar 19 15:38:59 minikube kubelet[1243]: I0319 15:38:59.061164    1243 reconciler_common.go:162] "operationExecutor.UnmountVolume started for volume \"kube-api-access-2xhj6\" (UniqueName: \"kubernetes.io/projected/b54dbce8-9122-4940-9f3b-fb62a64c008c-kube-api-access-2xhj6\") pod \"b54dbce8-9122-4940-9f3b-fb62a64c008c\" (UID: \"b54dbce8-9122-4940-9f3b-fb62a64c008c\") "
Mar 19 15:38:59 minikube kubelet[1243]: I0319 15:38:59.064379    1243 operation_generator.go:780] UnmountVolume.TearDown succeeded for volume "kubernetes.io/projected/b54dbce8-9122-4940-9f3b-fb62a64c008c-kube-api-access-2xhj6" (OuterVolumeSpecName: "kube-api-access-2xhj6") pod "b54dbce8-9122-4940-9f3b-fb62a64c008c" (UID: "b54dbce8-9122-4940-9f3b-fb62a64c008c"). InnerVolumeSpecName "kube-api-access-2xhj6". PluginName "kubernetes.io/projected", VolumeGIDValue ""
Mar 19 15:38:59 minikube kubelet[1243]: I0319 15:38:59.162377    1243 reconciler_common.go:299] "Volume detached for volume \"kube-api-access-2xhj6\" (UniqueName: \"kubernetes.io/projected/b54dbce8-9122-4940-9f3b-fb62a64c008c-kube-api-access-2xhj6\") on node \"minikube\" DevicePath \"\""
Mar 19 15:38:59 minikube kubelet[1243]: I0319 15:38:59.256266    1243 scope.go:117] "RemoveContainer" containerID="945fdf24169550824abd616e7717616e9db667d943c751ed2a7021bc5133e7cd"
Mar 19 15:38:59 minikube kubelet[1243]: I0319 15:38:59.360105    1243 scope.go:117] "RemoveContainer" containerID="945fdf24169550824abd616e7717616e9db667d943c751ed2a7021bc5133e7cd"
Mar 19 15:38:59 minikube kubelet[1243]: E0319 15:38:59.361076    1243 log.go:32] "ContainerStatus from runtime service failed" err="rpc error: code = Unknown desc = Error response from daemon: No such container: 945fdf24169550824abd616e7717616e9db667d943c751ed2a7021bc5133e7cd" containerID="945fdf24169550824abd616e7717616e9db667d943c751ed2a7021bc5133e7cd"
Mar 19 15:38:59 minikube kubelet[1243]: I0319 15:38:59.361119    1243 pod_container_deletor.go:53] "DeleteContainer returned error" containerID={"Type":"docker","ID":"945fdf24169550824abd616e7717616e9db667d943c751ed2a7021bc5133e7cd"} err="failed to get container status \"945fdf24169550824abd616e7717616e9db667d943c751ed2a7021bc5133e7cd\": rpc error: code = Unknown desc = Error response from daemon: No such container: 945fdf24169550824abd616e7717616e9db667d943c751ed2a7021bc5133e7cd"
Mar 19 15:39:01 minikube kubelet[1243]: I0319 15:39:01.008186    1243 kubelet_volumes.go:163] "Cleaned up orphaned pod volumes dir" podUID="b54dbce8-9122-4940-9f3b-fb62a64c008c" path="/var/lib/kubelet/pods/b54dbce8-9122-4940-9f3b-fb62a64c008c/volumes"
Mar 19 15:45:25 minikube kubelet[1243]: E0319 15:45:25.421030    1243 upgradeaware.go:427] Error proxying data from client to backend: readfrom tcp [::1]:53860->[::1]:45593: write tcp [::1]:53860->[::1]:45593: write: broken pipe
Mar 20 15:04:12 minikube kubelet[1243]: E0320 15:04:12.739004    1243 kubelet.go:2579] "Housekeeping took longer than expected" err="housekeeping took too long" expected="1s" actual="1.15s"
Mar 23 15:27:11 minikube kubelet[1243]: I0323 15:27:11.860326    1243 memory_manager.go:355] "RemoveStaleState removing state" podUID="b54dbce8-9122-4940-9f3b-fb62a64c008c" containerName="authentication"
Mar 23 15:27:11 minikube kubelet[1243]: I0323 15:27:11.860404    1243 memory_manager.go:355] "RemoveStaleState removing state" podUID="4908e7e0-6b55-4eb0-84b7-6f84cd43f96c" containerName="authentication"
Mar 23 15:27:11 minikube kubelet[1243]: I0323 15:27:11.945389    1243 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-2bwlj\" (UniqueName: \"kubernetes.io/projected/87aacd3a-06ef-433e-84be-45bd66d997f5-kube-api-access-2bwlj\") pod \"ingress-nginx-admission-create-74zdf\" (UID: \"87aacd3a-06ef-433e-84be-45bd66d997f5\") " pod="ingress-nginx/ingress-nginx-admission-create-74zdf"
Mar 23 15:27:11 minikube kubelet[1243]: I0323 15:27:11.945537    1243 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-8sp2w\" (UniqueName: \"kubernetes.io/projected/eaaa516a-9188-439c-aaaa-84a4fe5fefe1-kube-api-access-8sp2w\") pod \"ingress-nginx-admission-patch-qz5hf\" (UID: \"eaaa516a-9188-439c-aaaa-84a4fe5fefe1\") " pod="ingress-nginx/ingress-nginx-admission-patch-qz5hf"
Mar 23 15:27:11 minikube kubelet[1243]: I0323 15:27:11.945585    1243 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-ktmxg\" (UniqueName: \"kubernetes.io/projected/99525bc5-38e3-44e1-b967-8c1090f45aa3-kube-api-access-ktmxg\") pod \"ingress-nginx-controller-56d7c84fd4-dccrg\" (UID: \"99525bc5-38e3-44e1-b967-8c1090f45aa3\") " pod="ingress-nginx/ingress-nginx-controller-56d7c84fd4-dccrg"
Mar 23 15:27:11 minikube kubelet[1243]: I0323 15:27:11.945672    1243 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"webhook-cert\" (UniqueName: \"kubernetes.io/secret/99525bc5-38e3-44e1-b967-8c1090f45aa3-webhook-cert\") pod \"ingress-nginx-controller-56d7c84fd4-dccrg\" (UID: \"99525bc5-38e3-44e1-b967-8c1090f45aa3\") " pod="ingress-nginx/ingress-nginx-controller-56d7c84fd4-dccrg"
Mar 23 15:27:12 minikube kubelet[1243]: E0323 15:27:12.047255    1243 secret.go:189] Couldn't get secret ingress-nginx/ingress-nginx-admission: secret "ingress-nginx-admission" not found
Mar 23 15:27:12 minikube kubelet[1243]: E0323 15:27:12.047408    1243 nestedpendingoperations.go:348] Operation for "{volumeName:kubernetes.io/secret/99525bc5-38e3-44e1-b967-8c1090f45aa3-webhook-cert podName:99525bc5-38e3-44e1-b967-8c1090f45aa3 nodeName:}" failed. No retries permitted until 2025-03-23 15:27:12.547373431 +0000 UTC m=+58377.739463653 (durationBeforeRetry 500ms). Error: MountVolume.SetUp failed for volume "webhook-cert" (UniqueName: "kubernetes.io/secret/99525bc5-38e3-44e1-b967-8c1090f45aa3-webhook-cert") pod "ingress-nginx-controller-56d7c84fd4-dccrg" (UID: "99525bc5-38e3-44e1-b967-8c1090f45aa3") : secret "ingress-nginx-admission" not found
Mar 23 15:27:12 minikube kubelet[1243]: E0323 15:27:12.549212    1243 secret.go:189] Couldn't get secret ingress-nginx/ingress-nginx-admission: secret "ingress-nginx-admission" not found
Mar 23 15:27:12 minikube kubelet[1243]: E0323 15:27:12.549349    1243 nestedpendingoperations.go:348] Operation for "{volumeName:kubernetes.io/secret/99525bc5-38e3-44e1-b967-8c1090f45aa3-webhook-cert podName:99525bc5-38e3-44e1-b967-8c1090f45aa3 nodeName:}" failed. No retries permitted until 2025-03-23 15:27:13.549318755 +0000 UTC m=+58378.741408887 (durationBeforeRetry 1s). Error: MountVolume.SetUp failed for volume "webhook-cert" (UniqueName: "kubernetes.io/secret/99525bc5-38e3-44e1-b967-8c1090f45aa3-webhook-cert") pod "ingress-nginx-controller-56d7c84fd4-dccrg" (UID: "99525bc5-38e3-44e1-b967-8c1090f45aa3") : secret "ingress-nginx-admission" not found
Mar 23 15:27:13 minikube kubelet[1243]: E0323 15:27:13.557278    1243 secret.go:189] Couldn't get secret ingress-nginx/ingress-nginx-admission: secret "ingress-nginx-admission" not found
Mar 23 15:27:13 minikube kubelet[1243]: E0323 15:27:13.557418    1243 nestedpendingoperations.go:348] Operation for "{volumeName:kubernetes.io/secret/99525bc5-38e3-44e1-b967-8c1090f45aa3-webhook-cert podName:99525bc5-38e3-44e1-b967-8c1090f45aa3 nodeName:}" failed. No retries permitted until 2025-03-23 15:27:15.557382968 +0000 UTC m=+58380.749473141 (durationBeforeRetry 2s). Error: MountVolume.SetUp failed for volume "webhook-cert" (UniqueName: "kubernetes.io/secret/99525bc5-38e3-44e1-b967-8c1090f45aa3-webhook-cert") pod "ingress-nginx-controller-56d7c84fd4-dccrg" (UID: "99525bc5-38e3-44e1-b967-8c1090f45aa3") : secret "ingress-nginx-admission" not found
Mar 23 15:27:14 minikube kubelet[1243]: I0323 15:27:14.412315    1243 pod_container_deletor.go:80] "Container not found in pod's containers" containerID="2ee17dd8e12aeef85e1f36f80a9aa785d39088b064ed0e99b868bce24c725cb5"
Mar 23 15:27:14 minikube kubelet[1243]: I0323 15:27:14.436800    1243 pod_container_deletor.go:80] "Container not found in pod's containers" containerID="fd083f77d6c5120970aac15190a568c577f5a5a4311a3f32ac315b0a04b89e0d"
Mar 23 15:27:15 minikube kubelet[1243]: E0323 15:27:15.574410    1243 secret.go:189] Couldn't get secret ingress-nginx/ingress-nginx-admission: secret "ingress-nginx-admission" not found
Mar 23 15:27:15 minikube kubelet[1243]: E0323 15:27:15.574595    1243 nestedpendingoperations.go:348] Operation for "{volumeName:kubernetes.io/secret/99525bc5-38e3-44e1-b967-8c1090f45aa3-webhook-cert podName:99525bc5-38e3-44e1-b967-8c1090f45aa3 nodeName:}" failed. No retries permitted until 2025-03-23 15:27:19.57455483 +0000 UTC m=+58384.766644992 (durationBeforeRetry 4s). Error: MountVolume.SetUp failed for volume "webhook-cert" (UniqueName: "kubernetes.io/secret/99525bc5-38e3-44e1-b967-8c1090f45aa3-webhook-cert") pod "ingress-nginx-controller-56d7c84fd4-dccrg" (UID: "99525bc5-38e3-44e1-b967-8c1090f45aa3") : secret "ingress-nginx-admission" not found
Mar 23 15:27:19 minikube kubelet[1243]: E0323 15:27:19.604226    1243 secret.go:189] Couldn't get secret ingress-nginx/ingress-nginx-admission: secret "ingress-nginx-admission" not found
Mar 23 15:27:19 minikube kubelet[1243]: E0323 15:27:19.604793    1243 nestedpendingoperations.go:348] Operation for "{volumeName:kubernetes.io/secret/99525bc5-38e3-44e1-b967-8c1090f45aa3-webhook-cert podName:99525bc5-38e3-44e1-b967-8c1090f45aa3 nodeName:}" failed. No retries permitted until 2025-03-23 15:27:27.604744461 +0000 UTC m=+58392.796834593 (durationBeforeRetry 8s). Error: MountVolume.SetUp failed for volume "webhook-cert" (UniqueName: "kubernetes.io/secret/99525bc5-38e3-44e1-b967-8c1090f45aa3-webhook-cert") pod "ingress-nginx-controller-56d7c84fd4-dccrg" (UID: "99525bc5-38e3-44e1-b967-8c1090f45aa3") : secret "ingress-nginx-admission" not found
Mar 23 15:27:27 minikube kubelet[1243]: E0323 15:27:27.670904    1243 secret.go:189] Couldn't get secret ingress-nginx/ingress-nginx-admission: secret "ingress-nginx-admission" not found
Mar 23 15:27:27 minikube kubelet[1243]: E0323 15:27:27.671028    1243 nestedpendingoperations.go:348] Operation for "{volumeName:kubernetes.io/secret/99525bc5-38e3-44e1-b967-8c1090f45aa3-webhook-cert podName:99525bc5-38e3-44e1-b967-8c1090f45aa3 nodeName:}" failed. No retries permitted until 2025-03-23 15:27:43.670993734 +0000 UTC m=+58408.863083877 (durationBeforeRetry 16s). Error: MountVolume.SetUp failed for volume "webhook-cert" (UniqueName: "kubernetes.io/secret/99525bc5-38e3-44e1-b967-8c1090f45aa3-webhook-cert") pod "ingress-nginx-controller-56d7c84fd4-dccrg" (UID: "99525bc5-38e3-44e1-b967-8c1090f45aa3") : secret "ingress-nginx-admission" not found
Mar 23 15:27:43 minikube kubelet[1243]: E0323 15:27:43.699585    1243 secret.go:189] Couldn't get secret ingress-nginx/ingress-nginx-admission: secret "ingress-nginx-admission" not found
Mar 23 15:27:43 minikube kubelet[1243]: E0323 15:27:43.699710    1243 nestedpendingoperations.go:348] Operation for "{volumeName:kubernetes.io/secret/99525bc5-38e3-44e1-b967-8c1090f45aa3-webhook-cert podName:99525bc5-38e3-44e1-b967-8c1090f45aa3 nodeName:}" failed. No retries permitted until 2025-03-23 15:28:15.699676878 +0000 UTC m=+58440.891767030 (durationBeforeRetry 32s). Error: MountVolume.SetUp failed for volume "webhook-cert" (UniqueName: "kubernetes.io/secret/99525bc5-38e3-44e1-b967-8c1090f45aa3-webhook-cert") pod "ingress-nginx-controller-56d7c84fd4-dccrg" (UID: "99525bc5-38e3-44e1-b967-8c1090f45aa3") : secret "ingress-nginx-admission" not found
Mar 23 15:28:15 minikube kubelet[1243]: E0323 15:28:15.727549    1243 secret.go:189] Couldn't get secret ingress-nginx/ingress-nginx-admission: secret "ingress-nginx-admission" not found
Mar 23 15:28:15 minikube kubelet[1243]: E0323 15:28:15.727737    1243 nestedpendingoperations.go:348] Operation for "{volumeName:kubernetes.io/secret/99525bc5-38e3-44e1-b967-8c1090f45aa3-webhook-cert podName:99525bc5-38e3-44e1-b967-8c1090f45aa3 nodeName:}" failed. No retries permitted until 2025-03-23 15:29:19.727690127 +0000 UTC m=+58504.919780269 (durationBeforeRetry 1m4s). Error: MountVolume.SetUp failed for volume "webhook-cert" (UniqueName: "kubernetes.io/secret/99525bc5-38e3-44e1-b967-8c1090f45aa3-webhook-cert") pod "ingress-nginx-controller-56d7c84fd4-dccrg" (UID: "99525bc5-38e3-44e1-b967-8c1090f45aa3") : secret "ingress-nginx-admission" not found
Mar 23 15:28:59 minikube kubelet[1243]: I0323 15:28:59.252709    1243 reconciler_common.go:162] "operationExecutor.UnmountVolume started for volume \"kube-api-access-8sp2w\" (UniqueName: \"kubernetes.io/projected/eaaa516a-9188-439c-aaaa-84a4fe5fefe1-kube-api-access-8sp2w\") pod \"eaaa516a-9188-439c-aaaa-84a4fe5fefe1\" (UID: \"eaaa516a-9188-439c-aaaa-84a4fe5fefe1\") "
Mar 23 15:28:59 minikube kubelet[1243]: I0323 15:28:59.252792    1243 reconciler_common.go:162] "operationExecutor.UnmountVolume started for volume \"kube-api-access-2bwlj\" (UniqueName: \"kubernetes.io/projected/87aacd3a-06ef-433e-84be-45bd66d997f5-kube-api-access-2bwlj\") pod \"87aacd3a-06ef-433e-84be-45bd66d997f5\" (UID: \"87aacd3a-06ef-433e-84be-45bd66d997f5\") "
Mar 23 15:28:59 minikube kubelet[1243]: I0323 15:28:59.256917    1243 operation_generator.go:780] UnmountVolume.TearDown succeeded for volume "kubernetes.io/projected/eaaa516a-9188-439c-aaaa-84a4fe5fefe1-kube-api-access-8sp2w" (OuterVolumeSpecName: "kube-api-access-8sp2w") pod "eaaa516a-9188-439c-aaaa-84a4fe5fefe1" (UID: "eaaa516a-9188-439c-aaaa-84a4fe5fefe1"). InnerVolumeSpecName "kube-api-access-8sp2w". PluginName "kubernetes.io/projected", VolumeGIDValue ""
Mar 23 15:28:59 minikube kubelet[1243]: I0323 15:28:59.257767    1243 operation_generator.go:780] UnmountVolume.TearDown succeeded for volume "kubernetes.io/projected/87aacd3a-06ef-433e-84be-45bd66d997f5-kube-api-access-2bwlj" (OuterVolumeSpecName: "kube-api-access-2bwlj") pod "87aacd3a-06ef-433e-84be-45bd66d997f5" (UID: "87aacd3a-06ef-433e-84be-45bd66d997f5"). InnerVolumeSpecName "kube-api-access-2bwlj". PluginName "kubernetes.io/projected", VolumeGIDValue ""
Mar 23 15:28:59 minikube kubelet[1243]: I0323 15:28:59.354087    1243 reconciler_common.go:299] "Volume detached for volume \"kube-api-access-2bwlj\" (UniqueName: \"kubernetes.io/projected/87aacd3a-06ef-433e-84be-45bd66d997f5-kube-api-access-2bwlj\") on node \"minikube\" DevicePath \"\""
Mar 23 15:28:59 minikube kubelet[1243]: I0323 15:28:59.354162    1243 reconciler_common.go:299] "Volume detached for volume \"kube-api-access-8sp2w\" (UniqueName: \"kubernetes.io/projected/eaaa516a-9188-439c-aaaa-84a4fe5fefe1-kube-api-access-8sp2w\") on node \"minikube\" DevicePath \"\""
Mar 23 15:28:59 minikube kubelet[1243]: I0323 15:28:59.371866    1243 pod_container_deletor.go:80] "Container not found in pod's containers" containerID="fd083f77d6c5120970aac15190a568c577f5a5a4311a3f32ac315b0a04b89e0d"
Mar 23 15:28:59 minikube kubelet[1243]: I0323 15:28:59.394897    1243 pod_container_deletor.go:80] "Container not found in pod's containers" containerID="2ee17dd8e12aeef85e1f36f80a9aa785d39088b064ed0e99b868bce24c725cb5"
Mar 23 15:29:14 minikube kubelet[1243]: E0323 15:29:14.874290    1243 pod_workers.go:1301] "Error syncing pod, skipping" err="unmounted volumes=[webhook-cert], unattached volumes=[], failed to process volumes=[]: context deadline exceeded" pod="ingress-nginx/ingress-nginx-controller-56d7c84fd4-dccrg" podUID="99525bc5-38e3-44e1-b967-8c1090f45aa3"
Mar 23 15:29:31 minikube kubelet[1243]: I0323 15:29:31.392637    1243 pod_container_deletor.go:80] "Container not found in pod's containers" containerID="7f07a18c8bdb4726dad38af8e458a2b5ec006715c356983863156afa2819869f"
Mar 23 15:35:22 minikube kubelet[1243]: I0323 15:35:22.591787    1243 pod_startup_latency_tracker.go:104] "Observed pod startup duration" pod="ingress-nginx/ingress-nginx-controller-56d7c84fd4-dccrg" podStartSLOduration=143.40445622 podStartE2EDuration="8m11.591737815s" podCreationTimestamp="2025-03-23 15:27:11 +0000 UTC" firstStartedPulling="2025-03-23 15:29:31.654430453 +0000 UTC m=+58516.846520575" lastFinishedPulling="2025-03-23 15:35:19.841712018 +0000 UTC m=+58865.033802170" observedRunningTime="2025-03-23 15:35:22.590039393 +0000 UTC m=+58867.782129565" watchObservedRunningTime="2025-03-23 15:35:22.591737815 +0000 UTC m=+58867.783827957"


==> storage-provisioner [67738fa2de9f] <==
I0319 15:19:32.345382       1 storage_provisioner.go:116] Initializing the minikube storage provisioner...
I0319 15:19:32.526960       1 storage_provisioner.go:141] Storage provisioner initialized, now starting service!
I0319 15:19:32.527185       1 leaderelection.go:243] attempting to acquire leader lease kube-system/k8s.io-minikube-hostpath...
I0319 15:19:49.965935       1 leaderelection.go:253] successfully acquired lease kube-system/k8s.io-minikube-hostpath
I0319 15:19:49.966143       1 controller.go:835] Starting provisioner controller k8s.io/minikube-hostpath_minikube_03966c3b-0dd1-4447-9209-61790e528a99!
I0319 15:19:49.967507       1 event.go:282] Event(v1.ObjectReference{Kind:"Endpoints", Namespace:"kube-system", Name:"k8s.io-minikube-hostpath", UID:"e2cb37b8-1416-4dbe-832f-b6bd691082a2", APIVersion:"v1", ResourceVersion:"7630", FieldPath:""}): type: 'Normal' reason: 'LeaderElection' minikube_03966c3b-0dd1-4447-9209-61790e528a99 became leader
I0319 15:19:50.066953       1 controller.go:884] Started provisioner controller k8s.io/minikube-hostpath_minikube_03966c3b-0dd1-4447-9209-61790e528a99!


==> storage-provisioner [d7d187c1ca72] <==
I0319 15:18:55.962749       1 storage_provisioner.go:116] Initializing the minikube storage provisioner...
F0319 15:19:17.467869       1 main.go:39] error getting server version: Get "https://10.96.0.1:443/version?timeout=32s": dial tcp 10.96.0.1:443: connect: no route to host

